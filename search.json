[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Unsupervised Machine Learning",
    "section": "",
    "text": "Overview\nThis course on unsupervised learning provides a systematic introduction to dimensionality reduction and clustering techniques. The course covers fundamental concepts of unsupervised learning and data normalization, then progresses through the practical applications of Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and hierarchical clustering algorithms.\nThe course emphasizes both theoretical understanding and hands-on application, teaching students to recognize when different techniques are appropriate and when they may fail. A key learning objective is understanding the limitations of linear methods like PCA. Students learn to evaluate the performance of unsupervised learning methods across diverse data types, with the ultimate goal of generating meaningful hypotheses for further research.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Applied Unsupervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Applied Unsupervised Machine Learning",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\nThe learning objectives and course outline are detailed below. \n\n\n\n\nSession 1 (half‚Äëday) Introduction to unsupervised learning\n\nIntroduction to unsupervised learning and normalization:\nUnderstand the fundamental principles of unsupervised learning and recognize the role that data normalization plays in preparing datasets for analysis.\nWhy normalization is required:\nExplain why normalization is necessary to ensure that features with different scales do not unduly influence unsupervised learning algorithms.\nWhy dimensionality reduction is required\nWhy you need dimensionality reduction.\nBasics of dimensionality reduction:\nDescribe the core concepts of dimensionality reduction. Then describe Principal Component Analysis (PCA), including how it reduces dimensionality by identifying directions of maximum variance.\nEvaluating unsupervised learning results\nHow to check the performance and quality of your unsupervised learning results.\n\n\n\n\nSession 2 (half‚Äëday) Basics of dimensionality reduction\n\nBasic applications of PCA:\nApply PCA to real datasets, interpret the resulting principal components, and discuss how these components can reveal underlying structure.\nCurse of dimensionality:\nExplain the concept of the curse of dimensionality and its implications for the performance and interpretability of clustering and dimensionality‚Äëreduction algorithms.\nPCA and t‚ÄëSNE:\nCompare and contrast PCA and t‚ÄëDistributed Stochastic Neighbor Embedding (t‚ÄëSNE) as two popular techniques for dimensionality reduction and data visualization.\nBasics of t‚ÄëSNE:\nExplain how t‚ÄëSNE projects high‚Äëdimensional data into two or three dimensions while preserving local similarities between points.\nApplications to data:\nDemonstrate the use of both PCA and t‚ÄëSNE on sample datasets to visualize clustering tendencies and uncover hidden patterns.\n\n\n\n\nSession 3 (half‚Äëday) Basics of Clustering\n\nClustering:\nDefine clustering in the context of unsupervised learning and outline its importance in discovering groupings within data.\nBasics of k‚Äëmeans:\nDescribe the k‚Äëmeans clustering algorithm, including how cluster centroids are initialized and updated to minimize within‚Äëcluster variance.\nBasics of hierarchical clustering:\nExplain the steps of hierarchical clustering, heatmaps, agglomerative approaches, and interpret dendrograms.\nDeciding on your clustering approach:\nSituations in which you would want to apply hierarchical clustering. Discuss specific use cases: such as when the number of clusters is unknown or when a tree‚Äëbased representation is desired‚Äîwhere hierarchical clustering is advantageous.\n\n\n\n\nSession 4 (half‚Äëday) Practical applications (hands-on)\n\nWhen not to apply PCA and t‚ÄëSNE:\nIdentify situations where PCA or t‚ÄëSNE may produce misleading results or be computationally infeasible, and propose alternative strategies.\nPractical applications:\nExplore real‚Äëworld scenarios where unsupervised learning methods provide actionable insights across various domains.\nPractical applications of PCA, t‚ÄëSNE and hierarchical clustering to biological data:\nApply PCA, t‚ÄëSNE, and hierarchical clustering to biological datasets (e.g., gene expression or single‚Äëcell data), interpret the results, and discuss biological insights gained.\nEvaluating unsupervised learning methods\nHow to evaluate these techniques on different kinds of data (single-cell data, electronic healthcare records, social sciences data): these are used to generate hypotheses. Motivations for next steps.\n\n\n\nTarget Audience\nStudents who have some basic familiarity with Python. There are no prerequisites for knowledge of biology or statistics. The course is designed for those who want to learn how to apply unsupervised machine learning techniques to real-world datasets.\n\n\nPrerequisites\nBasic familiarity with Python. Course webpage is here: Introduction to Python\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Applied Unsupervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Applied Unsupervised Machine Learning",
    "section": "Citation & Authors",
    "text": "Citation & Authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: ‚ÄúWe carried our analyses based on the recommendations in YourReferenceHere‚Äù.\n\n\nYou can cite these materials as:\n\n\"Banerjee\", \". (2025). \"Applied Unsupervised Machine Learning\". \"https://cambiotraining.github.io/ml-unsupervised/\"\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {\"Banerjee\", \"Soumya\"},\n  month = {7},\n  title = {\"Applied Unsupervised Machine Learning\"},\n  url = {\"https://cambiotraining.github.io/ml-unsupervised/\"},\n  year = {2025}\n}\nAbout the authors:\n\"Soumya\" \"Banerjee\"  \nAffiliation: \"University of Cambridge\" Roles: \"writing - first draft, review & editing; conceptualisation; software\"",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Applied Unsupervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Applied Unsupervised Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nWe thank Martin van Rongen, Vicki Hodgson, Hugo Tavares, Paul Fannon, Matt Castle and the Bioinformatics Facility Training Team for their support and guidance.\nIntroduction to Statistical Learning in Python",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Applied Unsupervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\nDownload\nAlternatively you can use the link below to download the data from Google Drive:\nDownload",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Data & Setup",
    "section": "Software",
    "text": "Software\n\nüì¶ Installation Instructions (for Google Colab)\n\nHow to use in Colab:\n\nOpen a new notebook in Google Colab\nIf you want to run the notebooks in Colab, you can also use the Open in Colab badge below:\n\n\n\n\nOpen In Colab\n\n\n\nRun the commands in code cells.\nYou can now create notebooks and run any of the scripts in Google Colab.\n\nRepository link:\nhttps://cambiotraining.github.io/ml-unsupervised/\n\n(Optional) In order to access the files on your computer, you can save them to your Google Drive (in a directory named data) and mount the drive in Colab. Open a new Google Colab notebook. Then create a new code cell and type the following commands (and then click the play button to run the cell):\n\nfrom google.colab import drive\nimport os\nimport pandas as pd\n\ndrive.mount('/content/drive')\nos.chdir('/content/drive/My Drive/data')\n\npd.read_csv('diabetes_sample_data.csv')\n\nGoogle Colab will ask you to authenticate your Google account the first time you run the drive.mount() command. Follow the instructions in the output cell to complete the authentication process. You will need a gmail account or you can use your cam.ac.uk account. Some screenshots are shown below to guide you through the process:\n\n\n\n\nColab Authentication Step 1\n\n\n\n\n\nColab Authentication Step 2\n\n\n\n\n\nColab Authentication Step 3\n\n\n\nIf you have your files in a different directory, please change the path in the os.chdir() command above accordingly.\nA template notebook is also available to get you started.\nGoogle Colab comes with most of the required packages pre-installed. If you need to install any additional packages, you can do so using the !pip install package-name command in a code cell.\nCreate a new folder named data in the My Drive folder. Download the data files and and copy the files into the data folder (if you want to access the files from Colab). Your directory structure should look like this:\n\nMy Drive/\n‚îî‚îÄ data/\n\n(Optional) You can also follow the instructions below to setup from the browser.\nOpen a new Google Colab notebook.\n\n\n\n\nStep 1\n\n\n\nClick on the folder icon on the left-hand side to open the file explorer.\n\n\n\n\nStep 2\n\n\n\nRight-click and then mount your Google Drive and then click on the Run this cell button.\n\n\n\n\nStep 3\n\n\n\nCreate a new folder named data in the My Drive folder. Then upload the data files into the data folder.\n\n\n\n\nStep 4\n\n\n\n\n\n\n\n\n\n(Optional) Run locally\n\nInstall Python (we recommend using Anaconda or Miniconda to manage your Python environment).\nInstall Visual Studio Code (see below) or Spyder.\n\n\n\nDownload the data folder from the link here or unzip the file here and save it on your computer.\n\nIn Terminal/Command Prompt, navigate to the folder in which you save the data folder. Then change directory to the data folder:\n\ncd data\n\n\n(Optional) Setup a virtual environment\n\npython3 -m venv .venv\n\nActivate the virtual environment\n\nOn Windows (in Command Prompt):\n\n.venv\\Scripts\\activate\n\nOn MacOS/Linux:\n\nsource .venv/bin/activate\nInstall required Python packages\n\npip install numpy pandas scikit-learn seaborn matplotlib scanpy pca\n\nIf you have a local Python installation (see above), you can also run the scripts there (see instructions below). This assumes that you have downloaded and unzipped the data folder to your computer (in a folder called data) and that you have installed the required packages (see above).\n\nYour directory structure should look like this:\ndata/\n‚îî‚îÄ \n\nimport os\n\n# where are we?\nprint( os.getcwd() )\n\n# change directory to where the data is stored\nos.chdir('data/')\n\n# where are we now?\nprint( os.getcwd() )\n\n\nA template Python script is also available to get you started.\n\n\n\n(Optional) Visual Studio Code\n\nWindowsMac OSLinux (Ubuntu)\n\n\n\nGo to the Visual Studio Code download page and download the installer for your operating system. Double-click the downloaded file to install the software, accepting all the default options.\nAfter completing the installation, go to your Windows Menu, search for ‚ÄúVisual Studio Code‚Äù and launch the application.\nGo to ‚ÄúFile &gt; Preferences &gt; Settings‚Äù, then select ‚ÄúText Editor &gt; Files‚Äù on the drop-down menu on the left. Scroll down to the section named ‚ÄúEOL‚Äù and choose ‚Äú\\n‚Äù (this will ensure that the files you edit on Windows are compatible with the Linux operating system).\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for Mac.\nGo to the Downloads folder and double-click the file you just downloaded to extract the application. Drag-and-drop the ‚ÄúVisual Studio Code‚Äù file to your ‚ÄúApplications‚Äù folder.\nYou can now open the installed application to check that it was installed successfully (the first time you launch the application you will get a warning that this is an application downloaded from the internet - you can go ahead and click ‚ÄúOpen‚Äù).\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for your Linux distribution. Install the package using your system‚Äôs installer.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "3¬† References and Resources",
    "section": "",
    "text": "3.1 Resources\n[1] Introduction to Statistical Learning in Python (ISLP) book\n[2] Video lectures by the authors of the book Introduction to Statistical Learning in Python\n[3] Visual explanations of machine learning algorithms\n[4] How to Use t-SNE Effectively\n[5] Slides for the course on unsupervised machine learning\n[6] Course on introduction to Python\n[7] Data visualization in Python using matplotlib and seaborn\n[8] FAQs by the creator of tSNE\n[9] Other intricacies of tSNE\n[10] Explanation of linkage functions",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>References and Resources</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html",
    "href": "materials/walkthrough.html",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "",
    "text": "4.1 Setup instructions",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#setup-instructions",
    "href": "materials/walkthrough.html#setup-instructions",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "",
    "text": "Please go through the setup instructions here\nWalkthrough of getting setup with Google Colab in the web browser.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#poll",
    "href": "materials/walkthrough.html#poll",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.2 Poll",
    "text": "4.2 Poll\nWhat would you like to get out this course?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#introduction",
    "href": "materials/walkthrough.html#introduction",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.3 Introduction",
    "text": "4.3 Introduction\nUnsupervised learning is a branch of machine learning that deals with finding hidden patterns or intrinsic structures in data without the use of labeled responses. Unlike supervised learning, where the model learns from labeled data to predict outcomes, unsupervised learning works with input data that does not have any corresponding output variables. The primary goal is to explore the underlying structure, groupings, or features in the data.\nOne of the most common applications of unsupervised learning is clustering, where the algorithm groups similar data points together based on their characteristics. This is particularly useful in scenarios such as customer segmentation, anomaly detection, and image compression. Another key technique is dimensionality reduction, which aims to reduce the number of variables under consideration, making it easier to visualize and interpret large datasets.\nUnsupervised learning is valuable because it can reveal insights that may not be immediately apparent, uncovering relationships and patterns that might otherwise go unnoticed. It is commonly used in exploratory data analysis and as a preprocessing step for other algorithms. As data continues to grow in complexity and volume, unsupervised learning plays a critical role in making sense of unstructured information.\n\n4.3.1 Motivation\nHere is a picture (taken by Soumya Banerjee) of a pavement in Cambridge the day after Valentine‚Äôs Day. Why did this picture capture my attention? The starkness of the grey pavement contrasted with the bright red rose. It may have triggered some unsupervised learning mechanism in my brain that allows me to pick anomalies!\n\n\n\nRose after Valentine‚Äôs Day (picture taken by Soumya Banerjee)\n\n\nUnsupervised learning is all about discovering structure in data without any explicit ‚Äúright answers‚Äù to guide you. The rose‚Äëon‚Äëpavement photo is a perfect real‚Äëworld illustration of a few core ideas:\n\nAnomaly (or Outlier) Detection\nWhat happened in your brain:\nWhen you look at a uniform grey pavement, your visual system builds an internal ‚Äúmodel‚Äù of what is normal: flat, texture‚Äërepeating, monochrome. The bright red rose doesn‚Äôt fit that model, so it ‚Äúpops,‚Äù drawing your attention.\nIn machine learning:\nAlgorithms can learn a representation of ‚Äúnormal‚Äù data (e.g.¬†patches of pavement) and then flag anything that deviates significantly (e.g.¬†the rose) as an anomaly.\nHuman vision analogy:\nEarly in the visual cortex, neurons respond to edges, color contrasts, textures. A red circle on grey evokes strong responses in ‚Äúcolor‚Äù and ‚Äúshape‚Äëedge‚Äù channels.\n\n\n\n\n\n\n\n\n\n4.3.2 Example\nGiven the data below, how should we reduce the number of features and/or visualize it? This is an unsupervised machine learning problem.\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT): The columns of this data are the features.\n\n\n\n\n\n\n\n\n\n\n\n\nState\nMurder (per 100k)\nRobbery (per 100k)\nPopulation\n\n\n\n\nCalifornia\n9.1\n45.3\n39,512,223\n\n\nTexas\n7.8\n38.6\n28,995,881\n\n\nFlorida\n5.9\n31.7\n21,477,737\n\n\nNew York\n3.4\n26.4\n19,453,561\n\n\nIllinois\n6.4\n35.1\n12,671,821\n\n\nPennsylvania\n4.8\n22.9\n12,801,989\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT): Importantly, in unsupervised machine learning we are not trying to predict anything. For example, say in the data below we can try to predict the number of people who moved to that state last year. This would be a supervised machine learning problem (Gareth et al. 2017).\n\n\n\n\n\n\n\n\n\n\n\n\n\nState\nMurder (per 100k)\nRobbery (per 100k)\nPopulation\nPeople Who Moved (per 100k)\n\n\n\n\nCalifornia\n9.1\n45.3\n39,512,223\n5,400\n\n\nTexas\n7.8\n38.6\n28,995,881\n4,100\n\n\nFlorida\n5.9\n31.7\n21,477,737\n6,200\n\n\nNew York\n3.4\n26.4\n19,453,561\n3,800\n\n\nIllinois\n6.4\n35.1\n12,671,821\n2,900\n\n\nPennsylvania\n4.8\n22.9\n12,801,989\n2,500\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPTS):\n\nIf there is a column in the table that we are trying to predict, this would be called a label. Supervised machine learning (such as linear regression) tries to predict the label given the features (Gareth et al. 2017).\nHowever in unsupervised machine learning, we only deal with features and do not try to predict anything.\nHence there are no labels in unsupervised machine learning.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPTS):\n\nThe number of columns in the data, is called the dimensions of the data.\nFor example, if there are 3 columns, this is 3-dimensional data.\nWe can visualize it in 3 dimensions in a 3D plot.\nHence the columns of the data or dimensions form a co-ordinate system. We can visualize it in a plot. The X axis would represent the value in the first column, the Y axis would represent the values in the second column, and so on.\nHow would you visualize a table that has 14 columns/dimensions?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#curse-of-dimensionality",
    "href": "materials/walkthrough.html#curse-of-dimensionality",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.4 Curse of dimensionality",
    "text": "4.4 Curse of dimensionality\nHow would you visualize data that has 14 dimensions? How about 1 million dimensions (can happen in the age of big data)?\n\nWould you remove one column at a time?\nWould you plot each feature/column vs the other exhaustively? How many pairwise plots would you need to plot for exhaustive visualization?\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\n\n\nNumber of pairwise plots for 5 features: 10\nNumber of pairwise plots for 14 features: 91\nNumber of pairwise plots for 1000000 features: 499999500000\n\n\n\n\n\n\n\n\n\n\n\n\nYou will have an exponential blowup! So this approach will not work.\nWe need something more principled.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT)\n\nIt is not possible to exhaustively plot one feature vs.¬†another or remove any feature randomly. For example, in a dataset from patient healthcare records, we cannot just randomly throw away data on blood pressure.\nThis is why we use dimensionality reduction to reduce the dimensions of the data. This yields fewer new features.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#what-pca-does-to-the-data",
    "href": "materials/walkthrough.html#what-pca-does-to-the-data",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.5 What PCA does to the data",
    "text": "4.5 What PCA does to the data\n\n\n4.5.1 Projection of 3D Data\nWe generate three clusters of synthetic 3‚Äëdimensional points, compute the first two principal components using scikit‚Äëlearn‚Äôs PCA, and then create a two‚Äëpanel figure:\n\nLeft panel: A 3D scatter of the original points, the best‚Äêfit plane defined by the first two principal components, and projection lines from each point down onto that plane.\n\nRight panel: A 2D scatter of the projected coordinates (the principal component scores) along the first two components, colored by cluster.\n\nUse this visualization to understand how PCA finds the plane that maximizes variance and how the data look when reduced to two dimensions.\n\n\n\n\n\n\n\n\n\n\nPlay around with the figure below to get an intuition of what PCA does!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#another-intuition-behind-pca",
    "href": "materials/walkthrough.html#another-intuition-behind-pca",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.6 Another intuition behind PCA",
    "text": "4.6 Another intuition behind PCA\nPrincipal Component Analysis (PCA) reduces the number of dimensions in your data by finding the directions where the data varies the most and keeping only those directions.\nImagine you have a book: it has length, width, and height. But if the book is very thin, almost all of its size or volume is in the length and width‚Äîthe height is so small that you can almost ignore it. If you wanted to describe where the book is on a table, you could just use two numbers (length and width), and you wouldn‚Äôt lose much information by ignoring the height.\nPCA works the same way: it finds the thin directions in your data that do not add much information, and lets you focus on the most important dimensions. This makes it easier to visualize and analyze your data without losing the main patterns.\n\n\n\nA thin book on the floor. Image created using the DALL-E AI tool.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT)\nSay your data has too many columns/dimensions/features. Dimensionality reduction techniques (such as PCA) reduce the number of dimensions or the number of columns in your data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#coming-back-to-the-crime-data",
    "href": "materials/walkthrough.html#coming-back-to-the-crime-data",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.7 Coming back to the crime data",
    "text": "4.7 Coming back to the crime data\n\nOriginal Data (4 Features)\n\nOur crime dataset has 4 features (dimensions) for each state:\n\n\n\n\n\n\n\n\n\n\nState\nMurder (per 100k)\nRobbery (per 100k)\nPopulation\nPeople Who Moved (per 100k)\n\n\n\n\nCalifornia\n9.1\n45.3\n39,512,223\n5,400\n\n\nTexas\n7.8\n38.6\n28,995,881\n4,100\n\n\nFlorida\n5.9\n31.7\n21,477,737\n6,200\n\n\nNew York\n3.4\n26.4\n19,453,561\n3,800\n\n\nIllinois\n6.4\n35.1\n12,671,821\n2,900\n\n\nPennsylvania\n4.8\n22.9\n12,801,989\n2,500",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#what-pca-does",
    "href": "materials/walkthrough.html#what-pca-does",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.8 What PCA Does",
    "text": "4.8 What PCA Does\nPCA finds new features (called Principal Components) that are combinations of the original features. These new features capture the most important patterns in the data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#after-pca-reduced-to-2-new-features",
    "href": "materials/walkthrough.html#after-pca-reduced-to-2-new-features",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.9 After PCA: Reduced to 2 New Features",
    "text": "4.9 After PCA: Reduced to 2 New Features\nPCA transforms our data into Principal Components (PC1 and PC2). These are the new, reduced features:\n\n\n\n\n\n\n\n\n\nState\nPC1 (Primary Pattern)\nPC2 (Secondary Pattern)\nVariance Explained\n\n\n\n\nCalifornia\n3.45\n0.82\nPC1: 68%\n\n\nTexas\n2.18\n-0.15\nPC2: 23%\n\n\nFlorida\n1.35\n1.24\nTotal: 91%\n\n\nNew York\n0.92\n-1.05\n\n\n\nIllinois\n0.31\n0.18\n\n\n\nPennsylvania\n-0.22\n-0.94",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#what-did-we-gain",
    "href": "materials/walkthrough.html#what-did-we-gain",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.10 What Did We Gain?",
    "text": "4.10 What Did We Gain?\n\n4.10.1 Before PCA:\n\n4 features to track and visualize\nHard to see overall patterns\nFeatures may be correlated (redundant information)\n\n\n\n4.10.2 After PCA:\n\n2 features capture 91% of the information\nEasier to visualize (can plot in 2D)\nNew features are uncorrelated\nLost only 9% of information",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#what-do-the-principal-components-mean",
    "href": "materials/walkthrough.html#what-do-the-principal-components-mean",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.11 What Do the Principal Components Mean?",
    "text": "4.11 What Do the Principal Components Mean?\nPC1 (Primary Pattern): Combines all 4 original features with different weights. States with high PC1 values (like California) tend to have high values across multiple original features.\nPC2 (Secondary Pattern): Captures the remaining variation not explained by PC1. This might separate states based on different combinations of the original features.\nKey Insight: PC1 and PC2 are weighted combinations like: - PC1 = 0.45√óMurder + 0.48√óRobbery + 0.52√óPopulation + 0.54√óMigration - PC2 = 0.62√óMurder - 0.15√óRobbery - 0.68√óPopulation + 0.35√óMigration\n(These weights are determined by the data patterns, not chosen arbitrarily)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#pca-is-lossy",
    "href": "materials/walkthrough.html#pca-is-lossy",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.12 PCA is lossy",
    "text": "4.12 PCA is lossy\nPCA does lose some information. But it can capture some/most of the salient aspects of your data.\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT)\nDimensionality reduction techniques (such as PCA) always lose some information. In other words, it is lossy.\n\n\n\n\n4.12.1 Lesson on lossy compression (PCA applied to image)\n\n\n4.12.2 Learning Objectives\n\nUnderstand how Principal Component Analysis (PCA) can be applied to images.\nObserve how PCA captures the most significant patterns in image data.\nVisualize how the number of principal components affects image reconstruction.\nAppreciate the trade-off between compression and information loss.\n\n\n\n4.12.3 Key Concepts\n\nPCA is a dimensionality reduction technique that identifies directions (principal components) along which the variance in the data is maximized.\nImages can be viewed as high-dimensional data (each pixel as a feature), and PCA helps reduce that dimensionality while preserving key patterns.\n\n\n\n4.12.4 Procedure Overview\n\nLoad and display an image from a URL.\nApply PCA to each RGB channel of the image separately.\nReconstruct the image using an increasing number of principal components.\nVisualize the reconstructions to show how few components capture most of the image‚Äôs structure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.12.5 Takeaway Message\nPCA can significantly reduce image data dimensionality while preserving salient features, making it a powerful tool for image compression and understanding. However, perfect reconstruction is only possible with all components, revealing the balance between efficiency and fidelity.\n\n\n\n\n\n\nTipKey Concept\n\n\n\n\n\n\nInformation bottleneck\n\n\nIn unsupervised learning, the bottleneck concept refers to a deliberate constraint where information is compressed through a narrow intermediate step. The model is trained to reconstruct the input data after passing it through this low-dimensional bottleneck, forcing it to learn a compact and informative representation of the underlying structure of the data.\nNote: The figure shows features coming in from the right and getting compressed to fewer (new) features. What is not shown is that there are multiple rows in the dataset.\nSince there are no labels guiding the learning process, the model relies solely on reconstructing its input as accurately as possible, using only the limited information passed through this narrow channel. This compression encourages the model to capture essential features while discarding noise or redundancy.\n\nBecause the computer must recreate the original image from that stripped‚Äëdown summary, it learns to ignore random noise.\n\n\n\n\n\n\n\n\n\n\n\nTipActivity: Playable version of PCA in browser\n\n\n\nPCA in your browser",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#visual-explanations-of-pca",
    "href": "materials/walkthrough.html#visual-explanations-of-pca",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.13 Visual explanations of PCA",
    "text": "4.13 Visual explanations of PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\nPCA maximimizes the variance captured\n\n\n\n\n\n\n\n\n\n\n\nApp to explain the intuition behind PCA\n\n\n\n\nAnimation\n\n\n\n\n\nAnimation",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#lesson-summary",
    "href": "materials/walkthrough.html#lesson-summary",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.14 Lesson Summary",
    "text": "4.14 Lesson Summary\n\nBasics of unsupervised learning\nUseful for visualization, outlier detection and making sense of your data if there are many features\nWhat it is: Discover hidden patterns or groupings in unlabeled data, without predicting a specific target.\n\nKey techniques:\n\nClustering for grouping similar observations\n\nDimensionality reduction (e.g.¬†PCA) for compressing and visualizing high‚Äëdimensional data\n\n\nWhy it matters:\n\nReveals structure in customer segmentation, anomaly detection, image compression, etc.\n\nServes as exploratory analysis and preprocessing for downstream tasks\n\n\nInformation bottleneck: Forcing models to squeeze data through a narrow bottleneck uncovers the most essential features and removes noise\n\nHands‚Äëon example: Apply PCA to crime‚Äëand‚Äëpopulation data by state to project three features into two dimensions for visualization\n\nUnsupervised vs.¬†supervised:\n\nUnsupervised: No labels, focus on pattern discovery\n\nSupervised: With labels, focus on predicting a known outcome",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/walkthrough.html#resources",
    "href": "materials/walkthrough.html#resources",
    "title": "4¬† Introduction to Unsupervised Learning",
    "section": "4.15 Resources",
    "text": "4.15 Resources\n\nIntroduction to Statistical Learning in Python book\nVideo lectures by the authors of the book Introduction to Statistical Learning in Python\nGithub repository with more theoretical material\nInteractive explanations of machine learning models\nMathematics behind unsupervised machine learning\n\n\n\n\n\nGareth, James, Witten Daniela, Hastie Trevor, and Tibshirani Robert. 2017. Introduction to Statistical Learning with Applications in r. Springer. https://www.statlearning.com/.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/python_refresher.html",
    "href": "materials/python_refresher.html",
    "title": "5¬† Refresher on Python",
    "section": "",
    "text": "5.1 Refresher on Python\n!pip install pandas numpy scikit-learn seaborn matplotlib scanpy pca\nNote: Here is an alternative way to read a file\n# 1. IMPORTING PACKAGES\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# 2. READING DATA WITH PANDAS FROM GITHUB\n\n# GitHub URL for the diabetes data\n# Convert from GitHub web URL to raw data URL\ngithub_url = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv\"\n\n# Read CSV file directly from GitHub\ndiabetes_data = pd.read_csv(github_url)\n\n# Display basic information about the data\nprint(\"\\nData shape:\", diabetes_data.shape)\nprint(\"\\nFirst 5 rows:\")\nprint(diabetes_data.head())\n        \nprint(\"\\nBasic statistics:\")\nprint(diabetes_data.describe())\n\n# 3. PLOTTING WITH MATPLOTLIB\n\n# Plot 1: Histogram of Age\nplt.figure()\nplt.hist(diabetes_data['age'], bins=20, alpha=0.7)\nplt.title('Distribution of Age', fontsize=14, fontweight='bold')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.savefig('age_distribution.png', dpi=300)\nplt.show()\n\n# 4. NUMPY\na = np.array([17, 13, 78, 901])\nprint(\"This is a numpy array:\")\nprint(a)\nprint(\"Here is the mean/average:\")\nprint( np.mean(a) )\n\n\nData shape: (100, 6)\n\nFirst 5 rows:\n   patient_id   age  glucose   bmi  blood_pressure  diabetes\n0           1  62.5     97.5  29.8            71.7         0\n1           2  52.9    127.4  30.8            74.4         0\n2           3  64.7    129.7  33.4            87.5         0\n3           4  77.8    115.9  33.3            86.1         1\n4           5  51.5    135.2  21.1            79.8         1\n\nBasic statistics:\n       patient_id         age     glucose         bmi  blood_pressure  \\\ncount  100.000000  100.000000  100.000000  100.000000      100.000000   \nmean    50.500000   53.444000  140.670000   28.322000       81.066000   \nstd     29.011492   13.625024   28.611669    5.425223        8.842531   \nmin      1.000000   15.700000   82.400000   11.800000       58.800000   \n25%     25.750000   46.000000  115.800000   24.700000       74.350000   \n50%     50.500000   53.100000  142.550000   28.500000       80.500000   \n75%     75.250000   61.075000  156.175000   31.500000       86.825000   \nmax    100.000000   82.800000  221.600000   47.300000      101.900000   \n\n         diabetes  \ncount  100.000000  \nmean     0.250000  \nstd      0.435194  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      0.250000  \nmax      1.000000  \n\n\n\n\n\n\n\n\n\nThis is a numpy array:\n[ 17  13  78 901]\nHere is the mean/average:\n252.25",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Refresher on Python</span>"
    ]
  },
  {
    "objectID": "materials/python_refresher.html#refresher-on-python",
    "href": "materials/python_refresher.html#refresher-on-python",
    "title": "5¬† Refresher on Python",
    "section": "",
    "text": "See Python setup instructions here: Python Installation.\nWalkthrough of getting setup with Google Colab in the web browser.\nInstall Python packages\n\n\n\nLoading data and data visualization\n\n\nimport pandas as pd\nimport os\n# find out which directory is your current working directory\nos.getcwd()\n# now change directory to where your files are (my files are in the directory shown below)\nos.chdir(\"/Users/soumyabanerjee/soumya_cam_mac/teaching/ml-unsupervised/\") \n# now read the file\ndiabates_data = pd.read_csv(\"course_files/data/diabetes_sample_data.csv\")\n\n\nYou can also go through this Introduction to Visualization in Python course\n\n\n5.1.1 Optional exercise on Python\n\n\n\n\n\n\nExerciseExercise 1 - exercise_python\n\n\n\n\n\n\nLevel: \nLoad the dataset from this GitHub URL:\nhttps://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\nSave it to a variable called crime_data and display:\n\nThe shape of the data\n\nThe first 3 rows\n\nColumn names using .columns\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nSimple data munging and visualization\n\nPythonR\n\n\n\nimport pandas as pd\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\"\n\ncrime_data = pd.read_csv(url)\n\n# Display the shape of the data\nprint(\"Shape of the dataset: \", crime_data.shape)\n\n# Display the first 3 rows\nprint(\"\\nFirst 3 rows: \")\nprint(crime_data.head(3))\n\n# Display column names\nprint(\"\\nColumn names: \")\nprint(crime_data.columns)\n\nShape of the dataset:  (48, 5)\n\nFirst 3 rows: \n     State  Murder  Assault  UrbanPop  ViolentCrime\n0  Alabama    13.2      236        58          21.2\n1   Alaska    10.0      263        48          44.5\n2  Arizona     8.1      294        80          31.0\n\nColumn names: \nIndex(['State', 'Murder', 'Assault', 'UrbanPop', 'ViolentCrime'], dtype='object')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Optional exercise on Python\n\n\n\n\n\n\nExerciseExercise 2 - exercise_python_visualization\n\n\n\n\n\n\nLevel: \nLoad the dataset from this GitHub URL:\nhttps://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\nHistogram: Show the distribution of murders\n\nUse plt.hist() with the Murder column\n\nUse 15 bins\n\nAdd title: ‚ÄúDistribution of Murder‚Äù\n\nAdd axis labels and grid\n\nRemember to use plt.show() after each plot!\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nSimple data visualization\n\nPythonR\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\"\ncrime_data = pd.read_csv(url)\n\n# Draw histogram\nplt.figure()\nplt.hist(crime_data[\"Murder\"], bins=15)\nplt.grid(True)\nplt.xlabel(\"Murder\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Murder\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Optional exercise on Python\n\n\n\n\n\n\nExerciseExercise 3 - exercise_python_numpy\n\n\n\n\n\n\nLevel: \nLoad the dataset from this GitHub URL:\nhttps://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\nCalculate the mean/average number of murders in USA.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nSimple numpy usage\n\nPythonR\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\"\ncrime_data = pd.read_csv(url)\n\n# Draw histogram\nnumpy_array_murders = crime_data[\"Murder\"].to_numpy()\n\nprint( np.mean(numpy_array_murders) )\n\n7.916666666666667\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.4 Optional exercise on Python\n\n\n\n\n\n\nExerciseExercise 4 - exercise_python_data_munging_advanced\n\n\n\n\n\n\nLevel: \nLoad the dataset from this GitHub URL:\nhttps://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\nFind the state that has the highest number of murders.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nData munging (advanced)\n\nPythonR\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/USArrests.csv\"\ncrime_data = pd.read_csv(url)\n\n# use groupby\nprint(crime_data.groupby(\"State\")[\"Murder\"].mean().sort_values(ascending = False).head())\n\n# Alternative use idxmax() and loc()\n# arrests_data['Murder'].idxmax() ‚Üí finds the index (row number) of the maximum value in the Murder column.\n# .loc[ ... , ['State', 'Murder']] ‚Üí uses .loc[] to look up the row with the maximum murder rate and only show the State name and Murder value.\nprint(crime_data.loc [crime_data[\"Murder\"].idxmax(), [\"State\",\"Murder\"] ])\n\nState\nGeorgia           17.4\nMississippi       16.1\nFlorida           15.4\nLouisiana         15.4\nSouth Carolina    14.4\nName: Murder, dtype: float64\nState     Georgia\nMurder       17.4\nName: 9, dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.5 Optional exercise on Python\n\n\n\n\n\n\nExerciseExercise 5 - exercise_python_numpy\n\n\n\n\n\n\nLevel: \nFill in the blanks in the code below.\nimport numpy as np\n\n# 1) Reproducibility\nnp.random.seed(7)\n\n# 2) Make a 5x4 array of random numbers in [0, 1)\nX = np.random.rand(5, 4)\n\n# 3) Compute:\n# - mean of each column\n# - mean of each row\n# - overall mean\ncol_means = ...\nrow_means = ...\noverall_mean = ...\n\nprint(\"X:\\n\", X)\nprint(\"Column means:\", col_means)\nprint(\"Row means:\", row_means)\nprint(\"Overall mean:\", overall_mean)\n\n# 4) Bonus: random integers from 0..99 (size=12). Compare mean to 49.5\nints = np.random.randint(0, 100, size=12)\nints_mean = ...\nprint(\"Random integers:\", ints)\nprint(\"Integers mean:\", ints_mean)\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nNumpy (advanced)\n\nPythonR\n\n\n\nimport numpy as np\n\n# 1) Reproducibility\nnp.random.seed(7)\n\n# 2) Make a 5x4 array of random numbers in [0, 1)\nX = np.random.rand(5, 4)\n\n# 3) Compute:\n# - mean of each column\n# - mean of each row\n# - overall mean\ncol_means = X.mean(axis=0)\nrow_means = X.mean(axis=1)\noverall_mean = X.mean()\n\nprint(\"X:\\n\", X)\nprint(\"Column means:\", col_means)\nprint(\"Row means:\", row_means)\nprint(\"Overall mean:\", overall_mean)\n\n# 4) Bonus: random integers from 0..99 (size=12). Compare mean to 49.5\nints = np.random.randint(0, 100, size=12)\nints_mean = ints.mean()\nprint(\"Random integers:\", ints)\nprint(\"Integers mean:\", ints_mean)\n\nX:\n [[0.07630829 0.77991879 0.43840923 0.72346518]\n [0.97798951 0.53849587 0.50112046 0.07205113]\n [0.26843898 0.4998825  0.67923    0.80373904]\n [0.38094113 0.06593635 0.2881456  0.90959353]\n [0.21338535 0.45212396 0.93120602 0.02489923]]\nColumn means: [0.38341265 0.46727149 0.56762226 0.50674962]\nRow means: [0.50452537 0.52241424 0.56282263 0.41115415 0.40540364]\nOverall mean: 0.48126400765922045\nRandom integers: [61 64 34 56 73 78 38  4  9 87 99 67]\nIntegers mean: 55.833333333333336",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Refresher on Python</span>"
    ]
  },
  {
    "objectID": "materials/python_refresher.html#summary",
    "href": "materials/python_refresher.html#summary",
    "title": "5¬† Refresher on Python",
    "section": "5.2 Summary",
    "text": "5.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nA quick refresher on Python\nSimple exercises (optional)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Refresher on Python</span>"
    ]
  },
  {
    "objectID": "materials/python_refresher.html#resources",
    "href": "materials/python_refresher.html#resources",
    "title": "5¬† Refresher on Python",
    "section": "5.3 Resources",
    "text": "5.3 Resources\n[1] Course on data analysis using Python\n[2] Introduction to Visualization in Python course\n[3] Coursera courses on Python",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Refresher on Python</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html",
    "href": "materials/normalization.html",
    "title": "6¬† Normalizing your data and PCA",
    "section": "",
    "text": "6.1 Introduction\nThis chapter demonstrates basic unsupervised machine learning concepts using Python.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#introduction",
    "href": "materials/normalization.html#introduction",
    "title": "6¬† Normalizing your data and PCA",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\nUnderstand the difference between supervised and unsupervised learning.\nApply PCA and clustering to example data.\nVisualize results.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#normalization-z-score-standardization",
    "href": "materials/normalization.html#normalization-z-score-standardization",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.2 Normalization (Z-score Standardization)",
    "text": "6.2 Normalization (Z-score Standardization)\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\nIntuition: the value represents the number of standard deviations away from the mean for that variable. For example an 80-year-old person might be 3 standard deviations above the mean age.\nThe formula for Z-score is:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nWhere: - \\(x\\) is the original data point. - \\(\\mu\\) is the mean of the data. - \\(\\sigma\\) is the standard deviation of the data.\nFor example, say you have two variables or features on very different scales.\n\n\n\nAge\nWeight (grams)\n\n\n\n\n25\n65000\n\n\n30\n70000\n\n\n35\n75000\n\n\n40\n80000\n\n\n45\n85000\n\n\n50\n90000\n\n\n55\n95000\n\n\n60\n100000\n\n\n65\n105000\n\n\n70\n110000\n\n\n75\n115000\n\n\n80\n120000\n\n\n\nIf these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.\nHence we normalize each of the features separately, i.e.¬†age is normalized relative to age and weight is normalized relative to weight.\n\n\nOriginal data:\nAge: mean=43.6, std=13.1\nWeight: mean=69.8, std=9.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn an ideal scenario a feature/variable such as weight might be transformed in the following way after normalization:\n\n\n\n\n\n\n\n\n\n\n\nAnd here is what it might look like for a feature such as age.\n\n\n\nZ-scored mean: -0.00, std: 1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT):\n\nAfter normalization, the normalized features are on comparable scales. The features (such as weight and age) no longer have so much variation. They can be used as input to machine learning algorithms.\nThe rule of thumb is to (almost) always normalize your data before you use it in a machine learning algorithm. (There are a few exceptions and we will point this out in due course).\n\n\n\n\n\n6.2.1 Data visualization before doing PCA\n\n\n\n\n\n\nExerciseExercise 1 - exercise_data_visualization\n\n\n\n\n\n\nLevel: \nYou should always visualize your data before trying any algorithms on it.\nDiscuss in a group. What is wrong with the following plot?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nLooking at your data\nAlways look at your data before you try and machine learning technique on it. There is a 150 year old person in your data!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT):\n\nVisualize your data before you do any normalization. If there is anything odd about your data, discuss this with the person who gave you the data or did the experiment. This could be an error in the machine that generated the data or a data entry error. If there is justification, you can remove the data point.\nThen perform normalization and apply a machine learning technique.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#setup",
    "href": "materials/normalization.html#setup",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.3 Setup",
    "text": "6.3 Setup\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#example-data",
    "href": "materials/normalization.html#example-data",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.4 Example Data",
    "text": "6.4 Example Data",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#pca-example",
    "href": "materials/normalization.html#pca-example",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.5 PCA Example",
    "text": "6.5 PCA Example\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nPythonR\n\n\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.title(\"PCA Projection\")\nplt.show()\n\n\n\n\nA simple PCA plot (biplot)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#scree-plot",
    "href": "materials/normalization.html#scree-plot",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.6 Scree plot",
    "text": "6.6 Scree plot\nA scree plot is a simple graph that shows how much variance (information) each principal component explains in your data after running PCA. The x-axis shows the principal components (PC1, PC2, etc.), and the y-axis shows the proportion of variance explained by each one.\nYou can use a scree plot to decide how many principal components to keep: look for the point where the plot levels off (the elbow): this tells you that adding more components doesn‚Äôt explain much more variance.\n\n# Scree plot: variance explained by each component\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Variance Explained Ratio\")\nplt.show()\n\n\n\n\n\n\n\n\nA scree plot may have an elbow like the plot below.\n\n\n\n\n\n\n\n\n\n\n\n6.6.1 Hands-on coding\n\nPerform PCA on a dataset of US Arrests\nSimple method first\n\n\n!pip install pandas numpy scikit-learn seaborn matplotlib\n\n\nLoad libraries and data\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the US Arrests data\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\n\nX = pd.read_csv(url, index_col=0)\n\n# alternatively if you have downloaded the data folder \n#   on your computer try the following\n# import os\n# os.getcwd()\n# os.chdir(\"data\")\n# X = pd.read_csv(\"USArrests.csv\", index_col=0)\n\n# what is in the data?\nX.head()\n\n\n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nViolentCrime\n\n\nState\n\n\n\n\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10.0\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\n\n\n\n\n\n\nNormalize the data\n\n\nscaler_standard = StandardScaler()\nX_scaled = scaler_standard.fit_transform(X)\n\n\nPerform PCA\n\n\npca_fn = PCA()\nX_pca = pca_fn.fit_transform(X_scaled)\n\n\nPlotting\n\n\nplt.figure()\nplt.scatter(X_pca[:,0], X_pca[:,1])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA on crime data\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLabel the plot by US State\n\nNote: These are now categorical, i.e.¬†these take on discrete values (as opposed to continuous).\n\nIn machine learning, we will need to deal with them differently.\nDiscussion: on how to encode these values and how to ensure that these values are equidistant from each other.\n\n\n# States come from the index\nX.index\n\nstates = X.index # fetch states and assign it to a variable\n\n# map each state to a code\ncolour_codes_states = pd.Categorical(states).codes\n\n# pd.Categorical(states): Converts the sequence states (e.g., a list/Index of state names) into a categorical type. It internally builds:\n# categories: the unique labels (e.g., all distinct state names)\n# codes: integer labels pointing to those categories\n\nplt.figure()\nplt.scatter(X_pca[:,0], X_pca[:,1], c = colour_codes_states)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA on crime data (coloured by US state)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nA method to have text labels in the PCA plot\n\nWe need slightly more complex code to do this.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nPythonR\n\n\n\n#loadings = pca.components_.T * np.sqrt(pca.explained_variance_)  # variable vectors\n\n# Plot\nfig, ax = plt.subplots()\n\n# Scatter of states\nax.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n\n# Label each state\n# X.index has the state names\n# go through each point (which is each row in the table)\nfor i, state in enumerate(X.index):\n    ax.text(X_pca[i, 0], X_pca[i, 1], state, fontsize=8, va=\"center\", ha=\"left\")\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"PCA Biplot: US Arrests\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nA simple PCA plot with text labels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet the loadings\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nPythonR\n\n\n\n# get the loadings\n# pca.components_ contains the principal component vectors\n# transpose them using T\nloadings = pca_fn.components_.T\n\n# create a data frame\ndf_loadings = pd.DataFrame(loadings,\n                            index=X.columns\n)\n\n# the first column is PC1, then PC2, and so on ...\nprint(df_loadings)\n\n                     0         1         2         3\nMurder        0.533785 -0.428765 -0.331927 -0.648891\nAssault       0.583489 -0.190485 -0.267593  0.742732\nUrbanPop      0.284213  0.865950 -0.386784 -0.140542\nViolentCrime  0.542068  0.173225  0.817690 -0.086823\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.6.2 Interpreting the US Crime PCA biplot\nHere is an intutive explanation of the PCA biplot.\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT):\n\nDistances matter: Points that are far apart represent states with more dissimilar overall crime/urbanization profiles. For example, Vermont being far from California indicates very different feature patterns in the variables used (e.g., assault, murder, urban population).\nPC1 (horizontal) ‚âà Crime level/severity: Higher values indicate greater overall crime intensity (e.g., higher assault/murder rates), lower values indicate lower crime intensity.\nPC2 (vertical) ‚âà Urbanization: States to the right tend to have higher urban population and associated traits; those to the left are more rural.\nYou can inspect the loadings to understand what each principal component represents.\nWe will have an exercise on this later. \nReading clusters: States that cluster together have similar profiles. States on opposite sides of the plot (e.g., Vermont vs.¬†California) differ substantially along the dominant patterns captured by PC1 and PC2.\nInterpretation of loadings:\nPC1 (Urbanization axis): All crime variables (Murder, Assault, ViolentCrime) load positively, while UrbanPop has a smaller positive loading. This suggests PC1 captures overall crime levels.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT):\n\nNotice, that we have not told PCA anything about the US states\nYet it is still able to find some interesting patterns in the data\nThis is the strength of unsupervised machine learning\n\n\n\n\n\n\nAnother method using the pca package; prettier plots\n\nInstall the pca Python package\n!pip install pca\n\nLoad data\n\n\nfrom pca import pca\nimport pandas as pd\n\n# Load the US Arrests data (available online)\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\ndf = pd.read_csv(url, index_col=0)\n\nprint(\"US Arrests Data (first 5 rows):\")\nprint(df.head())\nprint(\"\\nData shape:\", df.shape)\n\nUS Arrests Data (first 5 rows):\n            Murder  Assault  UrbanPop  ViolentCrime\nState                                              \nAlabama       13.2      236        58          21.2\nAlaska        10.0      263        48          44.5\nArizona        8.1      294        80          31.0\nArkansas       8.8      190        50          19.5\nCalifornia     9.0      276        91          40.6\n\nData shape: (48, 4)\n\n\n\n\nNormalize the data and perform PCA\n\n\nmodel = pca(normalize=True)\nout = model.fit_transform(df)\nax = model.biplot()\n\n\n\n\n\n\n\n\n\nVariance explained plots\n\n\nmodel.plot()\n\n(&lt;Figure size 1440x960 with 1 Axes&gt;,\n &lt;Axes: title={'center': 'Cumulative explained variance\\n 3 Principal Components explain [100.0%] of the variance.'}, xlabel='Principal Component', ylabel='Percentage explained variance'&gt;)\n\n\n\n\n\n\n\n\n\n\n3D PCA biplots\n\n\nmodel.biplot3d()\n\n(&lt;Figure size 3000x2500 with 1 Axes&gt;,\n &lt;Axes3D: title={'center': '3 Principal Components explain [100.0%] of the variance'}, xlabel='PC1 (61.6% expl.var)', ylabel='PC2 (24.7% expl.var)', zlabel='PC3 (9.14% expl.var)'&gt;)\n\n\n\n\n\n\n\n\n\n\nLoadings\n\nRecall\nWhat is being plotted on the axes (PC1 and PC2) are the scores.\nThe scores for each principal component are calculated as follows:\n\\[\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + ....\n\\]\nwhere \\(X\\), \\(Y\\) and \\(Z\\) are the normalized features.\nThe constants \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) are determined by the PCA algorithm. They are called the loadings.\n\nprint(model.results)\n\n{'loadings':        Murder   Assault  UrbanPop  ViolentCrime\nPC1  0.533785  0.583489  0.284213      0.542068\nPC2 -0.428765 -0.190485  0.865950      0.173225\nPC3 -0.331927 -0.267593 -0.386784      0.817690, 'PC':                      PC1       PC2       PC3\nAlabama         0.923886 -1.127792 -0.437720\nAlaska          1.884005 -1.032585  2.032973\nArizona         1.705462  0.730059  0.043498\nArkansas       -0.198714 -1.092074  0.111217\nCalifornia      2.462479  1.513698  0.585558\nColorado        1.453427  0.982671  1.080932\nConnecticut    -1.406810  1.081895 -0.661238\nDelaware       -0.003621  0.319738 -0.730442\nFlorida         2.947649 -0.070435 -0.569823\nGeorgia         1.571384 -1.281416 -0.326932\nHawaii         -0.966398  1.557165  0.034386\nIdaho          -1.689257 -0.178154  0.241665\nIllinois        1.320695  0.653978 -0.681444\nIndiana        -0.561650  0.161720  0.218372\nIowa           -2.302281  0.133259  0.145716\nKansas         -0.850716  0.279295  0.013602\nKentucky       -0.808869 -0.934920 -0.029023\nLouisiana       1.500981 -0.882536 -0.772483\nMaine          -2.444195 -0.340245 -0.083049\nMaryland        1.702710 -0.431039 -0.158134\nMassachusetts  -0.536401  1.454143 -0.626920\nMichigan        2.044350  0.144860  0.383014\nMinnesota      -1.742422  0.647555  0.133541\nMississippi     0.932617 -2.374555 -0.724196\nMissouri        0.637255  0.263934  0.369919\nMontana        -1.239466 -0.507562  0.236769\nNebraska       -1.317489  0.212450  0.160150\nNevada          2.806905  0.760007  1.157898\nNew Hampshire  -2.431886  0.048021  0.018380\nNew Jersey      0.127587  1.417883 -0.775421\nNew Mexico      1.917815 -0.148279  0.181459\nNew York        1.623118  0.790157 -0.646164\nNorth Carolina  1.064086 -2.207350 -0.854340\nNorth Dakota   -3.038797 -0.548177  0.281399\nOhio           -0.281823  0.736114 -0.041732\nOklahoma       -0.366423  0.292555 -0.026415\nOregon          0.003276  0.556212  0.921912\nPennsylvania   -0.941353  0.568486 -0.411608\nRhode Island   -0.909909  1.464948 -1.387731\nSouth Carolina  1.257310 -1.914756 -0.290121\nSouth Dakota   -2.038884 -0.778125  0.375435\nTennessee       0.935690 -0.851392  0.192734\nTexas           1.293269  0.387317 -0.490484\nUtah           -0.602262  1.466342  0.271830\nVermont        -2.851337 -1.332665  0.825094\nVirginia       -0.153441 -0.190521  0.005751\nWashington     -0.270617  0.975724  0.604878\nWest Virginia  -2.160933 -1.375609  0.097337, 'explained_var': array([0.61629429, 0.86387677, 0.95532444, 1.        ]), 'variance_ratio': array([0.61629429, 0.24758248, 0.09144767, 0.04467556]), 'model': PCA(n_components=np.int64(3)), 'scaler': StandardScaler(), 'pcp': np.float64(1.0000000000000002), 'topfeat':     PC       feature   loading  type\n0  PC1       Assault  0.583489  best\n1  PC2      UrbanPop  0.865950  best\n2  PC3  ViolentCrime  0.817690  best\n3  PC1        Murder  0.533785  weak, 'outliers':                  y_proba     p_raw    y_score  y_bool  y_bool_spe  y_score_spe\nAlabama         0.883607  0.572223   4.780770   False       False     1.457903\nAlaska          0.771864  0.061516  12.020355   False       False     2.148419\nArizona         0.883607  0.487777   5.447889   False       False     1.855152\nArkansas        0.994964  0.849865   2.662427   False       False     1.110005\nCalifornia      0.771864  0.073767  11.512662   False       False     2.890516\nColorado        0.883607  0.291940   7.323773   False       False     1.754449\nConnecticut     0.883607  0.376298   6.434671   False       False     1.774715\nDelaware        0.997194  0.934870   1.827387   False       False     0.319759\nFlorida         0.827974  0.105185  10.498050   False       False     2.948491\nGeorgia         0.883607  0.334106   6.858767   False       False     2.027628\nHawaii          0.883607  0.482129   5.494442   False       False     1.832672\nIdaho           0.883607  0.589071   4.652640   False       False     1.698626\nIllinois        0.883607  0.518417   5.200102   False       False     1.473744\nIndiana         0.997522  0.957121   1.535211   False       False     0.584469\nIowa            0.883607  0.341171   6.785183   False       False     2.306135\nKansas          0.997194  0.915334   2.046914   False       False     0.895390\nKentucky        0.942972  0.766164   3.332046   False       False     1.236262\nLouisiana       0.883607  0.372576   6.470687   False       False     1.741210\nMaine           0.883607  0.264681   7.652531   False       False     2.467763\nMaryland        0.883607  0.543590   5.001739   False       False     1.756421\nMassachusetts   0.883607  0.512543   5.247023   False       False     1.549922\nMichigan        0.883607  0.406682   6.149239   False       False     2.049476\nMinnesota       0.883607  0.477475   5.533020   False       False     1.858861\nMississippi     0.827974  0.134373   9.776761   False       False     2.551134\nMissouri        0.997194  0.908446   2.118885   False       False     0.689750\nMontana         0.942972  0.701130   3.819185   False       False     1.339364\nNebraska        0.942972  0.758317   3.391711   False       False     1.334509\nNevada          0.771864  0.052403  12.462954   False       False     2.907976\nNew Hampshire   0.883607  0.317979   7.031119   False       False     2.432360\nNew Jersey      0.883607  0.577856   4.737795   False       False     1.423612\nNew Mexico      0.883607  0.502690   5.326337   False       False     1.923539\nNew York        0.883607  0.382425   6.375898   False       False     1.805231\nNorth Carolina  0.827974  0.137996   9.697217   False       False     2.450444\nNorth Dakota    0.771864  0.080403  11.269266   False       False     3.087845\nOhio            0.997194  0.934716   1.829224   False       False     0.788218\nOklahoma        0.997522  0.982272   1.082928   False       False     0.468886\nOregon          0.994964  0.843369   2.717557   False       False     0.556221\nPennsylvania    0.942972  0.749878   3.455517   False       False     1.099691\nRhode Island    0.883607  0.234460   8.050041   False       False     1.724530\nSouth Carolina  0.883607  0.243404   7.928295   False       False     2.290659\nSouth Dakota    0.883607  0.291991   7.323178   False       False     2.182321\nTennessee       0.942972  0.719456   3.683208   False       False     1.265063\nTexas           0.942972  0.649265   4.202711   False       False     1.350022\nUtah            0.883607  0.576700   4.746601   False       False     1.585206\nVermont         0.771864  0.038043  13.332939   False       False     3.147398\nVirginia        0.997522  0.997522   0.524914   False       False     0.244627\nWashington      0.942972  0.761722   3.365861   False       False     1.012557\nWest Virginia   0.883607  0.177782   8.926009   False       False     2.561626, 'outliers_params': {'paramT2': (np.float64(-2.4671622769447922e-17), np.float64(1.273765915366402)), 'paramSPE': (array([-9.25185854e-17, -1.38777878e-17]), array([[2.51762774e+00, 6.29946348e-17],\n       [6.29946348e-17, 1.01140077e+00]]))}}",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#sec-pcanorm",
    "href": "materials/normalization.html#sec-pcanorm",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.7 Exercise for normalization in PCA",
    "text": "6.7 Exercise for normalization in PCA\n\n\n\n\n\n\nExerciseExercise 2 - exercise_pca_normalization\n\n\n\n\n\n\nLevel: \nWork in a group.\n\nTry the same code above but now without normalisation.\nWhat differences do you observe in PCA with and without normalization?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#sec-ex-theoretical",
    "href": "materials/normalization.html#sec-ex-theoretical",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.8 Exercise (theoretical)",
    "text": "6.8 Exercise (theoretical)\n\n\n\n\n\n\nExerciseExercise 3 - exercise_theoretical\n\n\n\n\n\n\nLevel: \nBreak up into groups and discuss the following problem:\n\nShown are biological samples with scores\nThe features are genes\n\n\nWhy are Sample 33 and Sample 24 separated from the rest? What can we say about Gene1, Gene 2, Gene 3 and Gene 4?\nWhy is Sample 2 separated from the rest? What can we say about Gene1, Gene 2, Gene 3 and Gene 4?\nCan we treat Sample 2 as an outlier? Why or why not? Argue your case.\n\nThe PCA biplot is shown below:\n\n\n\n\n\n\n\n\n\nThe table of loadings is shown below:\n\n\n            PC1       PC2       PC3       PC4\nGene1 -0.535899  0.418181 -0.341233  0.649228\nGene2 -0.583184  0.187986 -0.268148 -0.743075\nGene3 -0.278191 -0.872806 -0.378016  0.133877\nGene4 -0.543432 -0.167319  0.817778  0.089024",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#pca-vs.-other-techniques",
    "href": "materials/normalization.html#pca-vs.-other-techniques",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.9 üß† PCA vs.¬†Other Techniques",
    "text": "6.9 üß† PCA vs.¬†Other Techniques\n\nPCA is unsupervised (no labels used)\nWorks best for linear relationships\nAlternatives:\n\nt-SNE for nonlinear structures (we will encounter this in the next session)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#in-practice-tips-for-biologists",
    "href": "materials/normalization.html#in-practice-tips-for-biologists",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.10 üß¨ In Practice: Tips for Biologists",
    "text": "6.10 üß¨ In Practice: Tips for Biologists\n\nAlways standardize data before PCA\nBe cautious interpreting PCs biologically: PCs are mathematical constructs\n\n\n\n\n\n\n\nTipSummary\n\n\n\n\nNeed to normalize data before doing dimensionality reduction\nPCA reduces dimensionality for visualization.\nClustering algorithms finds clusters in unlabeled data.\nThe goal of unsupervised learning is to find patterns and form hypotheses.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/normalization.html#resources",
    "href": "materials/normalization.html#resources",
    "title": "6¬† Normalizing your data and PCA",
    "section": "6.11 Resources",
    "text": "6.11 Resources\n[1] Article on normalization on Wikipedia\n[2] ISLP book\n[3] Video lectures by the authors of the book Introduction to Statistical Learning in Python\n[4] Visual explanations of machine learning algorithms",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Normalizing your data and PCA</span>"
    ]
  },
  {
    "objectID": "materials/goal_unsupervised.html",
    "href": "materials/goal_unsupervised.html",
    "title": "7¬† Goal of Unsupervised Learning",
    "section": "",
    "text": "7.1 Goals of unsupervised learning\nHere is an example from biological data (single-cell sequencing data) (the plot is from [2])(Aschenbrenner et al. 2020).\nYou can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.\nYou can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.\nAll of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Goal of Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/goal_unsupervised.html#goals-of-unsupervised-learning",
    "href": "materials/goal_unsupervised.html#goals-of-unsupervised-learning",
    "title": "7¬† Goal of Unsupervised Learning",
    "section": "",
    "text": "Finding patterns in data\n\n\n\n\n\nExample tSNE\n\n\n\n\n\nExample heatmaps\n\n\n\nFinding interesting patterns\n\n\n\n\nFinding outliers\n\n\n\n\nFinding hypotheses",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Goal of Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/goal_unsupervised.html#thought-exercise",
    "href": "materials/goal_unsupervised.html#thought-exercise",
    "title": "7¬† Goal of Unsupervised Learning",
    "section": "7.2 Thought exercise",
    "text": "7.2 Thought exercise\n\nCan you think of a technique where unsupervised learning gets used a lot? Hint: we use it almost every day now! (or atleast I do)\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nChatGPT or Generative AI. The first step in processing the huge amount of text is to reduce the dimensions of the data using something similar to PCA.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Goal of Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/goal_unsupervised.html#summary",
    "href": "materials/goal_unsupervised.html#summary",
    "title": "7¬† Goal of Unsupervised Learning",
    "section": "7.3 Summary",
    "text": "7.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe goal of unsupervised learning is to find patterns and form hypotheses.\n\n\n\n\n\n\n\nAschenbrenner, Dominik, Maria Quaranta, Soumya Banerjee, Nicholas Ilott, Joanneke Jansen, Boyd Steere, Yin-Huai Chen, et al. 2020. ‚ÄúDeconvolution of Monocyte Responses in Inflammatory Bowel Disease Reveals an IL-1 Cytokine Network That Regulates IL-23 in Genetic and Acquired IL-10 Resistance.‚Äù Gut, October, gutjnl-2020-321731. https://doi.org/10.1136/gutjnl-2020-321731.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Goal of Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "materials/conceptual_basis_PCA.html",
    "href": "materials/conceptual_basis_PCA.html",
    "title": "8¬† Conceptual and mathematical basis of PCA",
    "section": "",
    "text": "8.1 Intuitive explanation of PCA\nExplanation of PCA (by StatQuest)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Conceptual and mathematical basis of PCA</span>"
    ]
  },
  {
    "objectID": "materials/conceptual_basis_PCA.html#differences-between-pca-and-linear-regression",
    "href": "materials/conceptual_basis_PCA.html#differences-between-pca-and-linear-regression",
    "title": "8¬† Conceptual and mathematical basis of PCA",
    "section": "8.2 Differences between PCA and linear regression",
    "text": "8.2 Differences between PCA and linear regression\nDoes the figure above look similar to linear regression? Is PCA the same as linear regression?\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT): PCA is not linear regression. It looks similar though, does it not?\nLinear regression is a predictive model. PCA is not. You cannot use PCA to predict anything. You can use PCA to only pick out patterns in your data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Conceptual and mathematical basis of PCA</span>"
    ]
  },
  {
    "objectID": "materials/conceptual_basis_PCA.html#key-concepts",
    "href": "materials/conceptual_basis_PCA.html#key-concepts",
    "title": "8¬† Conceptual and mathematical basis of PCA",
    "section": "8.3 üìä Key Concepts",
    "text": "8.3 üìä Key Concepts\n\n8.3.1 1. Scores and Loadings\nWhat is being plotted on the axes (PC1 and PC2) are the scores.\nThe scores for each principal component are calculated as follows:\n\\[\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + ....\n\\]\nwhere \\(X\\), \\(Y\\) and \\(Z\\) are the normalized features.\nThe constants \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) are determined by the PCA algorithm. These are called the loadings.\n\n\n8.3.2 2. Linear combinations\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT): The principal components are linear combinations of the original features. Hence they can be a bit difficult to interpret.\n\n\n\n\n\n8.3.3 3. Variance\n\nVariance = how spread out the data is.\nPCA finds directions (principal components) that maximize variance.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Conceptual and mathematical basis of PCA</span>"
    ]
  },
  {
    "objectID": "materials/conceptual_basis_PCA.html#example-gene-expression-data",
    "href": "materials/conceptual_basis_PCA.html#example-gene-expression-data",
    "title": "8¬† Conceptual and mathematical basis of PCA",
    "section": "8.4 üî¨ Example: Gene Expression Data",
    "text": "8.4 üî¨ Example: Gene Expression Data\n\nRows = samples (patients)\nColumns = gene expression levels\n\n\n8.4.1 Goal:\n\nReduce dimensionality from 20,000 genes to 2 to 3 PCs\nVisualize patterns between patient groups (e.g., healthy vs.¬†cancer)\n\n# Sample Python code (requires numpy, sklearn, matplotlib)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nX = ...  # gene expression matrix\nX_scaled = StandardScaler().fit_transform(X)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA of Gene Expression')\nplt.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Conceptual and mathematical basis of PCA</span>"
    ]
  },
  {
    "objectID": "materials/conceptual_basis_PCA.html#summary",
    "href": "materials/conceptual_basis_PCA.html#summary",
    "title": "8¬† Conceptual and mathematical basis of PCA",
    "section": "8.5 Summary",
    "text": "8.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPCA is not linear regression!\nscores and loadings",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 1: Introduction to Unsupervised Learning",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Conceptual and mathematical basis of PCA</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html",
    "href": "materials/tsne.html",
    "title": "9¬† tSNE",
    "section": "",
    "text": "9.1 The curse of dimensionality\nIn very high-dimensional spaces, almost all the ‚Äúvolume‚Äù of a dataset lives near its corners, and pairwise Euclidean distances between points tend to concentrate around a single value.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#simplified-explanations",
    "href": "materials/tsne.html#simplified-explanations",
    "title": "9¬† tSNE",
    "section": "9.2 Simplified explanations",
    "text": "9.2 Simplified explanations\nThink of each cell as a point in a space where each gene‚Äôs activity is its own ‚Äúaxis.‚Äù When you have only a few genes (low dimensions), you can tell cells apart by how far apart they sit in that space. But as you add more genes, almost every cell ends up about the same distance from every other cell‚Äîso you lose any useful sense of ‚Äúclose‚Äù or ‚Äúfar.‚Äù\nImagine you‚Äôre trying to find similar cells in a dataset. As you measure more features (dimensions), it becomes harder to find truly similar cells, even though you have more information.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#visual-example-finding-similar-points",
    "href": "materials/tsne.html#visual-example-finding-similar-points",
    "title": "9¬† tSNE",
    "section": "9.3 Visual Example: Finding Similar Points",
    "text": "9.3 Visual Example: Finding Similar Points\n\n9.3.1 1 Dimension (1 feature)\nFeature 1: [0]----[1]----[2]----[3]----[4]----[5]\n           A      B      C      D      E      F\n\nPoints A and B are close (distance = 1)\nPoints A and F are far (distance = 5)\n\n\n9.3.2 2 Dimensions (2 features)\nFeature 2: 5 |     F\n           4 |  E\n           3 |     D\n           2 |  C\n           1 |     B\n           0 |A\n             0  1  2  3  4  5  Feature 1\n\nPoints A and B are still close\nPoints A and F are still far\n\n\n9.3.3 3+ Dimensions (3+ features)\nFeature 3: 5 |     F\n           4 |  E\n           3 |     D\n           2 |  C\n           1 |     B\n           0 |A\n             0  1  2  3  4  5  Feature 1\n             \nFeature 4, 5, 6... (more dimensions)\n\nAs dimensions increase:\n- All points become equally distant from each other\n- \"Close\" and \"far\" lose meaning\n- Finding similar cells becomes impossible",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#why-this-happens-the-empty-space-problem",
    "href": "materials/tsne.html#why-this-happens-the-empty-space-problem",
    "title": "9¬† tSNE",
    "section": "9.4 Why This Happens: The ‚ÄúEmpty Space‚Äù Problem",
    "text": "9.4 Why This Happens: The ‚ÄúEmpty Space‚Äù Problem\n2D Circle - most area near the edge:\n    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n  ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà\n ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà\n‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà\n‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà\n ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà\n  ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà\n    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT):\n\nALL volume concentrates at the ‚Äúsurface‚Äù The interior becomes essentially EMPTY!\nYour data points all end up at the edges, far apart from each other.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#high-dimensions-are-counter-intuitive",
    "href": "materials/tsne.html#high-dimensions-are-counter-intuitive",
    "title": "9¬† tSNE",
    "section": "9.5 High-dimensions are counter-intuitive",
    "text": "9.5 High-dimensions are counter-intuitive\nCounter-intuitive things happen in high dimensions. For example, most of the volume is near the edges!\n\n\n\n\n\n\n\n\n\nWhy k-Means fails:\nk-Means tries to draw boundaries around groups by asking ‚ÄúWhich centroid (group center) is each cell closest to?‚Äù In very high‚Äìgene spaces, every cell is nearly the same distance from all centroids. Small moves of the centroids don‚Äôt change which cells get assigned to them, so k-Means can‚Äôt find real groupings.\nWhy t-SNE helps:\nt-SNE ignores the idea of absolute distance and instead asks, ‚ÄúWhich cells are each cell‚Äôs few nearest neighbors?‚Äù It builds a map that keeps those local neighborhoods intact. In the final 2D picture, cells that were neighbors in the huge gene space stay neighbors on the screen, while cells that weren‚Äôt neighbors get pushed apart. This way, you still see meaningful clusters (e.g., cell types) even when dealing with hundreds or thousands of genes.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#tldr-simple-explanation",
    "href": "materials/tsne.html#tldr-simple-explanation",
    "title": "9¬† tSNE",
    "section": "9.6 TLDR (Simple explanation)",
    "text": "9.6 TLDR (Simple explanation)\nt-SNE (pronounced ‚Äútee-snee‚Äù) is a tool that helps us look at complex data by making it easier to see patterns.\n\n9.6.1 Imagine this:\n\nYou have a big box of mixed beads. Each bead has many features: color, size, shape, weight, etc.\nIt is hard to see how the beads are similar or different just by looking at all these features at once.\n\n\n\n9.6.2 What t-SNE does:\n\nt-SNE takes all those features and creates a simple map (like a 2D picture).\nIn this map, beads that are similar to each other are placed close together.\nBeads that are very different are placed far apart.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#pictorial-explanation-of-tsne",
    "href": "materials/tsne.html#pictorial-explanation-of-tsne",
    "title": "9¬† tSNE",
    "section": "9.7 Pictorial explanation of tSNE",
    "text": "9.7 Pictorial explanation of tSNE\nHigh-dimensional beads (hard to see groups):\n[üî¥] [üîµ] [üü¢] [üü°] [üî¥] [üü¢] [üîµ] [üü°] [üî¥] [üü¢] [üîµ] [üü°]\nEach bead has many features (color, size, shape, etc.)\n    |\n    v\nt-SNE makes a simple 2D map:\n[üî¥] [üî¥] [üî¥] | | [üîµ] [üîµ] [üîµ]\n[üü¢] [üü¢] [üü¢]\n[üü°] [üü°] [üü°]\nNow, similar beads are grouped together.\nIn summary:\nt-SNE is like a magic tool that turns complicated data into a simple picture, so we can easily see groups and patterns‚Äîeven if we do not understand the math behind it!\n\n9.7.1 Why is this useful?\n\nIt helps us see groups or clusters in our data.\nWe can spot patterns, like which beads are most alike, or if there are outliers.\nEmphasis on preserving local structure.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#why-tsne-works-in-high-dimensions",
    "href": "materials/tsne.html#why-tsne-works-in-high-dimensions",
    "title": "9¬† tSNE",
    "section": "9.8 Why t‚ÄëSNE Works in High Dimensions",
    "text": "9.8 Why t‚ÄëSNE Works in High Dimensions\n\nBypasses global distance concentration by focusing on nearest neighbors.\n\n\n\n\n\n\n\n\nTipThe complex explanation\n\n\n\n\n\nIn very high-dimensional spaces, almost all the ‚Äúvolume‚Äù of a dataset lives near its corners, and pairwise Euclidean distances between points tend to concentrate around a single value. As dimension \\(n\\) grows, the volume of an inscribed ball in the hypercube \\([-1,1]^n\\) shrinks toward zero, and the ratio\n\\[\n\\frac{\\max d - \\min d}{\\min d}\n\\]\nfor distances \\(d\\) between random points rapidly approaches zero. Intuitively, ‚Äúnearest‚Äù and ‚Äúfarthest‚Äù neighbors become indistinguishable, so any method that relies on global distances (like k-Means) loses its ability to meaningfully separate points into clusters.\nk-Means clustering exemplifies this breakdown: it repeatedly assigns each point to its nearest centroid based on squared-distance comparisons. When all inter-point distances look almost the same, tiny shifts in centroid positions barely affect those assignments, leading to noisy labels and flat optimization landscapes with no clear gradients. In practice, k-Means can ‚Äúget stuck‚Äù or fail to discover any meaningful grouping once dimensions rise into the dozens or hundreds.\nt-SNE sidesteps these problems by focusing only on local similarities rather than global distances. It first converts pairwise distances in the high-dimensional space into a distribution of affinities \\(p_{ij}\\) using Gaussian kernels centered on each point. Then it searches for a low-dimensional embedding whose Student-t affinity distribution \\(q_{ij}\\) best matches \\(p_{ij}\\). By emphasizing the preservation of each point‚Äôs nearest neighbors and using a heavy-tailed low-dimensional kernel to push dissimilar points apart, t-SNE highlights local clusters even when global geometry has become uninformative‚Äîmaking it a far more effective visualization and exploratory tool in very high dimensions.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#intuitive-explanation-of-tsne",
    "href": "materials/tsne.html#intuitive-explanation-of-tsne",
    "title": "9¬† tSNE",
    "section": "9.9 Intuitive explanation of tSNE",
    "text": "9.9 Intuitive explanation of tSNE\nExplanation of tSNE (by StatQuest)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#digging-into-tsne",
    "href": "materials/tsne.html#digging-into-tsne",
    "title": "9¬† tSNE",
    "section": "9.10 Digging into tSNE",
    "text": "9.10 Digging into tSNE",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#tsne-works-iteratively",
    "href": "materials/tsne.html#tsne-works-iteratively",
    "title": "9¬† tSNE",
    "section": "9.11 tSNE works iteratively",
    "text": "9.11 tSNE works iteratively\ntSNE works iteratively to find similar points and brings them together. This is similar to clustering which we will encounter later.\ntSNE gradually moves points to preserve neighborhoods. üéØ Similar points are pulled together, different cells pushed apart.\n\n\n\n\n\n\n\n\n\n\n\nAn animation of how tSNE works\n\n\n\n\n\nAnimation of how tSNE works",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#using-a-t-distribution-over-points",
    "href": "materials/tsne.html#using-a-t-distribution-over-points",
    "title": "9¬† tSNE",
    "section": "9.12 Using a t-distribution over points",
    "text": "9.12 Using a t-distribution over points\n\nProblem in 2D: When we compress high‚Äëdimensional data into 2D, many points that were moderately far apart get squashed together. With a Gaussian in 2D, those ‚Äúfar‚Äù points all look similarly unlikely, which causes crowding in the center.\nt-distribution has heavy tails: It decreases more slowly than a Gaussian. So points that are moderately far apart in 2D still get some probability‚Äînot zero.\nWhat this achieves:\n\nReduces crowding in the middle of the plot.\nSpreads clusters out more naturally.\nPreserves local neighborhoods (close points stay close) while allowing space between different groups.\n\nIntuition: In high dimensions we can identify close neighbours well. In 2D there isn‚Äôt enough room, so everything would pile up. The heavy tails of the t‚Äëdistribution give extra ‚Äúelbow room,‚Äù keeping clusters distinct and easier to interpret.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#explanation-of-perplexity",
    "href": "materials/tsne.html#explanation-of-perplexity",
    "title": "9¬† tSNE",
    "section": "9.13 Explanation of perplexity",
    "text": "9.13 Explanation of perplexity\ntSNE has an important parameter called perplexity.\n\nWhat it is: Perplexity is like the number of close friends each point listens to when arranging the map.\nHow to think about it: It sets the typical neighborhood size.\n\nLow perplexity: each point cares about a small, tight circle.\nHigh perplexity: each point listens to more distant neighbors too.\n\nIf it‚Äôs too low: You may get tiny, fragmented clusters or noisy structure.\n\nIf it‚Äôs too high: Different groups can blur together, losing fine details.\n\nGood starting range: 5‚Äì50 (try a few values to see what‚Äôs most interpretable).\nAnalogy: Imagine placing cells on a 2D table. Perplexity decides how many nearby ‚Äúreference cells‚Äù each one considers when finding its spot‚Äîtoo few and it overfits tiny patterns; too many and it smooths away meaningful differences.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#how-to-choose-perplexity",
    "href": "materials/tsne.html#how-to-choose-perplexity",
    "title": "9¬† tSNE",
    "section": "9.14 How to choose perplexity?",
    "text": "9.14 How to choose perplexity?\nThe most appropriate value of perplexity depends on the density of your data. Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity. Typical values for the perplexity range between 5 and 50.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#key-concept",
    "href": "materials/tsne.html#key-concept",
    "title": "9¬† tSNE",
    "section": "9.15 Key Concept",
    "text": "9.15 Key Concept\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPerplexity in t-SNE acts like a knob for the effective number of nearest neighbours.\n\n\n\n\nPerplexity as a knob for a complex machine learning model. Image created using DALL-E.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#activity-interactive-figure-showing-tsne-and-perplexity",
    "href": "materials/tsne.html#activity-interactive-figure-showing-tsne-and-perplexity",
    "title": "9¬† tSNE",
    "section": "9.16 Activity: Interactive figure showing tSNE and perplexity",
    "text": "9.16 Activity: Interactive figure showing tSNE and perplexity\nHere is an interactive tSNE on the Swiss roll dataset (we will encounter this later). Play around with this figure and use the slider to change the value of perplexity!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#exercise-building-intuition-on-how-to-use-tsne",
    "href": "materials/tsne.html#exercise-building-intuition-on-how-to-use-tsne",
    "title": "9¬† tSNE",
    "section": "9.17 Exercise: Building intuition on how to use tSNE",
    "text": "9.17 Exercise: Building intuition on how to use tSNE\n\nPitfalls in using tSNE\nLet us read the paper How to use t-SNE effectively (Wattenberg, Vi√©gas, and Johnson 2017).\nDistances are not preserved\nNormal does not always look normal",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#what-does-tsne-look-like-compared-to-pca",
    "href": "materials/tsne.html#what-does-tsne-look-like-compared-to-pca",
    "title": "9¬† tSNE",
    "section": "9.18 What does tSNE look like compared to PCA?",
    "text": "9.18 What does tSNE look like compared to PCA?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#simple-code-to-perform-tsne-hands-on-exercise",
    "href": "materials/tsne.html#simple-code-to-perform-tsne-hands-on-exercise",
    "title": "9¬† tSNE",
    "section": "9.19 Simple code to perform tSNE (hands-on exercise)",
    "text": "9.19 Simple code to perform tSNE (hands-on exercise)\nLet us now practice performing tSNE on some data. We will use the iris data. The Iris dataset is a small, classic dataset in machine learning.\nIt contains measurements of 150 flowers from three species of iris (setosa, versicolor, virginica).\nFor each flower, four features are recorded: - Sepal length - Sepal width - Petal length - Petal width\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.manifold import TSNE\nimport pandas as pd\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data            # The features (measurements)\ny = iris.target          # The species labels (0, 1, 2)\n\n# create a data frame for easier viewing\ndf_iris_simple = pd.DataFrame(X, columns = iris.feature_names)\n\ndf_iris_simple['species'] = iris.target\ndf_iris_simple['species_name'] = df_iris_simple['species'].map( {0:'setosa', 1:'versicolor', 2:'virginica'} )\n\n# display basic information\nprint(df_iris_simple.head)\n\n# scatter plots\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c = iris.target, cmap='viridis')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.title('Sepal length vs. Sepal width')\nplt.colorbar(label='species')\nplt.show()\n\n\n# Run t-SNE to reduce the data to 2 dimensions\ntsne = TSNE(n_components=2, random_state=0, perplexity=30)\nX_2d = tsne.fit_transform(X)\n\n# Plot the results (simple plot)\nplt.figure()\nplt.scatter(X_2d[:,0], X_2d[:,1], c = y) # color by spcies labels\nplt.show()\n\n# Plot the results, one species at a time\nplt.figure()\n# Setosa (label 0)\nidx_y_0 = (y == 0) # get index of those flowers where y == 0\nplt.scatter(X_2d[idx_y_0,0], X_2d[idx_y_0,1], color='red', label='setosa')\n\n# Versicolor (label 1)\nidx_y_1 = (y == 1) # get index of those flowers where y == 1\nplt.scatter(X_2d[idx_y_1,0], X_2d[idx_y_1,1], color='green', label='versicolor')\n\n# Virginica (label 2)\nidx_y_2 = (y == 2) # get indices of when y is 2\nplt.scatter(X_2d[idx_y_2,0], X_2d[idx_y_2,1], color='blue', label='virginica')\n\nplt.xlabel(\"t-SNE feature 1\")\nplt.ylabel(\"t-SNE feature 2\")\nplt.title(\"t-SNE visualization of the Iris dataset\")\nplt.legend()\nplt.show()\n\n&lt;bound method NDFrame.head of      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     species species_name  \n0          0       setosa  \n1          0       setosa  \n2          0       setosa  \n3          0       setosa  \n4          0       setosa  \n..       ...          ...  \n145        2    virginica  \n146        2    virginica  \n147        2    virginica  \n148        2    virginica  \n149        2    virginica  \n\n[150 rows x 6 columns]&gt;",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#exercise-tsne-is-stochastic",
    "href": "materials/tsne.html#exercise-tsne-is-stochastic",
    "title": "9¬† tSNE",
    "section": "9.20 Exercise: tSNE is stochastic",
    "text": "9.20 Exercise: tSNE is stochastic\n\nStochasticity: play around with the random_state parameter. Does your tSNE plot look different to the person you are seated next to?\nPlay around with the perplexity parameter (pair up with someone)\nWhich value of perplexity should you use?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#key-concept-1",
    "href": "materials/tsne.html#key-concept-1",
    "title": "9¬† tSNE",
    "section": "9.21 Key Concept",
    "text": "9.21 Key Concept\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nRecall that unsupervised machine learning can help you come up with new hypotheses\nVary the perplexity parameter: ideally your patterns or hypotheses should be true even if you change perplexity",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#perplexity-pitfalls-and-things-to-watch-out-for",
    "href": "materials/tsne.html#perplexity-pitfalls-and-things-to-watch-out-for",
    "title": "9¬† tSNE",
    "section": "9.22 Perplexity pitfalls and things to watch out for",
    "text": "9.22 Perplexity pitfalls and things to watch out for",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#exercise-hands-on-practical-applying-tsne-to-another-dataset",
    "href": "materials/tsne.html#exercise-hands-on-practical-applying-tsne-to-another-dataset",
    "title": "9¬† tSNE",
    "section": "9.23 Exercise: Hands-on practical applying tSNE to another dataset",
    "text": "9.23 Exercise: Hands-on practical applying tSNE to another dataset\n\nWork in a group.\nLoad the US Arrests data in Python and perform tSNE on this (pair up with a person)\nSome code to help you get started is here:\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n# Load the US Arrests data\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\ndf = pd.read_csv(url, index_col=0)\n\n# Prepare the data for t-SNE\nX = df.values  \n\n# Fill in your code here ..........\n\n\nHow would you evaluate this?\nVary the perplexity parameter\n\n\n\nAlso annotate the plot by US states. Hint: Use the plt.annotate() function. The US states names are available in df.index.\nIs California still far apart from Vermont even if you change the perplexity parameter?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#other-algorithms",
    "href": "materials/tsne.html#other-algorithms",
    "title": "9¬† tSNE",
    "section": "9.24 Other algorithms",
    "text": "9.24 Other algorithms\nWe have given a brief overview of some unsupervised machine learning techniques. There are many others. For example, you can also read about UMAP",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#summary",
    "href": "materials/tsne.html#summary",
    "title": "9¬† tSNE",
    "section": "9.25 Summary",
    "text": "9.25 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nHigh dimensions make global distances meaningless.\nMethods that leverage local structure (t‚ÄëSNE) can still find patterns.\ntSNE is stochastic and can be hard to interpret\nVary the perplexity parameter (ideally your patterns or hypotheses should be true even if you change perplexity)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/tsne.html#references",
    "href": "materials/tsne.html#references",
    "title": "9¬† tSNE",
    "section": "9.26 References",
    "text": "9.26 References\n[1] How to Use t-SNE Effectively\n[2] FAQs by the creator of tSNE\n[3] Other intricacies of tSNE\n\n\n\n\nWattenberg, Martin, Fernanda Vi√©gas, and Ian Johnson. 2017. ‚ÄúHow to Use t-SNE Effectively.‚Äù Distill 1 (October): e2. https://doi.org/10.23915/distill.00002.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 2: Dimensionality Reduction",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>tSNE</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html",
    "href": "materials/clustering.html",
    "title": "10¬† Clustering",
    "section": "",
    "text": "10.1 Warm-up Puzzle",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#warm-up-puzzle",
    "href": "materials/clustering.html#warm-up-puzzle",
    "title": "10¬† Clustering",
    "section": "",
    "text": "Is the picture below fake or real?\nHow can a computer determine if the picture below is fake or real?\n\n\n\n\nTaj Mahal bathed in the Northern Lights. Generated using the DALL-E tool.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#clustering-overview",
    "href": "materials/clustering.html#clustering-overview",
    "title": "10¬† Clustering",
    "section": "10.2 Clustering Overview",
    "text": "10.2 Clustering Overview\nClustering is an unsupervised learning technique used to group similar data points together. Unlike classification, there are no pre-defined labels. Instead, the algorithm tries to discover structure in the data by maximizing intra-cluster similarity and minimizing inter-cluster similarity.\nKey points:\n\nObjective: Identify natural groupings in the data.\nApplications: Customer segmentation, image compression, anomaly detection, document clustering.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#kmeans-clustering",
    "href": "materials/clustering.html#kmeans-clustering",
    "title": "10¬† Clustering",
    "section": "10.3 K‚ÄëMeans Clustering",
    "text": "10.3 K‚ÄëMeans Clustering\nK‚ÄëMeans is a clustering algorithm that aims to partition the data into K disjoint clusters.\n\n10.3.1 Algorithm Steps\n\nChoose K, the number of clusters.\n\nInitialization: Randomly select K initial centroids (or use k‚Äëmeans++ for better seeding).\n\nAssignment Step:\nfor each data point x_i:\n    assign x_i to cluster j whose centroid Œº_j is nearest (minimize ||x_i - Œº_j||¬≤)\nUpdate Step:\nfor each cluster j:\n    Œº_j = (1 / |C_j|) * sum_{x_i in C_j} x_i\nConvergence Check:\n\nStop when assignments no longer change, OR\nThe change in centroids is below a threshold, OR\nA maximum number of iterations is reached.\n\n\n\n\n10.3.2 Animation\n\n\n\nAnimation of k-means\n\n\n\n\n\n10.3.3 Within‚ÄìCluster Variation\nIn \\(K\\)‚Äëmeans clustering, we partition our \\(n\\) observations into \\(K\\) disjoint clusters \\(\\{C_1, C_2, \\dots, C_K\\}\\). A ‚Äúgood‚Äù clustering is one for which the within‚Äëcluster variation is minimized.\n\n\n10.3.4 Elbow point\nWhen using k‚Äëmeans clustering, one of the key questions is: how many clusters (k) should I choose? The elbow method is a simple, visual way to pick a reasonable k by looking at how the ‚Äúwithin‚Äëcluster‚Äù variation decreases as k increases.\n\n\n1. The Within‚ÄëCluster Sum of Squares (WCSS)\nFor each choice of k, you run k‚Äëmeans and compute the within‚Äëcluster sum of squares (WCSS), also called inertia or distortion. This is the sum of squared Euclidean distances between each point and the centroid of its cluster:\n\n\n\nWCSS\n\n\n\n\n\\(C_{i}\\) is cluster i\n\n\\(mu_{i}\\) is the centroid of cluster i\n\nAs k increases, WCSS will always decrease (or stay the same), because more centroids can only reduce distances.\n\n\n2. Plotting WCSS versus k\n\nChoose a range for k (e.g.¬†1 to 10).\n\nFor each k, fit k‚Äëmeans and record WCSS(k).\n\nPlot WCSS(k) on the y-axis against k on the x-axis.\n\nYou will get a curve that starts high at k = 1 and steadily goes down as k increases.\n\n\n\n3. Identifying the ‚ÄúElbow‚Äù\n\nAt first, adding clusters dramatically reduces WCSS, because you are splitting large, heterogeneous clusters into more homogeneous groups.\n\nAfter some point, adding more clusters yields diminishing returns‚Äîeach new cluster only slightly reduces WCSS.\n\nThe elbow point is the value of k at which the decrease in WCSS ‚Äúbends‚Äù most sharply: like an elbow in your arm. It balances model complexity (more clusters) against improved fit (lower WCSS).\n\n\n\nAn elbow point\n\n\n\n\n\n10.3.5 Exercise (k-means)\n\nNOTE: The c parameter in plt.scatter() is used to specify the color of the scatter plot points.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# create a data frame for easier viewing\ndf_iris_simple = pd.DataFrame(X, columns = iris.feature_names)\n\ndf_iris_simple['species'] = iris.target\ndf_iris_simple['species_name'] = df_iris_simple['species'].map( {0:'setosa', 1:'versicolor', 2:'virginica'} )\n\n# display basic information\nprint(df_iris_simple.head)\n\n# scatter plots\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c = iris.target, cmap='viridis')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.title('Sepal length vs. Sepal width')\nplt.colorbar(label='species')\nplt.show()\n\n&lt;bound method NDFrame.head of      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     species species_name  \n0          0       setosa  \n1          0       setosa  \n2          0       setosa  \n3          0       setosa  \n4          0       setosa  \n..       ...          ...  \n145        2    virginica  \n146        2    virginica  \n147        2    virginica  \n148        2    virginica  \n149        2    virginica  \n\n[150 rows x 6 columns]&gt;\n\n\n\n\n\n\n\n\n\n\n# Perform k means with k = 2\nkmeans = KMeans(n_clusters=2, random_state=2, n_init=20)\nkmeans.fit(X)\n\n# The cluster assignments of the observations are contained in kmeans.labels_\nkmeans.labels_\n\n# Plot the data, with each observation colored according to its cluster assignment.\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\nplt.title(\"K-means on Iris data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.3.6 Exercise (Evaluate k-means clusters using the within-cluster similarity)\n\nFind the optimal value of the number of clusters \\(K\\)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# Plot WCSS (inertia) as a function of the number of clusters\nwcss = []\n\n# try a few different values of k\nk_range = range(1,11)\n\nfor k_var in k_range:\n\n   # fit kmeans\n   kmeans = KMeans(n_clusters=k_var, random_state=2)\n   kmeans.fit(X)\n\n   # append WCSS to a list\n   wcss.append(kmeans.inertia_)\n\n\n# plot\nplt.figure()\nplt.scatter( k_range , wcss )\nplt.xlabel('Number of clusters')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('Elbow Method For Optimal')\nplt.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#hierarchical-clustering",
    "href": "materials/clustering.html#hierarchical-clustering",
    "title": "10¬† Clustering",
    "section": "10.4 Hierarchical Clustering",
    "text": "10.4 Hierarchical Clustering\nHierarchical clustering builds a tree (dendrogram) of clusters using either a bottom‚Äëup (agglomerative) or top‚Äëdown (divisive) approach.\n\n10.4.1 Agglomerative (Bottom‚ÄëUp)\n\nInitialization: Start with each data point as its own cluster.\nMerge Steps:\n\nCompute distance between every pair of clusters.\n\nMerge the two closest clusters.\n\nUpdate the distance matrix.\n\nTermination: Repeat until all points are in a single cluster or a stopping criterion (e.g., desired number of clusters) is met.\n\n\n\n10.4.2 Dendrogram\n        [ALL POINTS]\n         /      \\\n    Cluster A   Cluster B\n     /    \\       /    \\\n    ‚Ä¶      ‚Ä¶     ‚Ä¶      ‚Ä¶\n\nCutting the tree at different levels yields different numbers of clusters.\nLinkage methods determine how distance between clusters is computed:\n\nSingle linkage: Minimum pairwise distance\n\nComplete linkage: Maximum pairwise distance\n\nAverage linkage: Average pairwise distance\n\n\n\n\n10.4.3 Important Concepts\n\n\n\n\n\n\nTip\n\n\n\n\nMetric\nThe metric (or distance function or dissimilarity function) defines how you measure the distance between individual data points. Common choices include Euclidean, Manhattan (cityblock), or cosine distance. This metric determines the raw pairwise distances.\n\nManhattan distance\n\n\n\nManhattan distance. Image created using DALL-E.\n\n\nManhattan distance\n\n\n\nManhattan distance\n\n\nEuclidean distance\nHow would a crow navigate in Manhattan? (I have never been to Manhattan, but the internet says there are crows in Manhattan, so it must be true).\n\n\n\nA crow in Manhattan. Image created using DreamUp.\n\n\n\n\n\nEuclidean distance\n\n\n\nLinkage\nThe linkage method defines how to compute the distance between two clusters based on the pairwise distances of their members. Examples:\n\nSingle: the distance between the closest pair of points (one from each cluster).\n\nComplete: the distance between the farthest pair of points.\n\nAverage: the average of all pairwise distances.\n\nWard: the merge that minimizes the increase in total within‚Äëcluster variance.\n\n\nLinkage function\n\n\n\nLinkage function\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinkage Method\nHow It Works\nIntuition\n\n\n\n\nSingle\nDistance = minimum pairwise distance between points in the two clusters\n‚ÄúFriends‚Äëof‚Äëfriends‚Äù ‚Äì clusters join if any two points are close, yielding chain‚Äëlike clusters\n\n\nComplete\nDistance = maximum pairwise distance between points in the two clusters\n‚ÄúEveryone must be close‚Äù ‚Äì only merge when all points are relatively near, producing compact clusters\n\n\nAverage (UPGMA)\nDistance = average of all pairwise distances between points in the two clusters\nBalances single and complete by averaging close and far pairs\n\n\nWeighted (WPGMA)\nDistance = average of the previous cluster‚Äôs distance to the new cluster (equal weight per cluster)\nPrevents large clusters from dominating, giving equal say to each cluster\n\n\nCentroid\nDistance = distance between the centroids (mean vectors) of the two clusters\nMerges based on ‚Äúcenters of mass,‚Äù but centroids can shift non‚Äëmonotonically\n\n\nMedian (WPGMC)\nDistance = distance between the medians of the two clusters\nMore robust to outliers than centroid linkage, but can also invert dendrogram order\n\n\nWard‚Äôs\nMerge that minimizes the increase in total within‚Äëcluster sum of squares (variance)\nKeeps clusters as tight and homogeneous as possible, often resulting in evenly sized groups\n\n\n\n\n\n10.4.4 Single Linkage\n\nHow it works: Measures the distance between two clusters as the smallest distance between any single point in one cluster and any single point in the other.\n\nIntuition: ‚ÄúFriends‚Äëof‚Äëfriends‚Äù clustering‚Äîif any two points (one from each cluster) are close, the clusters join. Can produce long, straggly chains of points.\n\n\n\n10.4.5 Complete Linkage\n\nHow it works: Measures the distance between two clusters as the largest distance between any point in one cluster and any point in the other.\n\nIntuition: ‚ÄúEveryone must be close‚Äù‚Äîclusters merge only when all their points are relatively near each other, leading to tight, compact groups.\n\n\n\n10.4.6 Average Linkage (UPGMA)\n\nHow it works: Takes the average of all pairwise distances between points in the two clusters.\n\nIntuition: A middle‚Äëground between single and complete linkage‚Äîbalances the effect of very close and very far pairs by averaging them.\n\n\n\n10.4.7 Weighted Linkage (WPGMA)\n\nHow it works: Similar to average linkage, but treats each cluster as a single entity by averaging the distance from each original cluster to the target cluster, regardless of cluster size.\n\nIntuition: Prevents larger clusters from dominating the average‚Äîgives each cluster equal say in how far apart they are.\n\n\n\n10.4.8 Centroid Linkage\n\nHow it works: Computes the distance between the centroids (mean vectors) of the two clusters.\n\nIntuition: Clusters merge based on whether their ‚Äúcenters of mass‚Äù are close. Can sometimes lead to non‚Äëmonotonic merges if centroids shift oddly.\n\n\n\n10.4.9 Median Linkage (WPGMC)\n\nHow it works: Uses the median point of each cluster instead of the mean when computing distance between clusters.\n\nIntuition: Like centroid linkage but more robust to outliers, since the median isn‚Äôt pulled by extreme values‚Äîthough can also cause inversion issues.\n\n\n\n10.4.10 Ward‚Äôs Method\n\nHow it works: At each step, merges the two clusters whose union leads to the smallest possible increase in total within‚Äëcluster variance (sum of squared deviations).\n\nIntuition: Always chooses the merge that keeps clusters as tight and homogeneous as possible, often yielding groups of similar size and shape.\n\n\n\n\n\n\n\nTipConcept about distances\n\n\n\nThere is no single ‚Äúbest‚Äù distance metric for clustering‚Äîwhat works well for one dataset or problem may not work for another. The choice of distance metric (such as Euclidean, or Manhattan) depends on the nature of your data and what you want to capture about similarity.\nFor example, Euclidean distance works well when the scale of features is meaningful and differences are linear, while cosine distance is better for text data or situations where the direction of the data matters more than its magnitude.\nIt is important to experiment with different distance metrics and see which one produces clusters that make sense for your specific problem. Always check the results and, if possible, use domain knowledge to guide your choice.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#practical",
    "href": "materials/clustering.html#practical",
    "title": "10¬† Clustering",
    "section": "10.5 Practical",
    "text": "10.5 Practical\n\nn_clusters in AgglomerativeClustering specifies the number of clusters you want the algorithm to find. After building the hierarchical tree, the algorithm will cut the tree so that exactly n_clusters groups are formed. For example, n_clusters=3 will result in 3 clusters in your data.\nThe default value for n_clusters in AgglomerativeClustering is 2. The default value for linkage is ward. So if you do not specify these parameters, the algorithm will produce 2 clusters using Ward linkage.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# Fit Agglomerative Clustering\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels = agg.fit_predict(X)\n\nprint(labels)  # Cluster assignments for each sample\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Compute linkage matrix\nZ = linkage(X, method='ward')\n\n# Plot dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 0 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\n\n\n\n\n\n\n\n\n\n\n10.5.1 Scale the data\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# let us see what is in the data\ndf_iris = load_iris(as_frame=True)\ndata_frame_iris = df_iris.frame\nprint(data_frame_iris)\n#print(df_iris.frame.head())\n\n# scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     target  \n0         0  \n1         0  \n2         0  \n3         0  \n4         0  \n..      ...  \n145       2  \n146       2  \n147       2  \n148       2  \n149       2  \n\n[150 rows x 5 columns]\n\n\n\n\n10.5.2 Perform hierarchical clustering\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\n\n# Fit Agglomerative Clustering\nagg = AgglomerativeClustering(n_clusters=3)#, linkage='ward')\nlabels = agg.fit_predict(X_scaled)\n\nprint(labels)  # Cluster assignments for each sample\n\n# Compute linkage matrix\nZ = linkage(X_scaled)#, method='ward')\n\n# Plot dendrogram\nplt.figure()\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 2 1 1 1 1 1 1 1 1 0 0 0 2 0 2 0 2 0 2 2 0 2 0 2 0 2 2 2 2 0 0 0 0\n 0 0 0 0 0 2 2 2 2 0 2 0 0 2 2 2 2 0 2 2 2 2 2 0 2 2 0 0 0 0 0 0 2 0 0 0 0\n 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#alternative-code-using-sns.clustermap",
    "href": "materials/clustering.html#alternative-code-using-sns.clustermap",
    "title": "10¬† Clustering",
    "section": "10.6 Alternative code using sns.clustermap",
    "text": "10.6 Alternative code using sns.clustermap\nThere are many ways we can perform hierarchical clustering. An alternative is to use sns.clustermap using the seaborn package (see here). The interface is similar and it can produce professional quality plots.\n\nimport seaborn as sns\n\n# Basic clustermap with scaling\nsns.clustermap(X, cmap='RdBu_r', z_score=0, center=0)\n\n# Different linkage methods\nsns.clustermap(X, method='average', z_score=0, center = 0)\n\n# Different distance metrics  \nsns.clustermap(X, metric='correlation', method='average', z_score=0, center = 0)\n\n# Comprehensive example\nsns.clustermap(X, method='average', metric='correlation', \n               cmap='RdBu_r', z_score=0, center=0)\n\n# or if you prefer just the default options\nsns.clustermap(X, z_score=0, center=0)\n\nz_score=0 (Scaling Direction) - What it does: Standardizes (z-scores) the data before clustering - Options: - 0: Scale rows (genes) - each gene‚Äôs expression is standardized across samples - 1: Scale columns (samples) - each sample‚Äôs expression is standardized across genes - None: No scaling (use raw data) - Why use 0: For gene expression, you want to compare expression patterns, not absolute levels\ncmap='RdBu_r' (Color Map) - What it does: Defines the color scheme for the heatmap - 'RdBu_r': Red-Blue reversed (red = high, blue = low, white = middle) - Other options: 'viridis', 'coolwarm', 'seismic', 'plasma', etc. - Why use it: Intuitive for biologists (red = high expression, blue = low expression)\ncenter=0 (Color Center) - What it does: Centers the color map at this value - 0: White color represents zero (after scaling, this is the mean) - Other values: Could center at 1 (for fold-change), or other biologically meaningful values - Why use it: Makes it easy to see above/below average expression\nAdditional Common Parameters\n\n\nrow_cluster=True/False\n\nWhat it does: Whether to cluster rows (genes)\nDefault: True\n\n\n\ncol_cluster=True/False\n\nWhat it does: Whether to cluster columns (samples)\nDefault: True\n\n\n\ncbar_kws (Color Bar Keywords)\n\nWhat it does: Customize the color bar\nExample: cbar_kws={'label': 'Expression Level', 'shrink': 0.8}\nHere is some code to use sns.clustermap\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.clustermap(data_frame_iris, z_score=0, center=0)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#exercise-changing-the-linkage-function",
    "href": "materials/clustering.html#exercise-changing-the-linkage-function",
    "title": "10¬† Clustering",
    "section": "10.7 Exercise (changing the linkage function)",
    "text": "10.7 Exercise (changing the linkage function)\n\nWork in a group for this exercise\nLet us try another linkage function\nChange the linkage function in Z = linkage(X_scaled), method='ward')\nHow does the clustering change as you change this to another function?\nHow does this change if you do not scale the data?\n\n\n\n\n\n\n\nNoteClick to expand\n\n\n\n\n\n\n# Fit Agglomerative Clustering on unscaled data with 'average' linkage\nagg = AgglomerativeClustering(linkage='average')\nlabels = agg.fit_predict(X)\n\n# Compute linkage matrix on unscaled data with 'average' linkage\nZ = linkage(X, method='average')\n\n# Plot dendrogram\nplt.figure()\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data, Unscaled, Average Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#exercise-trying-a-different-dissimilarity-metric",
    "href": "materials/clustering.html#exercise-trying-a-different-dissimilarity-metric",
    "title": "10¬† Clustering",
    "section": "10.8 Exercise (trying a different dissimilarity metric)",
    "text": "10.8 Exercise (trying a different dissimilarity metric)\n\n# Fit Agglomerative Clustering on unscaled data with 'average' linkage and 'manhattan' distance\nagg = AgglomerativeClustering(\n    linkage='average',\n    metric='manhattan'      # use metric instead of deprecated affinity\n)\n\n\n# Compute linkage matrix on unscaled data with 'average' linkage and 'cityblock' (manhattan) distance\nZ = linkage(X, method='average', metric='cityblock')\n\n# Plot dendrogram\nplt.figure()\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data, Unscaled, Average Linkage, Manhattan Distance)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#theoretical-exercise-distance-functions-and-the-thin-line-between-unsupervised-and-supervised-machine-learning",
    "href": "materials/clustering.html#theoretical-exercise-distance-functions-and-the-thin-line-between-unsupervised-and-supervised-machine-learning",
    "title": "10¬† Clustering",
    "section": "10.9 Theoretical exercise: distance functions and the thin line between unsupervised and supervised machine learning",
    "text": "10.9 Theoretical exercise: distance functions and the thin line between unsupervised and supervised machine learning\n\nWe often think the choice between supervised and unsupervised machine learning is obvious: ‚Äúif you have labels, use supervised; otherwise use unsupervised‚Äù. In practice the decision can be subtle and depends on the problem framing, available data, and what constitutes a useful output.\nThis also impacts the choice of distance metric to use. We use whatever distance metric allows us to get the job done.\nFor each scenario: state your choice (supervised / unsupervised / hybrid)\nScenario 1: Discovering shopper archetypes: A marketing team wants to segment shoppers into behavioural archetypes to send them targetted ads/promotions through loyalty cards. Is this an unsupervised or supervised machine learning problem? What distance function would you use here?\n\n\n\n\n\n\n\n\n\n\n\nScenario 2: Detecting fraud / unusual shopping sessions: A small fraction of sessions are flagged as fraudulent (labels exist for some known fraud cases). The team wants to detect new, unseen fraud patterns.\nScenario 3: You have just started working for an exciting startup. StreamVision is a video streaming platform with 500 million subscribers worldwide. The platform hosts over 1500000 movies and TV shows across various genres. The company has noticed that users often struggle to find content they enjoy, leading to decreased engagement and higher churn rates. The content team has collected interaction data and wants to understand viewing patterns to improve the user experience.\n\nStreamVision‚Äôs executive team has set the following objectives:\n\nIncrease user engagement: Help users discover content they‚Äôre likely to enjoy\nReduce decision fatigue: Minimize the time users spend browsing without watching\nOptimize content acquisition: Understand what types of content resonate with different user segments\nPersonalize the experience: Create tailored homepages for different user groups\n\nThe analytics team has prepared two datasets for analysis:\n\nDataset 1: User Viewing History (Sample)\nThis dataset contains information about user interactions with content over the past 6 months. Each row represents viewing activity aggregated by user and content item.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUser_ID\nContent_ID\nWatch_Time_Minutes\nCompletion_Rate\nRating_Given\nRewatch_Count\nDays_Since_Release\nUser_Tenure_Days\n\n\n\n\nU10023\nC4521\n127\n0.98\n5\n0\n456\n892\n\n\nU10023\nC7834\n45\n0.35\nNULL\n0\n12\n892\n\n\nU10023\nC2109\n156\n1.00\n5\n2\n1203\n892\n\n\nU20441\nC4521\n18\n0.14\n2\n0\n456\n234\n\n\nU20441\nC8821\n201\n0.95\n4\n0\n89\n234\n\n\nU31205\nC7834\n121\n0.94\n5\n1\n12\n1456\n\n\nU31205\nC2109\n162\n1.00\n5\n0\n1203\n1456\n\n\nU31205\nC9012\n89\n0.88\n4\n0\n678\n1456\n\n\n\nField Descriptions: - User_ID: Unique identifier for each subscriber - Content_ID: Unique identifier for each movie/show - Watch_Time_Minutes: Total time user spent watching this content - Completion_Rate: Percentage of content watched (0-1 scale) - Rating_Given: Explicit rating (1-5 stars, NULL if not rated) - Rewatch_Count: Number of times user has rewatched this content - Days_Since_Release: How many days ago the content was added to platform - User_Tenure_Days: How long the user has been subscribed\n\n\nDataset 2: Content Metadata (Sample)\nThis dataset describes characteristics of each piece of content in the StreamVision library.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent_ID\nTitle\nPrimary_Genre\nSecondary_Genre\nRuntime_Minutes\nRelease_Year\nIMDB_Score\nLanguage\nDirector_Style_Code\nLead_Actor_Popularity\n\n\n\n\nC4521\nMidnight Runner\nAction\nThriller\n132\n2019\n7.2\nEnglish\nD_034\n8.1\n\n\nC7834\nThe Quiet Garden\nDrama\nRomance\n128\n2024\n6.8\nEnglish\nD_102\n6.9\n\n\nC2109\nStarship Protocol\nSci-Fi\nAction\n156\n2017\n8.4\nEnglish\nD_034\n9.2\n\n\nC8821\nVoices Within\nDocumentary\nBiography\n212\n2023\n8.9\nEnglish\nD_201\n7.5\n\n\nC9012\nSummer of ‚Äô82\nComedy\nDrama\n101\n2020\n7.6\nEnglish\nD_178\n7.8\n\n\n\nField Descriptions: - Content_ID: Unique identifier matching the viewing history dataset - Title: Name of the movie/show - Primary_Genre: Main genre classification - Secondary_Genre: Additional genre classification - Runtime_Minutes: Total length of content - Release_Year: Year the content was originally released - IMDB_Score: External quality metric (1-10 scale) - Language: Primary language of the content - Director_Style_Code: Encoded style signature of the director - Lead_Actor_Popularity: Popularity metric of main actor (1-10 scale)\n\n\n10.9.1 Analysis Questions\nThe product team has posed several questions they‚Äôd like you to explore:\n\nUser Segmentation: Are there natural groups of users with similar viewing preferences? How would you characterize these groups?\nContent Discovery: How can we identify which content items are similar to each other, even if they don‚Äôt share obvious genre tags?\nCold Start Problem: For new users with limited viewing history, how might we determine which user segment they belong to based on their first few viewing choices?\nContent Gaps: Are there user segments that might be underserved by the current content library?\nPersonalization Strategy: Given these insights, what approach would you recommend for creating personalized homepages?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#choosing-between-methods",
    "href": "materials/clustering.html#choosing-between-methods",
    "title": "10¬† Clustering",
    "section": "10.10 Choosing Between Methods",
    "text": "10.10 Choosing Between Methods\n\n10.10.1 Hierarchical Clustering\n\nNo need to pre-specify number of clusters (can decide by cutting dendrogram).\nProduces a full hierarchy of clusters.\n\n\n\n10.10.2 K‚ÄëMeans\n\nRequires pre-specifying \\(K\\).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#summary",
    "href": "materials/clustering.html#summary",
    "title": "10¬† Clustering",
    "section": "10.11 Summary",
    "text": "10.11 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nHierarchical clustering builds a tree-like structure (dendrogram) to group similar data points\n\nDistance Metrics How we measure similarity between points:\nEuclidean distance (straight-line distance)\nManhattan distance (city-block distance)\nCosine distance (for directional data)\n\nLinkage Methods\n\nHow we measure distance between clusters:\nSingle: Uses closest pair of points (can create chains)\nComplete: Uses farthest pair of points (creates compact clusters)\nAverage: Uses average of all pairwise distances (balanced approach)\nWard‚Äôs: Minimizes increase in variance (creates similar-sized clusters)\n\nDendrogram\n\nVisual representation showing how clusters merge:\nHeight shows distance when clusters merged\nCutting at different heights gives different numbers of clusters\n\nKey Code Patterns:\n\nimport seaborn as sns\n\nsns.clustermap(X, method='average', metric='correlation', z_score = 0, center=0)\n# Basic hierarchical clustering\nfrom sklearn.cluster import AgglomerativeClustering\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels = agg.fit_predict(X)\n\n# Create dendrogram\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nZ = linkage(X, method='ward')\ndendrogram(Z)\n\nImportant Takeaways\nNo ‚ÄúCorrect‚Äù Answer\n\nUnsupervised learning requires interpretation and domain knowledge\nMultiple Methods - Try different linkage methods and distance metrics\nEvaluation is Key - Use both internal (silhouette) and external (ARI) metrics. See the next chapter for more on evaluation.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/clustering.html#further-reading",
    "href": "materials/clustering.html#further-reading",
    "title": "10¬† Clustering",
    "section": "10.12 Further Reading",
    "text": "10.12 Further Reading\n\nIntroduction to Statistical Learning in Python (ISLP)\nIPython notebook from ISLP book\nScikit‚Äëlearn documentation:\n\nAgglomerativeClustering\nKMeans\n\nDocumentation of sns.clustermap",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "materials/missing_data.html",
    "href": "materials/missing_data.html",
    "title": "11¬† Missing values in unsupervised machine learning",
    "section": "",
    "text": "11.1 Exercise with missing data\nReal-world data frequently has missing values. PCA and clustering techniques can struggle on missing data.\nIn this exercise, you will work in a group and apply hierarchical clustering, PCA and tSNE on data which has missing values.\nRun the code below. All the missing data will be available in the variable missing_data.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ndef create_synthetic_biological_data():\n    \"\"\"\n    Create synthetic gene expression data with biological structure and missingness.\n    \n    Returns:\n    - complete_data: Original data without missing values\n    - missing_data: Data with various missingness patterns\n    - true_labels: True cluster labels for evaluation\n    \"\"\"\n    #print(\"Creating synthetic biological dataset...\")\n    \n    # Parameters\n    n_samples = 100\n    n_genes = 50\n    n_clusters = 4\n    \n    # Create base data structure\n    data = np.random.normal(0, 1, (n_samples, n_genes))\n    \n    # Add biological structure (clusters)\n    cluster_size = n_samples // n_clusters\n    \n    # Cluster 1: High expression in genes 0-12, samples 0-24\n    data[0:25, 0:13] += 2.5\n    # Cluster 2: High expression in genes 13-25, samples 25-49  \n    data[25:50, 13:26] += 2.0\n    # Cluster 3: High expression in genes 26-37, samples 50-74\n    data[50:75, 26:38] += 1.8\n    # Cluster 4: Low expression in genes 38-49, samples 75-99\n    data[75:100, 38:50] -= 2.2\n    \n    # Add some noise\n    data += np.random.normal(0, 0.5, data.shape)\n    \n    # Create sample and gene names\n    sample_names = [f'Sample_{i:03d}' for i in range(n_samples)]\n    gene_names = [f'Gene_{chr(65+i//26)}{chr(65+i%26)}' for i in range(n_genes)]\n    \n    # Create DataFrame\n    complete_data = pd.DataFrame(data, index=sample_names, columns=gene_names)\n    \n    # Create true cluster labels\n    true_labels = np.repeat(range(n_clusters), cluster_size)\n    if len(true_labels) &lt; n_samples:\n        true_labels = np.append(true_labels, [n_clusters-1] * (n_samples - len(true_labels)))\n    \n    #print(f\"Created dataset: {complete_data.shape[0]} samples √ó {complete_data.shape[1]} genes\")\n    #print(f\"True clusters: {n_clusters}\")\n    \n    return complete_data, true_labels\n\ndef introduce_missing_data_patterns(complete_data, true_labels):\n    \"\"\"\n    Introduce different types of missing data patterns.\n    \n    Parameters:\n    - complete_data: Original complete dataset\n    - true_labels: True cluster labels\n    \n    Returns:\n    - missing_data: Dataset with missing values\n    - missing_info: Information about missingness patterns\n    \"\"\"\n    #print(\"\\nIntroducing missing data patterns...\")\n    \n    missing_data = complete_data.copy()\n    missing_info = {}\n    \n    # Pattern 1: Missing Completely At Random (MCAR) - 5% random missing\n    #print(\"1. Adding MCAR missingness (5% random)...\")\n    mcar_mask = np.random.random(missing_data.shape) &lt; 0.05\n    missing_data[mcar_mask] = np.nan\n    missing_info['MCAR'] = mcar_mask.sum()\n    \n    # Pattern 2: Missing At Random (MAR) - correlated with expression level\n    #print(\"2. Adding MAR missingness (correlated with high expression)...\")\n    # Higher chance of missing for high expression values\n    high_expr_mask = missing_data &gt; missing_data.quantile(0.8)\n    mar_probability = np.where(high_expr_mask, 0.15, 0.02)  # 15% for high, 2% for low\n    mar_mask = np.random.random(missing_data.shape) &lt; mar_probability\n    missing_data[mar_mask] = np.nan\n    missing_info['MAR'] = mar_mask.sum()\n    \n    # Pattern 3: Missing Not At Random (MNAR) - systematic missing\n    #print(\"3. Adding MNAR missingness (systematic missing)...\")\n    # Missing entire samples (simulating failed experiments)\n    failed_samples = np.random.choice(missing_data.index, size=8, replace=False)\n    missing_data.loc[failed_samples, :] = np.nan\n    missing_info['MNAR_samples'] = len(failed_samples)\n    \n    # Missing entire genes (simulating detection failures)\n    failed_genes = np.random.choice(missing_data.columns, size=5, replace=False)\n    missing_data.loc[:, failed_genes] = np.nan\n    missing_info['MNAR_genes'] = len(failed_genes)\n    \n    # Pattern 4: Block missingness (simulating batch effects)\n    #print(\"4. Adding block missingness (batch effects)...\")\n    # Missing blocks of data (simulating different experimental conditions)\n    block_start_row = 20\n    block_end_row = 35\n    block_start_col = 10\n    block_end_col = 20\n    missing_data.iloc[block_start_row:block_end_row, block_start_col:block_end_col] = np.nan\n    missing_info['Block'] = (block_end_row - block_start_row) * (block_end_col - block_start_col)\n    \n    # Calculate total missingness\n    total_missing = missing_data.isnull().sum().sum()\n    total_values = missing_data.size\n    missing_percentage = (total_missing / total_values) * 100\n    \n    print(f\"\\nMissing data summary:\")\n    print(f\"Total missing values: {total_missing}\")\n    print(f\"Missing percentage: {missing_percentage:.1f}%\")\n    #print(f\"MCAR: {missing_info['MCAR']} values\")\n    #print(f\"MAR: {missing_info['MAR']} values\") \n    #print(f\"MNAR samples: {missing_info['MNAR_samples']} samples\")\n    #print(f\"MNAR genes: {missing_info['MNAR_genes']} genes\")\n    #print(f\"Block missing: {missing_info['Block']} values\")\n    \n    return missing_data, missing_info\n\ndef visualize_missing_patterns(complete_data, missing_data, true_labels):\n    \"\"\"\n    Visualize the missing data patterns.\n    \"\"\"\n    print(\"\\nCreating missing data visualizations...\")\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Plot 1: Complete data heatmap\n    plt.subplot(2, 2, 1)\n    sns.heatmap(complete_data.iloc[:50, :30], cmap='RdBu_r', center=0, \n                cbar_kws={'label': 'Expression Level'})\n    plt.title('Complete Data (First 50 samples, 30 genes)')\n    plt.xlabel('Genes')\n    plt.ylabel('Samples')\n    \n    # Plot 2: Missing data pattern\n    plt.subplot(2, 2, 2)\n    missing_mask = missing_data.iloc[:50, :30].isnull()\n    sns.heatmap(missing_mask, cmap='Reds', cbar_kws={'label': 'Missing (1=Yes, 0=No)'})\n    plt.title('Missing Data Pattern')\n    plt.xlabel('Genes')\n    plt.ylabel('Samples')\n    \n    # Plot 3: Missing data by sample\n    plt.subplot(2, 2, 3)\n    missing_by_sample = missing_data.isnull().sum(axis=1)\n    plt.bar(range(len(missing_by_sample)), missing_by_sample)\n    plt.title('Missing Values per Sample')\n    plt.xlabel('Sample Index')\n    plt.ylabel('Number of Missing Values')\n    \n    # Plot 4: Missing data by gene\n    plt.subplot(2, 2, 4)\n    missing_by_gene = missing_data.isnull().sum(axis=0)\n    plt.bar(range(len(missing_by_gene)), missing_by_gene)\n    plt.title('Missing Values per Gene')\n    plt.xlabel('Gene Index')\n    plt.ylabel('Number of Missing Values')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# Create synthetic data\ncomplete_data, true_labels = create_synthetic_biological_data()\n    \n# Introduce missing data patterns\nmissing_data, missing_info = introduce_missing_data_patterns(complete_data, true_labels)\n    \n# Visualize missing patterns\n#visualize_missing_patterns(complete_data, missing_data, true_labels)\n\n# All the missing data is available in the variable missing_data\n# fill in your code here ...\n\n\nMissing data summary:\nTotal missing values: 1346\nMissing percentage: 26.9%",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Missing values in unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/missing_data.html#sec-clustering-missing-data",
    "href": "materials/missing_data.html#sec-clustering-missing-data",
    "title": "11¬† Missing values in unsupervised machine learning",
    "section": "",
    "text": "All the missing data is available in the variable missing_data.\nNow perform hierarchical clustering, PCA and tSNE on this data.\nHow would you impute (infer) missing values? i.e.¬†figure out what the values may have been and fill them in.\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nLook into the impute functions in scikit-learn.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Missing values in unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/missing_data.html#summary",
    "href": "materials/missing_data.html#summary",
    "title": "11¬† Missing values in unsupervised machine learning",
    "section": "11.2 Summary",
    "text": "11.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nMissing data comes up a lot in data science problem.\nMany machine learning techniques struggle on missing data.\nYou can use various techniques to impute missing values.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Missing values in unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/evaluation.html",
    "href": "materials/evaluation.html",
    "title": "12¬† Evaluation for unsupervised machine learning",
    "section": "",
    "text": "12.1 Conceptual framing",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Evaluation for unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/evaluation.html#conceptual-framing",
    "href": "materials/evaluation.html#conceptual-framing",
    "title": "12¬† Evaluation for unsupervised machine learning",
    "section": "",
    "text": "Internal metrics: use only the data + clustering labels. Measure compactness vs separation (e.g., silhouette).\nExternal metrics: require ground truth/labels (experimental groups, annotated cell types). \nBiological validation: compare clusters to known marker genes, pathways, experimental metadata (batch, donor), or enrichment tests. Often the most important for biologists.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Evaluation for unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/evaluation.html#quick-metrics-cheat-sheet-what-they-tell-you",
    "href": "materials/evaluation.html#quick-metrics-cheat-sheet-what-they-tell-you",
    "title": "12¬† Evaluation for unsupervised machine learning",
    "section": "12.2 Quick metrics cheat-sheet (what they tell you)",
    "text": "12.2 Quick metrics cheat-sheet (what they tell you)\n\nExplained variance (PCA) ‚Äî fraction of variance captured by components (useful for dimensionality reduction decisions).\nSilhouette score (‚àí1..1) ‚Äî how well each sample fits its cluster vs nearest other cluster; good general-purpose internal metric. \nStability / reproducibility ‚Äî how consistent cluster assignments are under parameter changes.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Evaluation for unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/evaluation.html#optional-exercise-on-cancer-data",
    "href": "materials/evaluation.html#optional-exercise-on-cancer-data",
    "title": "12¬† Evaluation for unsupervised machine learning",
    "section": "12.3 (Optional) Exercise on cancer data",
    "text": "12.3 (Optional) Exercise on cancer data\n\nYou have been given some data on cancer cell lines\nTeam up with someone and perform hierarchical clustering on this data\nYou have been given some starter code to help you load the data\nThe data has been downloaded and processed for you (after you run the code below).\nThe data is in the variable named X\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport requests\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Load data\nX = pd.read_csv(\"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/cancer_data_saved_NC160.csv\", index_col=0)\n\n\nprint(\"Fetching labels from GitHub...\")\nlabs_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60labs.csv'\nresponse = requests.get(labs_url)\nresponse.raise_for_status()\n# Read the raw text and split into lines.\nall_lines = response.text.strip().splitlines()\n\n# Skip the first line (the header) to match the data dimensions.\nlabs = all_lines[1:]\n\n# The labels in the file are quoted (e.g., \"CNS\"), so we remove the quotes.\nlabs = [label.strip('\"') for label in labs]\n\n# Your code below ......\n\nFetching labels from GitHub...\n\n\n\nWrite your code while working in pairs or a group\n\n\n\n\n\n\n\nNoteClick to expand\n\n\n\n\n\n\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# Compute linkage matrix for the dendrogram\nZ = linkage(X, method='average', metric='cityblock')\n\n# Plot Dendrogram\nplt.figure()\ndendrogram(Z, labels=labs)\nplt.title('Hierarchical Clustering Dendrogram (NCI60, Average Linkage, Manhattan Distance)')\nplt.xlabel('Cell Line')\nplt.ylabel('Distance')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.3.1 Exercise: try another linkage method and another distance metric\n\n\n\n\n\n\nTipImportant Concept (recall)\n\n\n\n\nThere is no ‚Äúcorrect‚Äù answer in unsupervised machine learning!\nSo how do you know when you are done?",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Evaluation for unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/evaluation.html#evaluating-the-quality-of-clusters",
    "href": "materials/evaluation.html#evaluating-the-quality-of-clusters",
    "title": "12¬† Evaluation for unsupervised machine learning",
    "section": "12.4 Evaluating the Quality of Clusters",
    "text": "12.4 Evaluating the Quality of Clusters\nEvaluating the quality of clusters is a crucial step in any unsupervised learning task. Since we do not have a single correct answer, we use several methods that fall into three main categories:\n\n12.4.1 1. Internal Evaluation\nMeasures how good the clustering is based only on the data itself (e.g., how dense and well-separated the clusters are).\n\n\n12.4.2 2. External Evaluation\nMeasures how well the clustering results align with known, ground-truth labels. This is possible here because the NCI60 dataset has known cancer cell line types, which we loaded as labs.\n\n\n12.4.3 3. Visual Evaluation\nInspecting plots (like the dendrogram or PCA) to see if the groupings seem logical.\n\nLet us add the two most common metrics: one internal and one external.\n\n\n\n12.4.4 Internal Evaluation: Silhouette Score\nThe Silhouette Score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n\nScore Range: -1 to +1\n\nInterpretation:\n\n+1: The sample is far away from the neighboring clusters (very good).\n0: The sample is on or very close to the decision boundary between two neighboring clusters.\n-1: The sample is assigned to the wrong cluster.\n\n\n\n\n\n12.4.5 External Evaluation: Adjusted Rand Index (ARI)\nThe Adjusted Rand Index (ARI) measures the similarity between the true labels (labs) and the labels assigned by our clustering algorithm (cluster_labels). It accounts for chance groupings.\n\nScore Range: -1 to +1\n\nInterpretation:\n\n+1: Perfect agreement between true and predicted labels.\n0: Random labeling (no correlation).\n&lt; 0: Worse than random labeling.\n\nHere is how you would implement this\n\n\nfrom sklearn.metrics import silhouette_samples, silhouette_score, adjusted_rand_score # Import evaluation metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# 1. Internal Evaluation: Silhouette Score\n# Measures how well-separated clusters are based on the data itself.\nsilhouette = silhouette_score(X, cluster_labels, metric='manhattan')\nprint(\"Silhouette score\")\nprint(silhouette)\nprint(\"Score is from -1 to 1. Higher is better\")\n\n# 2. External Evaluation: Adjusted Rand Index\n# Compares our cluster labels to the true cancer type labels.\nari = adjusted_rand_score(labs, cluster_labels)\nprint(\"Adjusted Rand Index\")\nprint(ari)\nprint(\"Compares to true labels. Score is from -1 to 1. Higher is better\")\n\nSilhouette score\n0.1436950300449066\nScore is from -1 to 1. Higher is better\nAdjusted Rand Index\n0.0554671516253694\nCompares to true labels. Score is from -1 to 1. Higher is better\n\n\n\n\n12.4.6 Intuition of silhouette score\nThink of each data point as asking two questions:\n\nHow close am I to points in my own cluster? (call this a)\n\nHow close would I be, on average, to the nearest other cluster? (call this b)\n\nThe silhouette value for the point compares those two answers:\n\nIf a is much smaller than b (you are much closer to your own cluster than to any other), the silhouette is close to +1 ‚Üí great fit.\n\nIf a ‚âà b, silhouette is near 0 ‚Üí on the boundary between clusters.\n\nIf a &gt; b, silhouette is negative ‚Üí probably misassigned (you are closer to another cluster than your own).\n\nNumerically:\n[ s = ]\nSo\n[ s ]\n\n\n\nCompare to literature and what others have done\nPlain old visual evaluation\ncompare to labels of what these cell lines are (assuming this is available)\n\n\n\nHow to interpret (practical heuristics)\n\nMean silhouette ‚â≥ 0.5 ‚Üí strong structure (good clustering).\nMean silhouette ‚âà 0.25‚Äì0.5 ‚Üí weak to moderate structure; inspect clusters individually.\nMean silhouette ‚â≤ 0.25 ‚Üí little structure; clustering may be unreliable. (These are rules of thumb ‚Äî context and domain knowledge matter.)\n\n\n\nAlso compare to clusterings of other cancer cell lines\nDoes the cell line also show up in other datasets? (external validation)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Evaluation for unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/evaluation.html#summary",
    "href": "materials/evaluation.html#summary",
    "title": "12¬† Evaluation for unsupervised machine learning",
    "section": "12.5 Summary",
    "text": "12.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nWe learnt evaluation is difficult in unsupervised machine learning!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 3: Clustering",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Evaluation for unsupervised machine learning</span>"
    ]
  },
  {
    "objectID": "materials/pca_tsne_notwork.html",
    "href": "materials/pca_tsne_notwork.html",
    "title": "13¬† When PCA or tSNE might not work",
    "section": "",
    "text": "13.1 When PCA or tSNE may not work",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>When PCA or tSNE might not work</span>"
    ]
  },
  {
    "objectID": "materials/pca_tsne_notwork.html#when-pca-or-tsne-may-not-work",
    "href": "materials/pca_tsne_notwork.html#when-pca-or-tsne-may-not-work",
    "title": "13¬† When PCA or tSNE might not work",
    "section": "",
    "text": "Nonlinear structure\n\nSwiss roll/curved trajectories (e.g., differentiation trajectories); PCA flattens and mixes cells that are nearby in 2D but far along the curve.\nConcentric patterns (e.g., ring-like responses); PCA cannot separate circles.\n\nWhen variance ‚â† signal\n\nBatch effects or library size dominate variance in scRNA‚Äëseq; PCs reflect technical factors rather than biology.\nCell cycle effects overshadow subtle lineage differences.\nRare cell types: biologically important but low variance, thus missed by top PCs. \n\nOutliers and heavy tails\n\nA few extreme samples/genes drive the first PCs, masking true structure (common with QC issues or outlier libraries).\n\nFeature scaling and units\n\nMixed units or unscaled features: high-variance genes/proteins dominate; low-variance but important markers get ignored.\n\n\n\n\nTime/phase structure\n\nPeriodic processes (cell cycle phases): PCA captures amplitude rather than phase, mixing states.\n\np &gt;&gt; n instability\n\nMany more genes than samples: PCs become noisy/unstable without regularization or careful preprocessing.\n\nMissing data\n\nNonrandom missingness (dropouts in scRNA‚Äëseq) biases covariance; naive imputation can create artificial PCs.\n\n\n\n13.1.1 Quick remedies\n\nNonlinear structure: t‚ÄëSNE/UMAP. \nVariance ‚â† signal: regress out batch/cell cycle; use sctransform; combat/BBKNN/Harmony for batch correction; consider supervised methods (PLS/CCA) if labels exist.\nOutliers/heavy tails: robust PCA, rank genes with robust dispersion, Winsorize/log1p.\nScaling/units: standardize features; use variance-stabilizing transforms (log1p, VST). \n\n\n\n13.1.2 Non-linear data\n\nNon-linearity: Data that lies on curved surfaces or when data has non-linear relationships.\nSingle-cell data: Biological data where cell types form non-linear clusters in high-dimensional space\n\n\n\n13.1.3 Categorical Features\n\nPCA may work poorly with categorical data unless properly encoded\nOne-hot encoding categorical features can create sparse, high-dimensional data where PCA may not capture meaningful structure",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>When PCA or tSNE might not work</span>"
    ]
  },
  {
    "objectID": "materials/pca_tsne_notwork.html#alternatives",
    "href": "materials/pca_tsne_notwork.html#alternatives",
    "title": "13¬† When PCA or tSNE might not work",
    "section": "13.2 Alternatives",
    "text": "13.2 Alternatives\n\n13.2.1 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n\nBest for: Non-linear dimensionality reduction and visualization\nKey parameter: Perplexity (try values 5-50)\nUse case: Single-cell data, biological expression data, any non-linear clustering\n\n\n\n\n\n\n\nTip\n\n\n\nNOTE (IMPORTANT CONCEPT): Sometimes tSNE may not work as well! It is hard to predict which unsupervised machine learning technique will work best.\nYou just need to try a bunch of different techniques.\n\n\n\n\n\n\n13.2.2 Hierarchical Clustering + Heatmaps\n\nBest for: Categorical data and understanding relationships between samples\nUse case: When you want to see how samples group together based on multiple features\n\n\n\n13.2.3 Activity: Demonstrating how PCA may not work well on the Swiss roll data\n\nThe Swiss roll dataset\n\nA 2D curve embedded in 3D that looks like a rolled sheet. You can play around with the plot below! \n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nPerforming PCA on the Swiss roll dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming tSNE on the Swiss roll dataset\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.4 PCA on time series data and spiral data\n\n\n\n--- TIMESERIES DATA ---\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\nPCA Analysis for Timeseries Data\n==================================================\nPC1 explains 69.6% of variance\nPC2 explains 30.4% of variance\nTotal variance explained: 100.0%\n\nWhy PCA fails here:\n‚Ä¢ This is actually a function y = f(t) where t is time\n‚Ä¢ PCA treats it as 2D spatial data, ignoring temporal structure\n‚Ä¢ The relationship is nonlinear (sine waves)\n‚Ä¢ Important frequency components are lost in projection\n\nBetter alternatives:\n‚Ä¢ Manifold learning: t-SNE, UMAP, Isomap\n‚Ä¢ Kernel PCA for nonlinear relationships\n‚Ä¢ Autoencoders for complex nonlinear dimensionality reduction\n‚Ä¢ For time series: Fourier analysis, wavelet transforms\n‚Ä¢ Recurrent neural networks for temporal patterns\n\n============================================================\n\n--- SPIRAL DATA ---\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\nPCA Analysis for Spiral Data\n==================================================\nPC1 explains 54.6% of variance\nPC2 explains 45.4% of variance\nTotal variance explained: 100.0%\n\nWhy PCA fails here:\n‚Ä¢ Data follows a spiral pattern with rotational structure\n‚Ä¢ PCA cannot capture circular/rotational relationships\n‚Ä¢ Sequential ordering (temporal aspect) is lost\n‚Ä¢ Linear projection destroys the spiral geometry\n\nBetter alternatives:\n‚Ä¢ Manifold learning: t-SNE, UMAP, Isomap\n‚Ä¢ Kernel PCA for nonlinear relationships\n‚Ä¢ Autoencoders for complex nonlinear dimensionality reduction\n\n============================================================\n\n\n\n\n13.2.5 Missing data\nPCA and tSNE also do not work well when there is missing data. See Exercise Section 11.1.\n\n\n13.2.6 Demonstrating how PCA or tSNE may not work well on biological data\n\nGenerate synthetic biological expression data: matrix of 200 samples √ó 10 genes, where Gene_1 and Gene_2 follow a clustering (four corner clusters) and the remaining genes are just Gaussian noise. You can see from the scatter of Gene_1 vs Gene_2 that the true structure is non-linear and not aligned with any single variance direction: PCA (or tSNE) may fail to unfold these clusters into separate principal components.\n\n\n\n\n\n\n\n\n\n\n\nPerform PCA on this data\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Apply PCA\npca = PCA()\npcs = pca.fit_transform(df) # where df is a dataframe with your data\n\n\n# Scatter plot of the first two principal components\nplt.figure()\nplt.scatter(pcs[:, 0], pcs[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA on Synthetic Biological Dataset')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet us try tSNE on this data\n\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ntsne = TSNE()\ntsne_results = tsne.fit_transform(df)\n\n# plot\nplt.figure()\nplt.scatter(tsne_results[:,0], tsne_results[:,1])\nplt.xlabel('t-SNE component 1')\nplt.ylabel('t-SNE component 2')\nplt.title('t-SNE on Synthetic Biological Dataset')\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat if we try different values of perplexity?\n\n\n\n\n\n\n\n\n\n\n\nWhat if data has categorical features?\n\nPCA may or may not work if you have categorical features.\n\nFor example, if you have data that looks like this ‚Ä¶.\n\n\n  species tissue condition\n0   human  liver  diseased\n1   mouse  brain  diseased\n2   human  liver  diseased\n3   human  brain  diseased\n4   mouse  brain   healthy\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can split by disease/healthy, or other features.\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering\n\nRecall:\nLeaves: Each leaf at the bottom of the dendrogram represents one sample from your dataset.\nBranches: The branches connect the samples and groups of samples. The height of the branch represents the distance (dissimilarity) between the clusters being merged.\nHeight of Merges: Taller branches indicate that the clusters being merged are more dissimilar, while shorter branches indicate more similar clusters.\nClusters: By drawing a horizontal line across the dendrogram at a certain distance, you can define clusters. All samples below that line that are connected by branches form a cluster.\n\nIn the context of your one-hot encoded categorical data (species, tissue, condition), the dendrogram shows how samples are grouped based on their combinations of these categorical features.\nSamples with the same or very similar combinations of categories will be closer together in the dendrogram and merge at lower distances.\nThe structure of the dendrogram reflects the relationships and similarities between the different combinations of species, tissue, and condition present in your synthetic dataset.\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Assume 'encoded_data' exists from the previous one-hot encoding step\nlinked = linkage(y = encoded_data,\n    method = 'ward',\n    metric = 'euclidean',\n    optimal_ordering=True\n    )\n\n# plot dendrogram\nplt.figure()\ndendrogram(linked, \n            orientation='top',\n            distance_sort='descending',\n            show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram on One-Hot Encoded Categorical Data')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\n# or use sns.clustermap()\nsns.clustermap(data=encoded_data,\nmethod = \"ward\",\nmetric = \"euclidean\",\nrow_cluster = True,\ncol_cluster = True,\ncmap = \"vlag\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeatmaps\n\nHeatmaps are a great way to visualize data and clustering\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assume 'encoded_df' exists from the previous one-hot encoding step\n\nplt.figure()\nsns.heatmap(encoded_df.T, cmap='viridis', cbar_kws={'label': 'Encoded Value (0 or 1)'}) # Transpose for features on y-axis\n\nplt.title('Heatmap of One-Hot Encoded Categorical Data')\nplt.xlabel('Sample Index')\nplt.ylabel('Encoded Feature')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>When PCA or tSNE might not work</span>"
    ]
  },
  {
    "objectID": "materials/pca_tsne_notwork.html#summary",
    "href": "materials/pca_tsne_notwork.html#summary",
    "title": "13¬† When PCA or tSNE might not work",
    "section": "13.3 Summary",
    "text": "13.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPCA or tSNE are not magic bullets and may not work all the time\nUsually you need to try a bunch of different techniques\nRemember that unsupervised machine learning is exploratory",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>When PCA or tSNE might not work</span>"
    ]
  },
  {
    "objectID": "materials/pca_tsne_notwork.html#additional-reading",
    "href": "materials/pca_tsne_notwork.html#additional-reading",
    "title": "13¬† When PCA or tSNE might not work",
    "section": "13.4 Additional reading",
    "text": "13.4 Additional reading\n[1] tSNE FAQ by its creators\n[2] Swiss roll and SNE",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>When PCA or tSNE might not work</span>"
    ]
  },
  {
    "objectID": "materials/applications.html",
    "href": "materials/applications.html",
    "title": "14¬† Hands-on exercises (Applications of unsupervised machine learning)",
    "section": "",
    "text": "14.1 Fun fact",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Hands-on exercises (Applications of unsupervised machine learning)</span>"
    ]
  },
  {
    "objectID": "materials/applications.html#fun-fact",
    "href": "materials/applications.html#fun-fact",
    "title": "14¬† Hands-on exercises (Applications of unsupervised machine learning)",
    "section": "",
    "text": "Tip\n\n\n\nDid you know?: that unsupervised machine learning is the first step in building large-language models (LLMs).\nWatch this short video by 3blue1brown",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Hands-on exercises (Applications of unsupervised machine learning)</span>"
    ]
  },
  {
    "objectID": "materials/applications.html#exercises",
    "href": "materials/applications.html#exercises",
    "title": "14¬† Hands-on exercises (Applications of unsupervised machine learning)",
    "section": "14.2 Exercises",
    "text": "14.2 Exercises\n\nBreak up into small groups and work on any one of the following small projects.\nNone of the projects require any knowledge of biology or any other field.\nIf you are unsure, pick the first project on electronic healthcare records.\nI will be doing a live coding session (code walkthrough).\n\n\n14.2.1 Project using electronic healthcare records data\n\n\n\n\n\n\nExerciseExercise 1 - Electronic healthcare records data\n\n\n\n\n\n\nLevel: \nFor this exercise we will be using some data from hospital electronic healthcare records (EHR). No knowledge of biology/healthcare is required for this.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nProject briefing\nHere is a brief code snippet to help you load the data and get started.\nYou have to follow the following steps:\n\nData Loading and Preprocessing: Loading a diabetes dataset and normalizing numerical features.\nDimensionality Reduction: Applying PCA and t-SNE to reduce the dimensions of the data for visualization and analysis.\nClustering: Performing K-Means clustering on the reduced data to identify potential patient subgroups.\nVisualization: Visualizing the data in lower dimensions and the identified clusters to gain insights.\n\n\nPythonR\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\n#######################\n# Load diabetes data \n#######################\n\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/diabetes_kaggle.csv\"\ndf = pd.read_csv(url)\n\n######################################\n# Perform data munging and filtering\n######################################\nprint(df.head())\n\n# Normalize numeric columns\nnumeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\nscaler = MinMaxScaler()\ndf_normalized = df.copy() # make a copy\ndf_normalized[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n# ALTERNATIVE CODE (repeat for each numeric column)\n# Make a copy so the original DataFrame stays unchanged\n# df_normalized = df.copy()\n\n# Create the scaler\n# scaler = MinMaxScaler()\n\n# Select the 'Glucose' column as a DataFrame (double brackets keep 2D shape)\n# glucose_values = df_normalized[['Glucose']]\n\n# Fit the scaler and transform the values\n# glucose_scaled = scaler.fit_transform(glucose_values)\n\n# Put the scaled values back into the copy\n# df_normalized[['Glucose']] = glucose_scaled\n\n# Filter: Glucose &gt; 0.5 and BMI &lt; 0.3 (normalized values)\nfiltered_df = df_normalized[\n    (df_normalized['Glucose'] &gt; 0.5) &\n    (df_normalized['BMI'] &lt; 0.3)\n]\n\nprint(filtered_df.head())\n\n   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  \n     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n9       0.470588  0.628141       0.786885       0.000000  0.000000  0.000000   \n49      0.411765  0.527638       0.000000       0.000000  0.000000  0.000000   \n50      0.058824  0.517588       0.655738       0.111111  0.096927  0.289121   \n145     0.000000  0.512563       0.614754       0.232323  0.000000  0.000000   \n239     0.000000  0.522613       0.622951       0.000000  0.000000  0.274218   \n\n     DiabetesPedigreeFunction       Age  Outcome  \n9                    0.065756  0.550000      1.0  \n49                   0.096926  0.050000      0.0  \n50                   0.176345  0.016667      0.0  \n145                  0.210931  0.000000      0.0  \n239                  0.215201  0.100000      0.0  \n\n\n\nVisualize the data\n\n\n# Histogram\nplt.figure()\nsns.histplot(df_normalized['Glucose'], bins=30)\nplt.title('Distribution of Normalised Glucose')\nplt.xlabel('Normalised Glucose')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow visualize the other variables. Do you notice anything interesting/odd about them? Hint: use sns.histplot() as shown above or plt.hist().\nData visualization is a key step in machine learning. Make sure to spend some time visualizing all the variables/features. Discuss the plots in your group.\nPerform PCA\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Exclude the target column for PCA\n# We not want to include this because this is something you want to predict.\n# You can use this column in supervised machine learning.\nfeatures = df_normalized.drop(columns=['Outcome'])\n\n# Apply PCA\n# This is where you fill in your code .........\n\n\nFill in the rest of the code with your group members.\nPerform PCA and visualize it. Hint: use plt.scatter() or sns.scatterplot().\nEvaluation (how to interpret the PCA plots?)\nReminder: In plt.scatter, the c parameter controls the marker colour (or colours).\nThe alpha parameter controls the transparency (opacity) of the markers.\nWhen passing numbers, you can specify cmap (colour map) to control the gradient mapping (otherwise the default colourmap is used)\nLet us colour by the feature BMI now\n\n\n# Visualize PCA results colored by BMI\nplt.figure()\nscatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c = df_normalized['BMI'],\n                     cmap='viridis', alpha=0.7)\nplt.colorbar(scatter, label='BMI (normalized)')\nplt.title('PCA of Diabetes Dataset - Coloured by BMI')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nDo you see any patterns?\nNow colour by Pregnancies\nTry other features: Glucose, BloodPressure, SkinThickness, Insulin, DiabetesPedigreeFunction, Age\nTry spotting any patterns and discuss this in your group.\nRecall: The primary goal of unsupervised machine learning is to uncover hidden patterns, structures, and relationships within the data.\nThis can lead to the generation of new hypotheses about the underlying phenomena, which can then be tested in follow-up studies using statistical methods or through the application of supervised machine learning techniques with labeled data.\nEssentially, unsupervised learning helps us explore the data and formulate questions that can be further investigated.\nHowever it is never the end of the data science pipeline. It can lead to further investigations.\nNow try tSNE on this data\n\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Exclude the target column for t-SNE\nfeatures = df_normalized.drop(columns=['Outcome'])\n\n# Apply t-SNE\n# This is where you fill in your code .........\n\n\nPerform tSNE on this data\nVary the perplexity parameter\nNow let us colour the tSNE plot by BMI\n\n\n# Exclude the target column for t-SNE\n# Already done (so commenting out)\n# features_for_tsne = df_normalized.drop(columns=['Outcome', 'Cluster'])\n\n# Create a DataFrame for the t-SNE results\ntsne_df = pd.DataFrame(data=tsne_results, columns=['TSNE1', 'TSNE2'])\n\n# Visualize t-SNE colored by BMI\nplt.figure()\nscatter = plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], c=df_normalized['BMI'],\n                     cmap='viridis', alpha=0.7, s=50)\nplt.colorbar(scatter, label='BMI (normalized)')\nplt.title('t-SNE of Diabetes Dataset - Colored by BMI')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow colour the tSNE plot by some other feature. Try Glucose, BloodPressure, SkinThickness, Insulin, DiabetesPedigreeFunction, Age\nDo you observe any patterns? Discuss in your group.\nPerform hierarchical clustering on this data\n\n\nimport pandas as pd\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Exclude the target column for clustering\nfeatures = df_normalized.drop(columns=['Outcome'])\n\n# Perform hierarchical clustering\n# This is where you fill in your code .........\n\n\nAlternatively you can use sns.clustermap().\n\nHint: Here is some code to get you started.\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\n\nehr_row_linkage = linkage(features, method=\"ward\")\n\n# plot heatmap using sns.clustermap()\nsns.clustermap(data = features,\nrow_linkage = ehr_row_linkage,\ncmap = \"vlag\",\nstandard_scale = 0\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerform k-means on this data.\nDiscuss in your group the outcome of this project.\nWhat are your key findings?\nDo you think we can find partitions of patients/clusters of patients?\nOnce you discover these partitions/clusters, what will you do with this information?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nWork in a group!\n\n\n\n\n\n\n14.2.2 Project using single-cell sequencing data\n\n\n\n\n\n\nExerciseExercise 2 - Single-cell sequencing\n\n\n\n\n\n\nLevel: \nFor this exercise we will be using some single-cell sequencing data. No biological expertise is required for this.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nExercise\nHere is a brief code snippet to help you load the data and get started.\nYou have to follow the following steps:\n\nData Loading and Preprocessing: Loading a single-cell sequencing dataset and normalizing features.\n\n\nPythonR\n\n\n\nInstall packages\n\n\n!pip install scanpy scipy matplotlib pandas seaborn\n\n\nLoad libraries\n\n\nimport scanpy as sc\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\nLoad and Preprocess Data: Load the pbmc3k dataset using scanpy, normalizes the total counts per cell to 10,000, and then applies a log transformation.\nThen picks out a few ‚Äúmarker‚Äù genes (genes that may be important for the disease based on our prior knowledge).\nThe single cell data is just a table of numbers: the rows are different cells, the columns are genes measured in those cells. Here is what this would look like:\n\n\n\n\nCell\nCD3D\nCD4\nCD8A\nFOXP3\nIL2RA\n\n\n\n\nCell_001\n0.5\n1.2\n0.0\n2.1\n0.8\n\n\nCell_002\n1.1\n0.3\n1.5\n0.0\n1.9\n\n\nCell_003\n0.0\n2.4\n0.7\n1.3\n0.4\n\n\nCell_004\n1.8\n0.0\n2.2\n0.9\n1.1\n\n\nCell_005\n0.3\n1.7\n0.0\n1.6\n0.2\n\n\n\n\n# 1. Load data and preprocess\nadata = sc.datasets.pbmc3k()\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\n\n# 2. Subset to marker genes\nmarker_genes = [\n    'CD3D','CD3E','CD4','CD8A',\n    'CD14','LYZ',\n    'MS4A1',\n    'GNLY','NKG7'\n]\n\ngenes = [g for g in marker_genes if g in adata.var_names]\n\nexpr = pd.DataFrame(\n    adata[:, genes].X.toarray(),\n    index=adata.obs_names,\n    columns=genes\n)\n\nprint(expr.head())\n\n                      CD3D      CD3E  CD4      CD8A  CD14       LYZ     MS4A1  \\\nindex                                                                           \nAAACATACAACCAC-1  2.863463  2.225817  0.0  1.635208   0.0  1.635208  0.000000   \nAAACATTGAGCTAC-1  0.000000  0.000000  0.0  0.000000   0.0  1.962726  2.583047   \nAAACATTGATCAGC-1  3.489089  1.994867  0.0  0.000000   0.0  1.994867  0.000000   \nAAACCGTGCTTCCG-1  0.000000  0.000000  0.0  0.000000   0.0  4.521174  0.000000   \nAAACCGTGTATGCG-1  0.000000  0.000000  0.0  0.000000   0.0  0.000000  0.000000   \n\n                      GNLY      NKG7  \nindex                                 \nAAACATACAACCAC-1  0.000000  0.000000  \nAAACATTGAGCTAC-1  0.000000  1.111715  \nAAACATTGATCAGC-1  1.429261  0.000000  \nAAACCGTGCTTCCG-1  0.000000  1.566387  \nAAACCGTGTATGCG-1  3.452557  4.728542  \n\n\n\nWe now have a table of numbers: the rows are cells, and columns are genes measured in those cells.\nVisualize the data. Use plt.hist() or sns.histplot().\nNow perform PCA on this data (Hint: expr.values has all the values. Perform PCA on this.)\nNow colour this PCA plot by one marker gene CD3D. The CD3D gene is crucial for immune response. Mutations in this gene can lead to disease. Hint: expr[\"CD3D\"] will get you all the values of the gene. Use that in the c = option in plt.scatter().\nDiscuss in your group: what do you think the plot means?\nNow try the other marker genes: CD3E,CD4,CD8A, CD14, LYZ, MS4A1, GNLY, NKG7\nDiscuss in your group: what do you think the plot means?\nNow perform tSNE on this. Hint: expr.values has all the values. Perform tSNE on this.\nNow colour this PCA plot by one marker gene CD3D. Hint: expr[\"CD3D\"] will get you all the values of the gene. Use that in the c = option in plt.scatter().\nDiscuss in your group: what do you think the plot means?\nNow try the other marker genes: CD3E,CD4,CD8A, CD14, LYZ, MS4A1, GNLY, NKG7\nDiscuss in your group: what do you think the plot means?\nReminder: tSNE is stochastic.\nRun tSNE again. Do the clusters remain the same? Can you see the same patterns?\nRun tSNE with a different perplexity value. Do the clusters remain the same?\nDiscuss in your group your key findings. What can you say about these clusters?\nNow perform hierarchical clustering on this data.\n\n\n\nTry a few distance functions and linkage functions.\nPlot heatmaps or clustermaps (Hint: seaborn clustermap does both dendrograms + heatmap in one shot). A representative plot is shown below. Can you try to get a plot similar to this?\nHint: Here is some code to get you started.\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.spatial.distance import pdist\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# compute linkage\ncell_link = linkage( pdist(expr.T, metric=\"euclidean\"), method=\"ward\" )\ngene_link = linkage( pdist(expr,   metric=\"euclidean\"), method=\"ward\" )\n\n# seaborn clustermap does both dendrograms + heatmap in one shot\n# Fill in the code below .......\nsns.clustermap(.......)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerform k-means on this data.\nDiscuss in your group the outcome of this project.\nWhat are your key findings?\nDo you think we can find partitions of cells/clusters of cells?\nWhat can you do with these partitions?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nWork in a group!\n\n\n\n\n\n\n14.2.3 Project using GapMinder data\n\n\n\n\n\n\nExerciseExercise 3 - GapMinder data\n\n\n\n\n\n\nLevel: \nFor this exercise we will be using sociological data.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nExercise\nIn this exercise you will explore the Gapminder dataset, focusing on life expectancy, GDP per capita, and population data.\nThe Gapminder dataset contains global development indicators over time, tracking 142 countries from 1952 to 2007 at 5-year intervals.\n\nDataset Features\n\n\n\n14.2.4 1. country (Categorical)\n\nType: String/Categorical variable\nDescription: The name of the country\nExample: ‚ÄúAfghanistan‚Äù, ‚ÄúAlbania‚Äù, ‚ÄúAlgeria‚Äù, etc.\nPurpose: Identifies which country each observation belongs to\nUnique values: 142 different countries\n\n\n\n14.2.5 2. year (Numerical)\n\nType: Integer\nDescription: The year of the observation\nRange: 1952 to 2007\nInterval: Every 5 years (1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007)\nPurpose: Tracks temporal changes in development indicators\n\n\n\n14.2.6 3. pop (Numerical)\n\nType: Float\nDescription: Total population of the country\nUnits: Number of people\nRange: From thousands to over 1 billion\nExample: 8,425,333 people in Afghanistan in 1952\nPurpose: Measures country size and demographic changes over time\n\n\n\n14.2.7 4. continent (Categorical)\n\nType: String/Categorical variable\nDescription: The continent where the country is located\nCategories: ‚ÄúAfrica‚Äù, ‚ÄúAmericas‚Äù, ‚ÄúAsia‚Äù, ‚ÄúEurope‚Äù, ‚ÄúOceania‚Äù\nPurpose: Groups countries by geographical region for comparative analysis\n\n\n\n14.2.8 5. lifeExp (Numerical)\n\nType: Float\nDescription: Life expectancy at birth\nUnits: Years\nRange: Typically 20-85 years\nExample: 28.801 years in Afghanistan in 1952\nPurpose: Key health indicator measuring average lifespan\n\n\n\n14.2.9 6. gdpPercap (Numerical)\n\nType: Float\nDescription: Gross Domestic Product per capita\nUnits: US dollars (adjusted for inflation)\nRange: From hundreds to tens of thousands of dollars\nExample: $779.45 in Afghanistan in 1952\nPurpose: Economic indicator measuring average wealth per person\n\nYou will perform the following steps initially:\n\nData Loading and Setup: The gapminder dataset is loaded, and necessary libraries for data manipulation, visualization, and dimensionality reduction are imported.\n\nFeature Selection: The features lifeExp, gdpPercap, and pop are selected for analysis.\n\nHere is a brief code snippet to help you load the data and get started.\n\nPythonR\n\n\n\nInstall packages\n\n\n!pip install scipy matplotlib pandas seaborn\n\n\nLoad libraries\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\nLoad data\n\n\n# Download Gapminder data\nurl = \"https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv\"\ngap = pd.read_csv(url)\n\nprint(gap.head())\n\n       country  year         pop continent  lifeExp   gdpPercap\n0  Afghanistan  1952   8425333.0      Asia   28.801  779.445314\n1  Afghanistan  1957   9240934.0      Asia   30.332  820.853030\n2  Afghanistan  1962  10267083.0      Asia   31.997  853.100710\n3  Afghanistan  1967  11537966.0      Asia   34.020  836.197138\n4  Afghanistan  1972  13079460.0      Asia   36.088  739.981106\n\n\n\nSubset to countries in Asia and aggregate\n\n\n# Aggregate by country: mean of features for each Asian country\nfeatures = ['lifeExp', 'gdpPercap']\nasia_gap_unique = gap[gap['continent'] == 'Asia'].groupby('country')[features].mean().reset_index()\n\nprint(asia_gap_unique.head())\n\n       country    lifeExp     gdpPercap\n0  Afghanistan  37.478833    802.674598\n1      Bahrain  65.605667  18077.663945\n2   Bangladesh  49.834083    817.558818\n3     Cambodia  47.902750    675.367824\n4        China  61.785140   1488.307694\n\n\n\nVisualize the features by using plt.hist() or sns.histplot()\nThen perform PCA on it. Hint: you need to normalize your data also.\n\nDoes your plot look like this?\n\n\n\n\n\n\n\n\n\n\n\nIs there anything ‚Äúodd‚Äù about this plot? Discuss this in your group.\nNow label each point on the PCA biplot this by country names\n\nHint: The following code will not work (since ‚Äúcountry‚Äù is categorical). You will have to a bit creative!\nplt.figure()\nplt.scatter(pcs[:,0], pcs[:,1]), c=asia_gap_unique[\"country\"])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Plot of PCA on Gapminder data for Asian countries\")\nplt.show()\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nHere are some code hints to help you.\n\nplt.figure()\nplt.scatter(pcs[:,0], pcs[:,1])\n\n# add country labels\nfor i, country in enumerate( asia_gap_unique[\"country\"] ):\n    # fill in your code here \n    plt.annotate(.....)\n\nplt.show()\n\nYour plot may look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does PC1 mean? Are there any features that are correlated with PC1?\n\nHint: Perform a scatterplot (plt.scatter()) for each feature vs.¬†PC1\nCan we use PCA to approximate the human development index?\n\nPerform tSNE on this data\n\n\n\n\n\n\n\n\n\n\n\nDo you know notice anything ‚Äúodd‚Äù/‚Äúinteresting‚Äù about this plot?\nChange the perplexity parameter and observe what it does to the plot.\nNow add the labels of the countries to this tSNE plot. Here is some code to give you a hint.\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nHere are some code hints to help you.\n\nplt.figure()\nplt.scatter(asia_tsne[:,0], asia_tsne[:,1])\n\n# add country labels\nfor i, country in enumerate( asia_gap_unique[\"country\"] ):\n    # fill in your code here \n    plt.annotate(.....)\n\nplt.show()\n\n\n\n\n\n\n\n\nNow perform hierarchical clustering on this data.\nDiscuss the outcomes of your project in your group. Explain your key outcomes (in a few minutes) to everyone in the class.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nWork in a group!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Hands-on exercises (Applications of unsupervised machine learning)</span>"
    ]
  },
  {
    "objectID": "materials/applications.html#wrap-up",
    "href": "materials/applications.html#wrap-up",
    "title": "14¬† Hands-on exercises (Applications of unsupervised machine learning)",
    "section": "14.3 Wrap-up",
    "text": "14.3 Wrap-up\n\nRemember that learning does not stop once you leave class.\nContinue practicing your newly learnt skills on new data.\nPlease take a few minutes to fill out the survey!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Hands-on exercises (Applications of unsupervised machine learning)</span>"
    ]
  },
  {
    "objectID": "materials/applications.html#summary",
    "href": "materials/applications.html#summary",
    "title": "14¬† Hands-on exercises (Applications of unsupervised machine learning)",
    "section": "14.4 Summary",
    "text": "14.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nUnderstand real-world scenarios where unsupervised learning is applied\nIdentify situations where PCA and other dimensionality reduction techniques may not be effective\nPractical examples of data that you try unsupervised learning techniques on",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Session 4: Applications of Unsupervised Learning",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Hands-on exercises (Applications of unsupervised machine learning)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html",
    "href": "materials/umap.html",
    "title": "15¬† UMAP (Optional)",
    "section": "",
    "text": "16 UMAP Intuition",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#what-is-umap",
    "href": "materials/umap.html#what-is-umap",
    "title": "15¬† UMAP (Optional)",
    "section": "16.1 What is UMAP?",
    "text": "16.1 What is UMAP?\nUMAP (Uniform Manifold Approximation and Projection) is another powerful unsupervised machine learning technique that helps visualize high-dimensional data in 2D or 3D. Think of it as a ‚Äúsmart cartographer‚Äù that creates a map of your complex data, revealing hidden patterns.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#the-core-idea-preserve-both-local-and-global-structure",
    "href": "materials/umap.html#the-core-idea-preserve-both-local-and-global-structure",
    "title": "15¬† UMAP (Optional)",
    "section": "16.2 The Core Idea: Preserve Both Local and Global Structure",
    "text": "16.2 The Core Idea: Preserve Both Local and Global Structure\n\n16.2.1 The Problem We are Solving\n\nSay you have cells with 20,000+ gene measurements\nYou want to see which cells are similar to each other\nYou want to understand both local neighborhoods AND global structure\nBut 20,000 dimensions are impossible to visualize!\n\n\n\n16.2.2 The Solution\nUMAP takes your high-dimensional data and creates a 2D map where: - Similar cells stay close together (local structure preserved) - Different cells stay far apart (global structure preserved) - Both local neighborhoods AND global relationships are maintained",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#how-umap-works-the-intuition",
    "href": "materials/umap.html#how-umap-works-the-intuition",
    "title": "15¬† UMAP (Optional)",
    "section": "16.3 How UMAP Works: The Intuition",
    "text": "16.3 How UMAP Works: The Intuition\n\n16.3.1 Step 1: Build a Graph of Relationships\nOriginal Space (20,000+ genes):\nCell A: [Gene1=5, Gene2=10, Gene3=2, ... Gene20000=8]\nCell B: [Gene1=6, Gene2=11, Gene3=3, ... Gene20000=9]\nCell C: [Gene1=50, Gene2=100, Gene3=20, ... Gene20000=80]\n\nUMAP creates a \"friendship network\":\n- A and B are close friends (very similar)\n- A and C are distant acquaintances (very different)\n- B and C are also distant acquaintances\n\n\n16.3.2 Step 2: Create a 2D Map\nUMAP creates a 2D layout where:\n- Close friends (A and B) are placed near each other\n- Distant acquaintances (A and C, B and C) are placed far apart\n- The overall \"social network\" structure is preserved",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#the-manifold-concept",
    "href": "materials/umap.html#the-manifold-concept",
    "title": "15¬† UMAP (Optional)",
    "section": "16.4 The ‚ÄúManifold‚Äù Concept",
    "text": "16.4 The ‚ÄúManifold‚Äù Concept\n\n16.4.1 What is a Manifold?\nThink of a manifold like the surface of a balloon: - From far away, it looks like a simple sphere - Up close, you can see it is actually a 2D surface curved in 3D space - Your high-dimensional biological data might be ‚Äúcurved‚Äù in ways we can‚Äôt see\n\n\n16.4.2 Why ‚ÄúUniform‚Äù?\nUMAP assumes your data is spread ‚Äúuniformly‚Äù across this curved surface: - No empty regions (uniform coverage) - No overly crowded regions (uniform density) - This helps create a balanced, interpretable map",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#umap-vs-t-sne-key-differences",
    "href": "materials/umap.html#umap-vs-t-sne-key-differences",
    "title": "15¬† UMAP (Optional)",
    "section": "16.5 UMAP vs t-SNE: Key Differences",
    "text": "16.5 UMAP vs t-SNE: Key Differences\n\n16.5.1 What UMAP Does Better\nPreserves Global Structure: - t-SNE: Focuses mainly on local neighborhoods - UMAP: Maintains both local AND global relationships",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#key-parameters-to-understand",
    "href": "materials/umap.html#key-parameters-to-understand",
    "title": "15¬† UMAP (Optional)",
    "section": "16.6 Key Parameters to Understand",
    "text": "16.6 Key Parameters to Understand\n\n16.6.1 n_neighbors (Default: 15)\n\nControls how many ‚Äúfriends‚Äù each cell considers\nLow (5-10): Focus on very close neighbors, creates many small clusters\nHigh (30-50): Consider more distant neighbors, creates fewer, larger clusters\nDefault (15): Usually works well for most datasets\n\n\n\n16.6.2 min_dist (Default: 0.1)\n\nControls how tightly packed points can be in the final map\nLow (0.01): Points can be very close together (tight clusters)\nHigh (0.5): Points spread out more (looser clusters)\nDefault (0.1): Good balance between tightness and readability\n\n\n\n16.6.3 metric (Default: ‚Äòeuclidean‚Äô)\n\nHow to measure distances between cells\n‚Äòeuclidean‚Äô: Standard geometric distance (good for most data)\n‚Äòcosine‚Äô: Angle-based distance (good for normalized data)\n‚Äòmanhattan‚Äô: City-block distance (good for sparse data)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#umap-vs-other-methods",
    "href": "materials/umap.html#umap-vs-other-methods",
    "title": "15¬† UMAP (Optional)",
    "section": "16.7 UMAP vs Other Methods",
    "text": "16.7 UMAP vs Other Methods\n\n16.7.1 UMAP vs PCA\n\nPCA: Linear method, preserves variance, good for linear relationships\nUMAP: Non-linear method, preserves local structure, good for complex relationships\n\n\n\n16.7.2 UMAP vs t-SNE\n\nt-SNE: Great for local structure, slower, harder to interpret distances\nUMAP: Good for both local and global structure, faster, more interpretable",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#practical-tips",
    "href": "materials/umap.html#practical-tips",
    "title": "15¬† UMAP (Optional)",
    "section": "16.8 Practical Tips",
    "text": "16.8 Practical Tips\n\n16.8.1 1. Start with Default Parameters\n\nUMAP‚Äôs defaults work well for most biological data\nDon‚Äôt over-optimize parameters initially\n\n\n\n16.8.2 2. Try Different n_neighbors Values\n\n5-10: If you want to see fine-grained subpopulations\n15-30: For general exploration (recommended)\n50+: If you want to see only major cell types\n\n\n\n16.8.3 3. Adjust min_dist for Readability\n\n0.01-0.05: If points are too spread out\n0.1-0.3: Default range (recommended)\n0.5+: If clusters are too tight\n\n\n\n16.8.4 4. Use Multiple Runs\n\nUMAP has some randomness\nRun multiple times to ensure results are consistent\nUse random_state parameter for reproducible results\n\n\n\n16.8.5 5. Validate with Biology\n\nAlways check if UMAP results make biological sense\nCompare with known cell type markers\nLook for expected developmental trajectories",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#summary",
    "href": "materials/umap.html#summary",
    "title": "15¬† UMAP (Optional)",
    "section": "16.9 Summary",
    "text": "16.9 Summary\nUMAP is like a smart cartographer who: 1. Studies your high-dimensional data (20,000+ genes/proteins) 2. Identifies both local neighborhoods AND global relationships 3. Creates a beautiful 2D map that preserves both types of structure 4. Reveals hidden patterns you couldn‚Äôt see before\nThe key insight: UMAP preserves both local and global structure - cells that are similar stay close together, while the distances between different cell types remain meaningful.\nThis makes UMAP perfect for biologists who want to understand the complete structure of their complex, high-dimensional data - from individual cell relationships to overall tissue organization!\n\nRemember: UMAP is a tool for exploration and visualization, not a replacement for careful analysis!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#hands-on-with-umap",
    "href": "materials/umap.html#hands-on-with-umap",
    "title": "15¬† UMAP (Optional)",
    "section": "16.10 Hands-on with UMAP",
    "text": "16.10 Hands-on with UMAP\nThe way to use UMAP is similar to how we did tSNE.\n\n!pip install umap-learn\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nimport umap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")  # nice simple plots\n\niris = datasets.load_iris()\nX_iris = iris.data           # 150 samples, 4 features (sepal/petal lengths/widths)\ny_iris = iris.target         # species labels (0,1,2)\nlabels_iris = iris.target_names\n\n# create a data frame for easier viewing\ndf_iris_simple = pd.DataFrame(X_iris, columns = iris.feature_names)\n\ndf_iris_simple['species'] = iris.target\ndf_iris_simple['species_name'] = df_iris_simple['species'].map( {0:'setosa', 1:'versicolor', 2:'virginica'} )\n\n# display basic information\nprint(df_iris_simple.head)\n\n# scatter plots\nplt.figure()\nplt.scatter(X_iris[:,0], X_iris[:,1], c = iris.target, cmap='viridis')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\nplt.title('Sepal length vs. Sepal width')\nplt.colorbar(label='species')\nplt.show()\n\n# Standardize features \nscaler = StandardScaler()\nX_iris_scaled = scaler.fit_transform(X_iris)\n\n# Run UMAP\numap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\nembedding_iris = umap_model.fit_transform(X_iris_scaled)\n\n# Put into DataFrame for plotting\ndf_iris = pd.DataFrame({\n    \"UMAP1\": embedding_iris[:, 0],\n    \"UMAP2\": embedding_iris[:, 1],\n    \"species\": [labels_iris[i] for i in y_iris]\n})\n\nplt.figure()\nsns.scatterplot(data=df_iris, x=\"UMAP1\", y=\"UMAP2\", hue=\"species\", s=60)\nplt.title(\"UMAP on Iris data\")\nplt.legend(loc=\"best\")\nplt.show()\n\n&lt;bound method NDFrame.head of      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     species species_name  \n0          0       setosa  \n1          0       setosa  \n2          0       setosa  \n3          0       setosa  \n4          0       setosa  \n..       ...          ...  \n145        2    virginica  \n146        2    virginica  \n147        2    virginica  \n148        2    virginica  \n149        2    virginica  \n\n[150 rows x 6 columns]&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick tips:\nn_neighbors (default ~15): how many neighbors UMAP uses to learn local structure.\nSmaller =&gt; captures very local structure (more fragmentation), larger =&gt; more global structure.\nmin_dist (default ~0.1): how tightly points are packed in the low-dimensional space. Smaller =&gt; tighter clusters; larger =&gt; more spread out.\nAlways standardize or log-transform expression data before UMAP (depending on data type).\nTry different random_state values or parameters to see what changes.\nExercises for learners:\n\n\nChange n_neighbors to 5 and then to 50 and observe how the plot changes.\nChange min_dist to 0.01 and 0.8 and observe clustering differences.\nReplace synthetic data with a small real gene-expression matrix and try the pipeline: counts -&gt; log1p -&gt; StandardScaler -&gt; UMAP.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/umap.html#summary-1",
    "href": "materials/umap.html#summary-1",
    "title": "15¬† UMAP (Optional)",
    "section": "16.11 Summary",
    "text": "16.11 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nA brief introduction to UMAP",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>UMAP (Optional)</span>"
    ]
  },
  {
    "objectID": "materials/bonus_material.html",
    "href": "materials/bonus_material.html",
    "title": "16¬† Mathematical Basics and Additional Material",
    "section": "",
    "text": "16.1 Advanced material",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Mathematical Basics and Additional Material</span>"
    ]
  },
  {
    "objectID": "materials/bonus_material.html#advanced-material",
    "href": "materials/bonus_material.html#advanced-material",
    "title": "16¬† Mathematical Basics and Additional Material",
    "section": "",
    "text": "3blue1brown videos on eigenvectors\n3blue1brown resource of eigenvectors\n3blue1brown resource on eigenvalues\nDerivations, equations and mathematics for PCA and eigenvectors\nGithub repository with more theoretical material\nBasics of vector algebra\nMathematics for Machine Learning book by Marc Deisenroth\nIntroduction to single-cell sequencing analysis\nShallow copy vs.¬†deep copy in Python",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Mathematical Basics and Additional Material</span>"
    ]
  },
  {
    "objectID": "materials/bonus_material.html#mathematical-details",
    "href": "materials/bonus_material.html#mathematical-details",
    "title": "16¬† Mathematical Basics and Additional Material",
    "section": "16.2 Mathematical details",
    "text": "16.2 Mathematical details\n\n16.2.1 Mathematics behind PCA\nHere are the key equations involved in Principal Component Analysis (PCA):\n1. Data Centering\nBefore applying PCA, the data is typically centered by subtracting the mean of each feature.\n\\(\\mathbf{X}_{centered} = \\mathbf{X} - \\mathbf{\\mu}\\)\nwhere: - \\(\\mathbf{X}\\) is the original data matrix (samples √ó features) - \\(\\mathbf{\\mu}\\) is the vector of means for each feature - \\(\\mathbf{X}_{centered}\\) is the centered data matrix\nEquations\n\\(PC_{1} = \\phi_{1} * X + \\phi_{2} * Y + \\phi_{3} * Z + ....\\)\n\nVariance = how spread out the data is.\nPCA finds directions (principal components) that maximize variance.\n\nVariance\n\n\n\n\n\n\nNoteClick to expand\n\n\n\n\n\nFormula for variance of variable \\(x\\):\n\\[\n\\text{Var}(x) = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\n\n\n\nCovariance Matrix\nThe covariance matrix captures the relationships between different features.\n\\(\\mathbf{\\Sigma} = \\frac{1}{n-1} \\mathbf{X}_{centered}^T \\mathbf{X}_{centered}\\)\nwhere: - \\(\\mathbf{\\Sigma}\\) is the covariance matrix - \\(n\\) is the number of samples - \\(\\mathbf{X}_{centered}^T\\) is the transpose of the centered data matrix\nEigenvalue Decomposition\nThe core of PCA involves finding the eigenvalues and eigenvectors of the covariance matrix.\n\\(\\mathbf{\\Sigma} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\)\nwhere: - \\(\\mathbf{\\Sigma}\\) is the covariance matrix - \\(\\mathbf{v}_i\\) is the \\(i\\)-th eigenvector - \\(\\lambda_i\\) is the \\(i\\)-th eigenvalue\nThe eigenvectors represent the principal components (the directions of maximum variance), and the eigenvalues represent the amount of variance explained by each principal component.\nSelecting Principal Components\nPrincipal components are typically ordered by their eigenvalues in descending order. You select the top \\(k\\) eigenvectors corresponding to the largest eigenvalues to form the projection matrix.\n\\(\\mathbf{W} = [\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k]\\)\nwhere: - \\(\\mathbf{W}\\) is the projection matrix (features √ó k) - \\(\\mathbf{v}_i\\) are the selected eigenvectors (principal components)\n5. Projecting Data onto New Space\nFinally, the centered data is projected onto the new lower-dimensional space defined by the selected principal components.\n\\(\\mathbf{Y} = \\mathbf{X}_{centered} \\mathbf{W}\\)\nwhere: - \\(\\mathbf{Y}\\) is the transformed data in the lower-dimensional space (samples √ó k) - \\(\\mathbf{X}_{centered}\\) is the centered data matrix - \\(\\mathbf{W}\\) is the projection matrix\nThese equations outline the mathematical process of transforming data into a new coordinate system defined by the principal components, ordered by the amount of variance they capture.\n\n\n16.2.2 Normalization (Z-score Standardization)\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\nThe formula for Z-score is:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nWhere: - \\(x\\) is the original data point. - \\(\\mu\\) is the mean of the data. - \\(\\sigma\\) is the standard deviation of the data.\n\n\n16.2.3 Details of tSNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique primarily used for visualizing high-dimensional datasets. Unlike linear methods like PCA, t-SNE is particularly good at preserving the local structure of the data, making it effective for revealing clusters and relationships between data points in a lower-dimensional space (typically 2D or 3D).\nHere are some key aspects of t-SNE:\n\nFocus on Local Structure: t-SNE aims to map high-dimensional data points to a lower-dimensional space such that the pairwise similarities between points are preserved. It does this by modeling the probability distribution of pairwise similarities in both the high-dimensional and low-dimensional spaces and minimizing the difference between these distributions. The ‚Äút-distributed‚Äù part comes from using a heavy-tailed Student‚Äôs t-distribution in the low-dimensional space to model similarities, which helps to alleviate the ‚Äúcrowding problem‚Äù where points from different clusters can be squeezed together.\nNon-linear: t-SNE is a non-linear technique, meaning it can capture complex, non-linear relationships in the data that linear methods might miss. This makes it suitable for visualizing data with intricate structures, such as the manifold-like data often seen in single-cell genomics or image datasets.\nVisualization Tool: While t-SNE can be used for dimensionality reduction, its primary strength lies in creating insightful visualizations. The plots it generates can reveal clusters, outliers, and the overall shape of the data distribution in a way that is often more interpretable than linear methods for complex data.\nPerplexity Parameter: A key parameter in t-SNE is perplexity. This parameter can be thought of as a knob that tunes the balance between focusing on local and global structure. It‚Äôs related to the number of nearest neighbors considered for each point. Choosing an appropriate perplexity is important for obtaining a meaningful visualization, and it often requires some experimentation.\nInterpretation Caution: It‚Äôs important to interpret t-SNE plots with caution. The distances between clusters in a t-SNE plot may not accurately reflect the true distances in the high-dimensional space. t-SNE is excellent at showing whether clusters exist and how points are related within those clusters, but the relative spacing and size of the clusters themselves should not be over-interpreted as precise measures of distance or density in the original data.\n\nIn summary, t-SNE is a powerful visualization tool for exploring the structure of high-dimensional data, especially when that structure is non-linear and involves distinct clusters.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Mathematical Basics and Additional Material</span>"
    ]
  },
  {
    "objectID": "materials/bonus_material.html#mathematical-definition-of-the-swiss-roll-dataset",
    "href": "materials/bonus_material.html#mathematical-definition-of-the-swiss-roll-dataset",
    "title": "16¬† Mathematical Basics and Additional Material",
    "section": "16.3 Mathematical definition of the Swiss roll dataset",
    "text": "16.3 Mathematical definition of the Swiss roll dataset\n\n16.3.1 Parametric definition\n\\([ x = \\phi \\cos(\\phi),\\quad y = \\phi \\sin(\\phi),\\quad z = \\psi ]\\)\n\n\n16.3.2 Sampling\n\\(\\phi\\) is drawn uniformly from (1.5\\(\\pi\\), 4.5\\(\\pi\\)) and \\(\\psi\\) is drawn uniformly from (0, 50).\n\n\\(\\phi \\sim \\mathrm{Uniform}(1.5\\pi,\\; 4.5\\pi)\\)\n\\(\\psi \\sim \\mathrm{Uniform}(0,\\; 50)\\)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Mathematical Basics and Additional Material</span>"
    ]
  },
  {
    "objectID": "materials/bonus_material.html#reading-papers",
    "href": "materials/bonus_material.html#reading-papers",
    "title": "16¬† Mathematical Basics and Additional Material",
    "section": "16.4 Reading papers",
    "text": "16.4 Reading papers\n\nRemember that learning does not stop once you leave class\nYou can stay in touch with your group members on Slack or Discord and discuss the latest papers using the tool Hypothesis. You can use this to run your own reading group or hackathon.\nContinue practicing your newly learnt skills on new data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Mathematical Basics and Additional Material</span>"
    ]
  },
  {
    "objectID": "materials/bonus_material.html#summary",
    "href": "materials/bonus_material.html#summary",
    "title": "16¬† Mathematical Basics and Additional Material",
    "section": "16.5 Summary",
    "text": "16.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThis section lists some resources for the advanced students and mathematically inclined.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='GitHub' >}}",
      "Bonus Material",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Mathematical Basics and Additional Material</span>"
    ]
  }
]