---
title: When PCA or tSNE might not work
---

::: {.callout-tip}
#### Learning Objectives

- Understand real-world scenarios where unsupervised learning is applied
- Identify situations where PCA and other dimensionality reduction techniques may not be effective

:::



## When PCA or tSNE may *not* work


- **Nonlinear structure**
  - Swiss roll/curved trajectories (e.g., differentiation trajectories); PCA flattens and mixes cells that are nearby in 2D but far along the curve.
  - Concentric patterns (e.g., ring-like responses); PCA cannot separate circles.

- **When variance ≠ signal**
  - Batch effects or library size dominate variance in scRNA‑seq; PCs reflect technical factors rather than biology.
  - Cell cycle effects overshadow subtle lineage differences.
  - Rare cell types: biologically important but low variance, thus missed by top PCs.
  <!-- 
  - XOR-like structure: classes differ along interactions, not along single linear directions.
  -->

- **Outliers and heavy tails**
  - A few extreme samples/genes drive the first PCs, masking true structure (common with QC issues or outlier libraries).

- **Feature scaling and units**
  - Mixed units or unscaled features: high-variance genes/proteins dominate; low-variance but important markers get ignored.

<!--
- **Compositional/count data**
  - Microbiome (relative abundances) or RNA-seq counts: closure and zero inflation violate PCA assumptions; raw-count PCA is misleading.

- **Multimodal mixtures**
  - Multiple clusters with different orientations/variances: a single global linear projection can blur cluster separation.
-->

- **Time/phase structure**
  - Periodic processes (cell cycle phases): PCA captures amplitude rather than phase, mixing states.

- **p >> n instability**
  - Many more genes than samples: PCs become noisy/unstable without regularization or careful preprocessing.

- **Missing data**
  - Nonrandom missingness (dropouts in scRNA‑seq) biases covariance; naive imputation can create artificial PCs.

### Quick remedies
- Nonlinear structure: t‑SNE/UMAP. <!--, diffusion maps, PHATE, kernel PCA. -->
- Variance ≠ signal: regress out batch/cell cycle; use sctransform; combat/BBKNN/Harmony for batch correction; consider supervised methods (PLS/CCA) if labels exist.
- Outliers/heavy tails: robust PCA, rank genes with robust dispersion, Winsorize/log1p.
- Scaling/units: standardize features; use variance-stabilizing transforms (log1p, VST).
<!--
- Compositional data: CLR/ALR transforms; use compositional-aware methods before PCA.
- p >> n: feature selection (highly variable genes), shrinkage covariance, cross-validation for PC selection.
-->


### **Non-linear data**

- **Non-linearity**: Data that lies on curved surfaces or when data has non-linear relationships.
- **Single-cell data**: Biological data where cell types form non-linear clusters in high-dimensional space

### **Categorical Features**
- PCA may work poorly with categorical data unless properly encoded
- One-hot encoding categorical features can create sparse, high-dimensional data where PCA may not capture meaningful structure


<!-- TODO: CCA comment by Hugo -->

<!--
### 3. **When Variance Direction ≠ Structure Direction**
- PCA maximizes variance along principal components, but this doesn't always align with the true underlying structure
- The lesson shows how PCA on XOR data captures only ~15% variance per component, failing to reveal the true four-cluster structure
-->

## Alternatives

### **t-SNE** (t-Distributed Stochastic Neighbor Embedding)
- **Best for**: Non-linear dimensionality reduction and visualization
- **Key parameter**: Perplexity (try values 5-50)
- **Use case**: Single-cell data, biological expression data, any non-linear clustering


::: {.callout-tip}
**NOTE (IMPORTANT CONCEPT)**: Sometimes tSNE may not work as well! It is hard to predict which unsupervised machine learning technique will work best.

You just need to try a bunch of different techniques.
:::
<!-- end callout -->


<!--
### **Autoencoders**
- **Best for**: Complex non-linear relationships in deep learning contexts
- **Use case**: When you need to learn complex representations
-->

### **Hierarchical Clustering + Heatmaps**
- **Best for**: Categorical data and understanding relationships between samples
- **Use case**: When you want to see how samples group together based on multiple features



### Activity: Demonstrating how PCA may not work well on the Swiss roll data

* The `Swiss roll` dataset

A 2D curve embedded in 3D that looks like a rolled sheet. You can play around with the plot below! 
<!--; distances along the roll are nonlinear, so linear projections (like PCA) flatten and mix points that are far along the spiral but close in Euclidean space.-->

```{python ch4-swissroll}
#| warning: false
#| echo: false

from sklearn.datasets import make_swiss_roll
import plotly.graph_objects as go

X, t = make_swiss_roll(n_samples=2000, noise=0.05, random_state=42)

fig = go.Figure(go.Scatter3d(
    x=X[:,0], y=X[:,1], z=X[:,2],
    mode="markers",
    marker=dict(size=3, color=t, colorscale="Turbo", showscale=False, opacity=0.85)
))
fig.update_layout(scene=dict(aspectmode="data"))
fig
```

* Performing PCA on the Swiss roll dataset

```{python ch4-pca-notw-ro-swiss-roll}
#| warning: false
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.datasets import make_swiss_roll, make_circles
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


def generate_swiss_roll(n_samples: int = 1500, noise: float = 0.05, random_state: int = 42):
    X, t = make_swiss_roll(n_samples=n_samples, noise=noise, random_state=random_state)
    # Only keep X, Y, Z
    return X, t


def generate_concentric_circles(n_samples: int = 1000, noise: float = 0.06, factor: float = 0.5, random_state: int = 42):
    X, y = make_circles(n_samples=n_samples, noise=noise, factor=factor, random_state=random_state)
    return X, y


def apply_pca(X: np.ndarray, n_components: int = 2, scale: bool = True):
    if scale:
        X_proc = StandardScaler().fit_transform(X)
    else:
        X_proc = X
    pca = PCA(n_components=n_components, random_state=0)
    X_pca = pca.fit_transform(X_proc)
    return X_pca, pca


def plot_swiss_roll(X3d: np.ndarray, color: np.ndarray, Xpca: np.ndarray, out_prefix: str = "swiss_roll"):
    # Figure 1: 3D Swiss roll
    fig1 = plt.figure(figsize=(6, 5))
    ax1 = fig1.add_subplot(1, 1, 1, projection="3d")
    sc1 = ax1.scatter(X3d[:, 0], X3d[:, 1], X3d[:, 2], c=color, cmap="Spectral", s=5)
    ax1.set_title("Swiss roll (3D)")
    ax1.set_xlabel("X")
    ax1.set_ylabel("Y")
    ax1.set_zlabel("Z")
    cbar1 = fig1.colorbar(sc1, ax=ax1, fraction=0.046, pad=0.04)
    cbar1.set_label("Intrinsic parameter t")
    fig1.tight_layout()
    fig1.savefig(f"{out_prefix}_3d.png", dpi=200)
    plt.show()
    #plt.close(fig1)

    # Figure 2: 2D PCA projection
    fig2, ax2 = plt.subplots(1, 1, figsize=(6, 5))
    sc2 = ax2.scatter(Xpca[:, 0], Xpca[:, 1], c=color, cmap="Spectral", s=5)
    ax2.set_title("PCA projection (2D): flattens and mixes trajectory")
    ax2.set_xlabel("PC1")
    ax2.set_ylabel("PC2")
    cbar2 = fig2.colorbar(sc2, ax=ax2, fraction=0.046, pad=0.04)
    cbar2.set_label("Intrinsic parameter t")
    fig2.tight_layout()
    fig2.savefig(f"{out_prefix}_pca.png", dpi=200)
    plt.show()
    #plt.close(fig2)


def plot_circles(X2d: np.ndarray, y: np.ndarray, Xpca: np.ndarray, out_prefix: str = "circles"):
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))

    cmap = ListedColormap(["#1f77b4", "#ff7f0e"])  # blue, orange
    axes[0].scatter(X2d[:, 0], X2d[:, 1], c=y, cmap=cmap, s=8)
    axes[0].set_title("Concentric circles (nonlinear)")
    axes[0].set_xlabel("x1")
    axes[0].set_ylabel("x2")

    axes[1].scatter(Xpca[:, 0], Xpca[:, 1], c=y, cmap=cmap, s=8)
    axes[1].set_title("PCA projection: cannot separate rings linearly")
    axes[1].set_xlabel("PC1")
    axes[1].set_ylabel("PC2")

    fig.tight_layout()
    fig.savefig(f"{out_prefix}_pca.png", dpi=200)
    plt.show()
    #plt.close(fig)


# Swiss roll example
X3d, t = generate_swiss_roll(n_samples=1500, noise=0.05, random_state=42)
Xpca_swiss, pca_swiss = apply_pca(X3d, n_components=2, scale=True)
plot_swiss_roll(X3d, t, Xpca_swiss, out_prefix="swiss_roll")

# Concentric circles example
X2d, y = generate_concentric_circles(n_samples=1000, noise=0.06, factor=0.5, random_state=42)
Xpca_circles, pca_circles = apply_pca(X2d, n_components=2, scale=True)
plot_circles(X2d, y, Xpca_circles, out_prefix="circles")
```

* Performing tSNE on the Swiss roll dataset

```{python ch4-tsne-swissroll}
#| warning: false
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.datasets import make_swiss_roll, make_circles
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE


def generate_swiss_roll(n_samples: int = 1500, noise: float = 0.05, random_state: int = 42):
    X, t = make_swiss_roll(n_samples=n_samples, noise=noise, random_state=random_state)
    # Only keep X, Y, Z
    return X, t


def generate_concentric_circles(n_samples: int = 1000, noise: float = 0.06, factor: float = 0.5, random_state: int = 42):
    X, y = make_circles(n_samples=n_samples, noise=noise, factor=factor, random_state=random_state)
    return X, y


def apply_pca(X: np.ndarray, n_components: int = 2, scale: bool = True):
    if scale:
        X_proc = StandardScaler().fit_transform(X)
    else:
        X_proc = X
    pca = PCA(n_components=n_components, random_state=0)
    X_pca = pca.fit_transform(X_proc)
    return X_pca, pca


def apply_tsne(
    X: np.ndarray,
    n_components: int = 2,
    perplexity: float = 30.0,
    random_state: int = 42,
    scale: bool = True,
):
    if scale:
        X_proc = StandardScaler().fit_transform(X)
    else:
        X_proc = X
    tsne = TSNE(
        n_components=n_components,
        perplexity=perplexity,
        random_state=random_state,
        init="pca",
        learning_rate="auto",
        verbose=0,
    )
    X_tsne = tsne.fit_transform(X_proc)
    return X_tsne, tsne


def plot_swiss_roll(X3d: np.ndarray, color: np.ndarray, Xpca: np.ndarray, out_prefix: str = "swiss_roll"):
    # Figure 1: 3D Swiss roll
    fig1 = plt.figure(figsize=(6, 5))
    ax1 = fig1.add_subplot(1, 1, 1, projection="3d")
    sc1 = ax1.scatter(X3d[:, 0], X3d[:, 1], X3d[:, 2], c=color, cmap="Spectral", s=5)
    ax1.set_title("Swiss roll (3D)")
    ax1.set_xlabel("X")
    ax1.set_ylabel("Y")
    ax1.set_zlabel("Z")
    cbar1 = fig1.colorbar(sc1, ax=ax1, fraction=0.046, pad=0.04)
    cbar1.set_label("Intrinsic parameter t")
    fig1.tight_layout()
    fig1.savefig(f"{out_prefix}_3d.png", dpi=200)
    plt.show()
    plt.close(fig1)

    # Figure 2: 2D PCA projection
    fig2, ax2 = plt.subplots(1, 1, figsize=(6, 5))
    sc2 = ax2.scatter(Xpca[:, 0], Xpca[:, 1], c=color, cmap="Spectral", s=5)
    ax2.set_title("PCA projection (2D): flattens and mixes trajectory")
    ax2.set_xlabel("PC1")
    ax2.set_ylabel("PC2")
    cbar2 = fig2.colorbar(sc2, ax=ax2, fraction=0.046, pad=0.04)
    cbar2.set_label("Intrinsic parameter t")
    fig2.tight_layout()
    fig2.savefig(f"{out_prefix}_pca.png", dpi=200)
    plt.show()
    plt.close(fig2)


def plot_tsne_swiss(X_tsne: np.ndarray, color: np.ndarray, out_prefix: str = "swiss_roll_tsne"):
    fig, ax = plt.subplots(1, 1, figsize=(6, 5))
    sc = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], s=5) # , c=color, cmap="Spectral"
    ax.set_title("t-SNE (2D) of Swiss roll: preserves local neighbourhoods")
    ax.set_xlabel("t-SNE 1")
    ax.set_ylabel("t-SNE 2")
    cbar = fig.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label("Intrinsic parameter t")
    fig.tight_layout()
    fig.savefig(f"{out_prefix}.png", dpi=200)
    plt.show()
    plt.close(fig)


def plot_circles(X2d: np.ndarray, y: np.ndarray, Xpca: np.ndarray, out_prefix: str = "circles"):
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))

    cmap = ListedColormap(["#1f77b4", "#ff7f0e"])  # blue, orange
    axes[0].scatter(X2d[:, 0], X2d[:, 1], c=y, cmap=cmap, s=8)
    axes[0].set_title("Concentric circles (nonlinear)")
    axes[0].set_xlabel("x1")
    axes[0].set_ylabel("x2")

    axes[1].scatter(Xpca[:, 0], Xpca[:, 1], c=y, cmap=cmap, s=8)
    axes[1].set_title("PCA projection: cannot separate rings linearly")
    axes[1].set_xlabel("PC1")
    axes[1].set_ylabel("PC2")

    fig.tight_layout()
    fig.savefig(f"{out_prefix}_pca.png", dpi=200)
    plt.show()
    plt.close(fig)


# Swiss roll example
#X3d, t = generate_swiss_roll(n_samples=1500, noise=0.05, random_state=42)
#Xpca_swiss, pca_swiss = apply_pca(X3d, n_components=2, scale=True)
#plot_swiss_roll(X3d, t, Xpca_swiss, out_prefix="swiss_roll")

# t-SNE on Swiss roll
Xtsne_swiss, tsne_swiss = apply_tsne(X3d, n_components=2, perplexity=30.0, random_state=42, scale=True)
plot_tsne_swiss(Xtsne_swiss, t, out_prefix="swiss_roll_tsne")

# Concentric circles example
#X2d, y = generate_concentric_circles(n_samples=1000, noise=0.06, factor=0.5, random_state=42)
#Xpca_circles, pca_circles = apply_pca(X2d, n_components=2, scale=True)
#plot_circles(X2d, y, Xpca_circles, out_prefix="circles")
```


### PCA on time series data and spiral data

```{python ch4-pca-timeseries-plotly}
#| warning: false
#| echo: false
#| eval: false
#| output: false

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import ipywidgets as widgets
from IPython.display import display

class PCAFailureDemo:
    def __init__(self):
        self.datasets = {}
        self.generate_all_datasets()
        
    def generate_horseshoe_data(self, n=200):
        """Generate horseshoe-shaped data"""
        t = np.linspace(0, np.pi, n)
        noise = 0.1
        x = np.cos(t) + np.random.normal(0, noise, n)
        y = np.sin(t) + np.random.normal(0, noise, n)
        
        return pd.DataFrame({
            'x': x, 
            'y': y,
            'sequence': range(n),
            'dataset': 'horseshoe'
        })
    
    def generate_spiral_data(self, n=200):
        """Generate spiral data"""
        t = np.linspace(0, 4 * np.pi, n)
        r = 0.5 + np.linspace(0, 2, n)
        noise = 0.05
        x = r * np.cos(t) + np.random.normal(0, noise, n)
        y = r * np.sin(t) + np.random.normal(0, noise, n)
        
        return pd.DataFrame({
            'x': x,
            'y': y, 
            'sequence': range(n),
            'dataset': 'spiral'
        })
    
    def generate_timeseries_data(self, n=200):
        """Generate nonlinear time series data"""
        t = np.linspace(0, 4 * np.pi, n)
        noise = 0.1
        x = t
        y = np.sin(t) + 0.3 * np.sin(3 * t) + np.random.normal(0, noise, n)
        
        return pd.DataFrame({
            'x': x,
            'y': y,
            'sequence': range(n),
            'dataset': 'timeseries'
        })
    
    def generate_all_datasets(self):
        """Generate all datasets"""
        self.datasets = {
            'horseshoe': self.generate_horseshoe_data(),
            'spiral': self.generate_spiral_data(), 
            'timeseries': self.generate_timeseries_data()
        }
    
    def perform_pca(self, data):
        """Perform PCA on the data"""
        X = data[['x', 'y']].values
        
        # Standardize the data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Perform PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        
        # Create results dataframe
        pca_data = pd.DataFrame({
            'PC1': X_pca[:, 0],
            'PC2': X_pca[:, 1], 
            'sequence': data['sequence'],
            'dataset': data['dataset'].iloc[0] + '_pca'
        })
        
        return pca_data, pca, scaler
    
    def create_comparison_plot(self, dataset_name, show_pca=False):
        """Create comparison plot showing original data and optionally PCA"""
        original_data = self.datasets[dataset_name].copy()
        
        if show_pca:
            pca_data, pca_model, scaler = self.perform_pca(original_data)
            
            # Create subplot
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=('Original Data', 'PCA Projection'),
                horizontal_spacing=0.1
            )
            
            # Original data plot
            fig.add_trace(
                go.Scatter(
                    x=original_data['x'], 
                    y=original_data['y'],
                    mode='markers',
                    marker=dict(
                        size=6,
                        color=original_data['sequence'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="Sequence", x=0.45)
                    ),
                    name='Original',
                    showlegend=False
                ),
                row=1, col=1
            )
            
            # PCA projection plot
            fig.add_trace(
                go.Scatter(
                    x=pca_data['PC1'],
                    y=pca_data['PC2'], 
                    mode='markers',
                    marker=dict(
                        size=6,
                        color=pca_data['sequence'],
                        colorscale='Plasma',
                        showscale=True,
                        colorbar=dict(title="Sequence", x=1.02)
                    ),
                    name='PCA',
                    showlegend=False
                ),
                row=1, col=2
            )
            
            # Add PCA component arrows to original plot
            mean_x, mean_y = original_data[['x', 'y']].mean()
            components = pca_model.components_
            
            # Scale arrows for visibility
            scale = 2.0 if dataset_name != 'timeseries' else 5.0
            
            for i, (comp, color, name) in enumerate(zip(components, ['red', 'orange'], ['PC1', 'PC2'])):
                fig.add_annotation(
                    x=mean_x + comp[0] * scale,
                    y=mean_y + comp[1] * scale,
                    ax=mean_x,
                    ay=mean_y,
                    xref=f'x', yref=f'y',
                    axref=f'x', ayref=f'y',
                    arrowhead=2,
                    arrowsize=1,
                    arrowwidth=3,
                    arrowcolor=color,
                    row=1, col=1
                )
                
                fig.add_annotation(
                    text=name,
                    x=mean_x + comp[0] * scale * 1.2,
                    y=mean_y + comp[1] * scale * 1.2,
                    showarrow=False,
                    font=dict(color=color, size=12),
                    row=1, col=1
                )
            
            # Add variance explained text
            variance_ratio = pca_model.explained_variance_ratio_
            fig.add_annotation(
                text=f"PC1: {variance_ratio[0]:.1%}<br>PC2: {variance_ratio[1]:.1%}",
                xref="x2", yref="y2",
                x=0.02, y=0.98,
                xanchor="left", yanchor="top",
                showarrow=False,
                font=dict(size=12),
                bgcolor="rgba(255,255,255,0.8)",
                row=1, col=2
            )
            
            # Update axis labels for subplots
            if dataset_name == 'timeseries':
                fig.update_xaxes(title="Time", row=1, col=1)
                fig.update_yaxes(title="Value", row=1, col=1)
            else:
                fig.update_xaxes(title="X", row=1, col=1)
                fig.update_yaxes(title="Y", row=1, col=1)
                
            fig.update_xaxes(title="PC1", row=1, col=2)
            fig.update_yaxes(title="PC2", row=1, col=2)
            
        else:
            # Single plot with original data only
            fig = go.Figure()
            
            fig.add_trace(
                go.Scatter(
                    x=original_data['x'],
                    y=original_data['y'],
                    mode='markers',
                    marker=dict(
                        size=8,
                        color=original_data['sequence'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="Sequence")
                    ),
                    name='Original Data'
                )
            )
            
            # Update axis labels for single plot
            if dataset_name == 'timeseries':
                fig.update_xaxes(title="Time")
                fig.update_yaxes(title="Value")
            else:
                fig.update_xaxes(title="X")
                fig.update_yaxes(title="Y")
        
        # Update layout
        title = f"{dataset_name.title()} Data"
        if show_pca:
            title += " - Original vs PCA Projection"
            
        fig.update_layout(
            title=title,
            height=500,
            showlegend=False
        )
        
        return fig
    
    def create_interactive_widget(self):
        """Create interactive widget for exploring PCA failures"""
        
        # Create widgets
        dataset_dropdown = widgets.Dropdown(
            options=[('Horseshoe Data', 'horseshoe'),
                    ('Spiral Data', 'spiral'), 
                    ('Time Series Data', 'timeseries')],
            value='horseshoe',
            description='Dataset:'
        )
        
        show_pca_checkbox = widgets.Checkbox(
            value=False,
            description='Show PCA Projection'
        )
        
        regenerate_button = widgets.Button(
            description='Regenerate Data',
            button_style='info'
        )
        
        output = widgets.Output()
        
        def update_plot(*args):
            with output:
                output.clear_output(wait=True)
                fig = self.create_comparison_plot(
                    dataset_dropdown.value, 
                    show_pca_checkbox.value
                )
                fig.show()
        
        def regenerate_data(*args):
            self.generate_all_datasets()
            update_plot()
        
        # Connect widgets to functions
        dataset_dropdown.observe(update_plot, names='value')
        show_pca_checkbox.observe(update_plot, names='value')
        regenerate_button.on_click(regenerate_data)
        
        # Initial plot
        update_plot()
        
        # Display widgets
        controls = widgets.VBox([
            widgets.HBox([dataset_dropdown, show_pca_checkbox, regenerate_button]),
            output
        ])
        
        return controls
    
    def analyze_pca_failure(self, dataset_name):
        """Analyze and print why PCA fails for this dataset"""
        data = self.datasets[dataset_name]
        pca_data, pca_model, scaler = self.perform_pca(data)
        
        print(f"PCA Analysis for {dataset_name.title()} Data")
        print("=" * 50)
        
        variance_ratio = pca_model.explained_variance_ratio_
        print(f"PC1 explains {variance_ratio[0]:.1%} of variance")
        print(f"PC2 explains {variance_ratio[1]:.1%} of variance")
        print(f"Total variance explained: {sum(variance_ratio):.1%}")
        
        print("\nWhy PCA fails here:")
        
        if dataset_name == 'horseshoe':
            print("• The data forms a nonlinear U-shaped manifold")
            print("• PCA finds linear directions but misses the curved structure") 
            print("• The intrinsic dimensionality is 1D (position along curve)")
            print("• But PCA treats it as 2D problem")
            
        elif dataset_name == 'spiral':
            print("• Data follows a spiral pattern with rotational structure")
            print("• PCA cannot capture circular/rotational relationships")
            print("• Sequential ordering (temporal aspect) is lost")
            print("• Linear projection destroys the spiral geometry")
            
        elif dataset_name == 'timeseries':
            print("• This is actually a function y = f(t) where t is time")
            print("• PCA treats it as 2D spatial data, ignoring temporal structure")
            print("• The relationship is nonlinear (sine waves)")
            print("• Important frequency components are lost in projection")
        
        print("\nBetter alternatives:")
        print("• Manifold learning: t-SNE, UMAP, Isomap")
        print("• Kernel PCA for nonlinear relationships")
        print("• Autoencoders for complex nonlinear dimensionality reduction")
        
        if dataset_name == 'timeseries':
            print("• For time series: Fourier analysis, wavelet transforms")
            print("• Recurrent neural networks for temporal patterns")

# Usage examples:
def run_demo():
    """Run the interactive demo"""
    demo = PCAFailureDemo()
    return demo.create_interactive_widget()

def create_static_plots():
    """Create static plots for all datasets"""
    demo = PCAFailureDemo()
    
    #for dataset_name in ['horseshoe', 'spiral', 'timeseries']:
    for dataset_name in ['timeseries', 'spiral' ]:
        print(f"\n--- {dataset_name.upper()} DATA ---")
        
        # Show original data
        print("1. Original Data:")
        fig1 = demo.create_original_plot(dataset_name)
        fig1.show()
        
        # Show original with PCA components
        print("2. Original Data with PCA Components:")
        fig2 = demo.create_pca_comparison_plot(dataset_name)
        fig2.show()
        
        # Show PCA projection
        print("3. PCA Projection:")
        fig3 = demo.create_pca_projection_plot(dataset_name)
        fig3.show()
        
        # Print analysis
        demo.analyze_pca_failure(dataset_name)
        print("\n" + "="*60)

# If running in Jupyter notebook:
# demo = PCAFailureDemo()
# demo.create_interactive_widget()

# Create demo instance
demo = PCAFailureDemo()
    
#create_static_plots()
```

```{python ch4-pca-timeseries-notwork}
#| warning: false
#| echo: false

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import ipywidgets as widgets
from IPython.display import display

class PCAFailureDemo:
    def __init__(self):
        self.datasets = {}
        self.generate_all_datasets()
        
    def generate_horseshoe_data(self, n=200):
        """Generate horseshoe-shaped data"""
        t = np.linspace(0, np.pi, n)
        noise = 0.1
        x = np.cos(t) + np.random.normal(0, noise, n)
        y = np.sin(t) + np.random.normal(0, noise, n)
        
        return pd.DataFrame({
            'x': x, 
            'y': y,
            'sequence': range(n),
            'dataset': 'horseshoe'
        })
    
    def generate_spiral_data(self, n=200):
        """Generate spiral data"""
        t = np.linspace(0, 4 * np.pi, n)
        r = 0.5 + np.linspace(0, 2, n)
        noise = 0.05
        x = r * np.cos(t) + np.random.normal(0, noise, n)
        y = r * np.sin(t) + np.random.normal(0, noise, n)
        
        return pd.DataFrame({
            'x': x,
            'y': y, 
            'sequence': range(n),
            'dataset': 'spiral'
        })
    
    def generate_timeseries_data(self, n=200):
        """Generate nonlinear time series data"""
        t = np.linspace(0, 4 * np.pi, n)
        noise = 0.1
        x = t
        y = np.sin(t) + 0.3 * np.sin(3 * t) + np.random.normal(0, noise, n)
        
        return pd.DataFrame({
            'x': x,
            'y': y,
            'sequence': range(n),
            'dataset': 'timeseries'
        })
    
    def generate_all_datasets(self):
        """Generate all datasets"""
        self.datasets = {
            'horseshoe': self.generate_horseshoe_data(),
            'spiral': self.generate_spiral_data(), 
            'timeseries': self.generate_timeseries_data()
        }
    
    def perform_pca(self, data):
        """Perform PCA on the data"""
        X = data[['x', 'y']].values
        
        # Standardize the data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Perform PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        
        # Create results dataframe
        pca_data = pd.DataFrame({
            'PC1': X_pca[:, 0],
            'PC2': X_pca[:, 1], 
            'sequence': data['sequence'],
            'dataset': data['dataset'].iloc[0] + '_pca'
        })
        
        return pca_data, pca, scaler
    
    def create_comparison_plot(self, dataset_name, show_pca=False):
        """Create comparison plot showing original data and optionally PCA"""
        original_data = self.datasets[dataset_name].copy()
        
        if show_pca:
            pca_data, pca_model, scaler = self.perform_pca(original_data)
            
            # Create subplot
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=('Original Data', 'PCA Projection'),
                horizontal_spacing=0.1
            )
            
            # Original data plot
            fig.add_trace(
                go.Scatter(
                    x=original_data['x'], 
                    y=original_data['y'],
                    mode='markers',
                    marker=dict(
                        size=6,
                        color=original_data['sequence'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="Sequence", x=0.45)
                    ),
                    name='Original',
                    showlegend=False
                ),
                row=1, col=1
            )
            
            # PCA projection plot
            fig.add_trace(
                go.Scatter(
                    x=pca_data['PC1'],
                    y=pca_data['PC2'], 
                    mode='markers',
                    marker=dict(
                        size=6,
                        color=pca_data['sequence'],
                        colorscale='Plasma',
                        showscale=True,
                        colorbar=dict(title="Sequence", x=1.02)
                    ),
                    name='PCA',
                    showlegend=False
                ),
                row=1, col=2
            )
            
            # Add PCA component arrows to original plot
            mean_x, mean_y = original_data[['x', 'y']].mean()
            components = pca_model.components_
            
            # Scale arrows for visibility
            scale = 2.0 if dataset_name != 'timeseries' else 5.0
            
            for i, (comp, color, name) in enumerate(zip(components, ['red', 'orange'], ['PC1', 'PC2'])):
                fig.add_annotation(
                    x=mean_x + comp[0] * scale,
                    y=mean_y + comp[1] * scale,
                    ax=mean_x,
                    ay=mean_y,
                    xref=f'x', yref=f'y',
                    axref=f'x', ayref=f'y',
                    arrowhead=2,
                    arrowsize=1,
                    arrowwidth=3,
                    arrowcolor=color,
                    row=1, col=1
                )
                
                fig.add_annotation(
                    text=name,
                    x=mean_x + comp[0] * scale * 1.2,
                    y=mean_y + comp[1] * scale * 1.2,
                    showarrow=False,
                    font=dict(color=color, size=12),
                    row=1, col=1
                )
            
            # Add variance explained text
            variance_ratio = pca_model.explained_variance_ratio_
            fig.add_annotation(
                text=f"PC1: {variance_ratio[0]:.1%}<br>PC2: {variance_ratio[1]:.1%}",
                xref="x2", yref="y2",
                x=0.02, y=0.98,
                xanchor="left", yanchor="top",
                showarrow=False,
                font=dict(size=12),
                bgcolor="rgba(255,255,255,0.8)",
                row=1, col=2
            )
            
            # Update axis labels for subplots
            if dataset_name == 'timeseries':
                fig.update_xaxes(title="Time", row=1, col=1)
                fig.update_yaxes(title="Value", row=1, col=1)
            else:
                fig.update_xaxes(title="X", row=1, col=1)
                fig.update_yaxes(title="Y", row=1, col=1)
                
            fig.update_xaxes(title="PC1", row=1, col=2)
            fig.update_yaxes(title="PC2", row=1, col=2)
            
        else:
            # Single plot with original data only
            fig = go.Figure()
            
            fig.add_trace(
                go.Scatter(
                    x=original_data['x'],
                    y=original_data['y'],
                    mode='markers',
                    marker=dict(
                        size=8,
                        color=original_data['sequence'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="Sequence")
                    ),
                    name='Original Data'
                )
            )
            
            # Update axis labels for single plot
            if dataset_name == 'timeseries':
                fig.update_xaxes(title="Time")
                fig.update_yaxes(title="Value")
            else:
                fig.update_xaxes(title="X")
                fig.update_yaxes(title="Y")
        
        # Update layout
        title = f"{dataset_name.title()} Data"
        if show_pca:
            title += " - Original vs PCA Projection"
            
        fig.update_layout(
            title=title,
            height=500,
            showlegend=False
        )
        
        return fig
    
    def create_interactive_widget(self):
        """Create interactive widget for exploring PCA failures"""
        
        # Create widgets
        dataset_dropdown = widgets.Dropdown(
            options=[('Horseshoe Data', 'horseshoe'),
                    ('Spiral Data', 'spiral'), 
                    ('Time Series Data', 'timeseries')],
            value='horseshoe',
            description='Dataset:'
        )
        
        show_pca_checkbox = widgets.Checkbox(
            value=False,
            description='Show PCA Projection'
        )
        
        regenerate_button = widgets.Button(
            description='Regenerate Data',
            button_style='info'
        )
        
        output = widgets.Output()
        
        def update_plot(*args):
            with output:
                output.clear_output(wait=True)
                fig = self.create_comparison_plot(
                    dataset_dropdown.value, 
                    show_pca_checkbox.value
                )
                fig.show()
        
        def regenerate_data(*args):
            self.generate_all_datasets()
            update_plot()
        
        # Connect widgets to functions
        dataset_dropdown.observe(update_plot, names='value')
        show_pca_checkbox.observe(update_plot, names='value')
        regenerate_button.on_click(regenerate_data)
        
        # Initial plot
        update_plot()
        
        # Display widgets
        controls = widgets.VBox([
            widgets.HBox([dataset_dropdown, show_pca_checkbox, regenerate_button]),
            output
        ])
        
        return controls
    
    def analyze_pca_failure(self, dataset_name):
        """Analyze and print why PCA fails for this dataset"""
        data = self.datasets[dataset_name]
        pca_data, pca_model, scaler = self.perform_pca(data)
        
        print(f"PCA Analysis for {dataset_name.title()} Data")
        print("=" * 50)
        
        variance_ratio = pca_model.explained_variance_ratio_
        print(f"PC1 explains {variance_ratio[0]:.1%} of variance")
        print(f"PC2 explains {variance_ratio[1]:.1%} of variance")
        print(f"Total variance explained: {sum(variance_ratio):.1%}")
        
        print("\nWhy PCA fails here:")
        
        if dataset_name == 'horseshoe':
            print("• The data forms a nonlinear U-shaped manifold")
            print("• PCA finds linear directions but misses the curved structure") 
            print("• The intrinsic dimensionality is 1D (position along curve)")
            print("• But PCA treats it as 2D problem")
            
        elif dataset_name == 'spiral':
            print("• Data follows a spiral pattern with rotational structure")
            print("• PCA cannot capture circular/rotational relationships")
            print("• Sequential ordering (temporal aspect) is lost")
            print("• Linear projection destroys the spiral geometry")
            
        elif dataset_name == 'timeseries':
            print("• This is actually a function y = f(t) where t is time")
            print("• PCA treats it as 2D spatial data, ignoring temporal structure")
            print("• The relationship is nonlinear (sine waves)")
            print("• Important frequency components are lost in projection")
        
        print("\nBetter alternatives:")
        print("• Manifold learning: t-SNE, UMAP, Isomap")
        print("• Kernel PCA for nonlinear relationships")
        print("• Autoencoders for complex nonlinear dimensionality reduction")
        
        if dataset_name == 'timeseries':
            print("• For time series: Fourier analysis, wavelet transforms")
            print("• Recurrent neural networks for temporal patterns")

# Usage examples:
def run_demo():
    """Run the interactive demo"""
    demo = PCAFailureDemo()
    return demo.create_interactive_widget()

def create_static_plots():
    """Create static plots for all datasets"""
    demo = PCAFailureDemo()
    
    #for dataset_name in ['horseshoe', 'spiral', 'timeseries']:
    for dataset_name in ['timeseries', 'spiral']:
        print(f"\n--- {dataset_name.upper()} DATA ---")
        
        # Show original data
        fig1 = demo.create_comparison_plot(dataset_name, show_pca=False)
        fig1.show()
        
        # Show PCA comparison
        fig2 = demo.create_comparison_plot(dataset_name, show_pca=True)
        fig2.show()
        
        # Print analysis
        demo.analyze_pca_failure(dataset_name)
        print("\n" + "="*60)

# If running in Jupyter notebook:
demo = PCAFailureDemo()
demo.create_interactive_widget()

# Create demo instance
demo = PCAFailureDemo()
    
create_static_plots()
```

### Missing data

PCA and tSNE also do not work well when there is missing data. See Exercise @sec-clustering-missing-data.

### Demonstrating how PCA or tSNE may not work well on biological data

* Generate synthetic biological expression data: matrix of 200 samples × 10 genes, where Gene_1 and Gene_2 follow a clustering (four corner clusters) and the remaining genes are just Gaussian noise. You can see from the scatter of Gene_1 vs Gene_2 that the true structure is non-linear and not aligned with any single variance direction: PCA (or tSNE) may fail to unfold these clusters into separate principal components.

```{python ch4-pca-not-work}
#| warning: false
#| echo: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Set random seed for reproducibility
np.random.seed(42)

# Parameters
n_samples = 200
n_genes = 10

# Generate XOR structure for genes 1 and 2
gene1 = np.random.choice([0, 1], size=n_samples)
gene2 = np.random.choice([0,1],  size=n_samples)
# For continuous spread, add small noise
gene1 = gene1 + 0.1 * np.random.randn(n_samples)
gene2 = gene2 + 0.1 * np.random.randn(n_samples)

# Generate remaining genes as random noise
other_genes = np.random.randn(n_samples, n_genes - 2)

# Combine into a DataFrame
data = np.hstack([gene1.reshape(-1,1), gene2.reshape(-1,1), other_genes])
genes = [f'Gene_{i+1}' for i in range(n_genes)]
df = pd.DataFrame(data, columns=genes)

# Scatter plot for Gene_1 vs Gene_2 to visualize the non-linear structure
plt.figure()
plt.scatter(df['Gene_1'], df['Gene_2'])
plt.title('Scatter of Gene_1 vs Gene_2')
plt.xlabel('Gene_1 expression')
plt.ylabel('Gene_2 expression')
plt.show()
```

* Perform PCA on this data

```{python ch4-pca-synthetic_data}
#| warning: false

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA
pca = PCA()
pcs = pca.fit_transform(df) # where df is a dataframe with your data


# Scatter plot of the first two principal components
plt.figure()
plt.scatter(pcs[:, 0], pcs[:, 1])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA on Synthetic Biological Dataset')
plt.show()
```

* Let us try tSNE on this data

```{python ch4-tsne-synthetic-data}
#| warning: false

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE()
tsne_results = tsne.fit_transform(df)

# plot
plt.figure()
plt.scatter(tsne_results[:,0], tsne_results[:,1])
plt.xlabel('t-SNE component 1')
plt.ylabel('t-SNE component 2')
plt.title('t-SNE on Synthetic Biological Dataset')
plt.show()

```

* What if we try different values of *perplexity*?

```{python ch4-tnse-perplexity-vary-synthetic-data}
#| warning: false
#| echo: false

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# Convert DataFrame to numpy array
data = df.values

# Define different perplexity values to explore
perplexities = [2, 5, 30, 50, 100] # Exploring values between 2 and 100

plt.figure(figsize=(20,5)) # Adjust figure size for more subplots

for i, perplexity in enumerate(perplexities):
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
    tsne_results = tsne.fit_transform(data)

    # Create a subplot for each perplexity value
    plt.subplot(1, len(perplexities), i + 1)
    plt.scatter(tsne_results[:, 0], tsne_results[:, 1])
    plt.title(f't-SNE (Perplexity={perplexity})')
    plt.xlabel('t-SNE component 1')
    plt.ylabel('t-SNE component 2')

plt.tight_layout() # Adjust layout to prevent overlap
plt.show()
```


#### What if data has categorical features?

* PCA may or may not work if you have categorical features.

For example, if you have data that looks like this ....

```{python ch4-pca-categorical-generate}
#| warning: false
#| echo: false

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Generate synthetic data with categorical features
np.random.seed(42)
n_samples = 200

# Categorical features
species = np.random.choice(['mouse', 'rat', 'human'], size=n_samples)
tissue = np.random.choice(['liver', 'brain', 'heart'], size=n_samples)
condition = np.random.choice(['healthy', 'diseased'], size=n_samples)

# Create DataFrame
df_cat = pd.DataFrame({
    'species': species,
    'tissue': tissue,
    'condition': condition
})

print(df_cat.head())
```

<!-- TODO: explain one hot encoding, also talkabout CCA and put in bonus material Issue #17 -->

```{python, ch4-pca-notwork-showcategroical}
#| warning: false
#| echo: false

# One-hot encode the categorical features
encoder = OneHotEncoder(sparse_output=False)
encoded_data = encoder.fit_transform(df_cat)

# Apply PCA
pca = PCA(n_components=2)
pcs = pca.fit_transform(encoded_data)

# Plot PCA result
plt.figure(figsize=(6,5))
plt.scatter(pcs[:, 0], pcs[:, 1], alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA on One-Hot Encoded Categorical Data')
plt.grid(True)
plt.tight_layout()
plt.show()

# Show the one-hot encoded feature names
encoded_feature_names = encoder.get_feature_names_out(df_cat.columns)
encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names)

```

* We can split by disease/healthy, or other features.

```{python ch4-pca-color-by-type}
#| warning: false
#| echo: false

# Plot PCA result
plt.figure(figsize=(10, 8))

# Create subplot for PCA
plt.subplot(2, 2, 1)
plt.scatter(pcs[:, 0], pcs[:, 1], alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA on One-Hot Encoded Categorical Data')
plt.grid(True)

# Create subplot showing PCA with colors based on species
plt.subplot(2, 2, 2)
colors = {'mouse': 'red', 'rat': 'blue', 'human': 'green'}
for sp in ['mouse', 'rat', 'human']:
    mask = df_cat['species'] == sp
    plt.scatter(pcs[mask, 0], pcs[mask, 1], c=colors[sp], label=sp, alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Colored by Species')
plt.legend()
plt.grid(True)

# Create subplot showing PCA with colors based on tissue
plt.subplot(2, 2, 3)
colors_tissue = {'liver': 'orange', 'brain': 'purple', 'heart': 'brown'}
for tissue_type in ['liver', 'brain', 'heart']:
    mask = df_cat['tissue'] == tissue_type
    plt.scatter(pcs[mask, 0], pcs[mask, 1], c=colors_tissue[tissue_type], label=tissue_type, alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Colored by Tissue')
plt.legend()
plt.grid(True)

# Create subplot showing PCA with colors based on condition
plt.subplot(2, 2, 4)
colors_condition = {'healthy': 'green', 'diseased': 'red'}
for cond in ['healthy', 'diseased']:
    mask = df_cat['condition'] == cond
    plt.scatter(pcs[mask, 0], pcs[mask, 1], c=colors_condition[cond], label=cond, alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Colored by Condition')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

<!--* TODO: XX tSNE and color by tissue type-->


* Hierarchical clustering

_Recall_:

**Leaves**: Each leaf at the bottom of the dendrogram represents one sample from your dataset.

**Branches**: The branches connect the samples and groups of samples. The height of the branch represents the distance (dissimilarity) between the clusters being merged.

**Height of Merges**: Taller branches indicate that the clusters being merged are more dissimilar, while shorter branches indicate more similar clusters.

**Clusters**: By drawing a horizontal line across the dendrogram at a certain distance, you can define clusters. All samples below that line that are connected by branches form a cluster.

* In the context of your one-hot encoded categorical data (species, tissue, condition), the dendrogram shows how samples are grouped based on their combinations of these categorical features.

* Samples with the same or very similar combinations of categories will be closer together in the dendrogram and merge at lower distances.

* The structure of the dendrogram reflects the relationships and similarities between the different combinations of species, tissue, and condition present in your synthetic dataset.

```{python ch4-hclust-cat-data}
#| warning: false

from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt
import seaborn as sns

# Assume 'encoded_data' exists from the previous one-hot encoding step
linked = linkage(y = encoded_data,
    method = 'ward',
    metric = 'euclidean',
    optimal_ordering=True
    )

# plot dendrogram
plt.figure()
dendrogram(linked, 
            orientation='top',
            distance_sort='descending',
            show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram on One-Hot Encoded Categorical Data')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# or use sns.clustermap()
sns.clustermap(data=encoded_data,
method = "ward",
metric = "euclidean",
row_cluster = True,
col_cluster = True,
cmap = "vlag"
)
```

* Heatmaps

Heatmaps are a great way to visualize data and clustering

```{python ch4-heatmaps}
#| warning: false

import seaborn as sns
import matplotlib.pyplot as plt

# Assume 'encoded_df' exists from the previous one-hot encoding step

plt.figure()
sns.heatmap(encoded_df.T, cmap='viridis', cbar_kws={'label': 'Encoded Value (0 or 1)'}) # Transpose for features on y-axis

plt.title('Heatmap of One-Hot Encoded Categorical Data')
plt.xlabel('Sample Index')
plt.ylabel('Encoded Feature')
plt.tight_layout()
plt.show()
```

<!--## How to evaluate unsupervised learning methods ->

<!--TODO: XX Potentially NC160 data exercise can live here -->




## Summary

::: {.callout-tip}
#### Key Points

- PCA or tSNE are not magic bullets and may not _work_ all the time 
- Usually you need to try a bunch of different techniques
- Remember that unsupervised machine learning is _exploratory_
:::


## Additional reading

[1] [tSNE FAQ by its creators](https://lvdmaaten.github.io/tsne/#faq)

[2] [Swiss roll and SNE](https://jlmelville.github.io/smallvis/swisssne.html)
