{
  "hash": "bfff62da971f5aa0ba4e52ca407e4049",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Normalizing your data and PCA\"\nformat: html\n---\n\n## Introduction\n\nThis chapter demonstrates basic unsupervised machine learning concepts using Python.\n\n::: {.callout-tip}\n## Learning Objectives\n\n- Understand the difference between supervised and unsupervised learning.\n- Apply PCA and clustering to example data.\n- Visualize results.\n:::\n<!-- end callout -->\n\n\n## Normalization (Z-score Standardization)\n\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\n\n_Intuition_: the value represents the number of standard deviations away from the mean for that variable. For example an 80-year-old person might be 3 standard deviations above the mean age.\n\nThe formula for Z-score is:\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\nWhere:\n- $x$ is the original data point.\n- $\\mu$ is the mean of the data.\n- $\\sigma$ is the standard deviation of the data.\n\nFor example, say you have two variables or *features* on very different scales. \n\n\n| Age | Weight (grams) |\n|-----|------------|\n| 25  | 65000      |\n| 30  | 70000      |\n| 35  | 75000      |\n| 40  | 80000      |\n| 45  | 85000      |\n| 50  | 90000      |\n| 55  | 95000      |\n| 60  | 100000     |\n| 65  | 105000     |\n| 70  | 110000     |\n| 75  | 115000     |\n| 80  | 120000     |\n\nIf these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.\n\nHence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.\n\n::: {#8089de04 .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal data:\nAge: mean=43.6, std=13.1\nWeight: mean=69.8, std=9.8\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-3.png){width=662 height=470}\n:::\n:::\n\n\n* In an ideal scenario a feature/variable such as `weight` might be transformed in the following way after normalization:\n\n::: {#5c303113 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-1.png){width=971 height=395}\n:::\n:::\n\n\n* And here is what it might look like for a feature such as `age`.\n\n::: {#27705ef7 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nZ-scored mean: -0.00, std: 1.00\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-4-output-2.png){width=971 height=395}\n:::\n:::\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* After normalization, the *normalized features* are on comparable scales. The features (such as `weight` and `age`) no longer have so much variation. They can be used as input to machine learning algorithms.\n\n* The rule of thumb is to (almost) always *normalize* your data before you use it in a machine learning algorithm. (There are a few exceptions and we will point this out in due course).\n\n:::\n<!-- end callout -->\n\n\n\n\n\n\n\n### Data visualization before doing PCA {#sec-datavizbeforePCA}\n\n::::: {#ex-titledaatviz .callout-exercise}\n\n#### exercise_data_visualization\n\n{{< level 1 >}}\n\nYou should always visualize your data before trying any algorithms on it.\n\nDiscuss in a group. What is wrong with the following plot?\n\n::: {#a3986be0 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-5-output-1.png){width=971 height=395}\n:::\n:::\n\n\n:::: {.callout-answer collapse=\"true\"}\n\n#### Looking at your data\n\nAlways look at your data before you try and machine learning technique on it. There is a 150 year old person in your data!\n\n\n\n::::\n\n:::::\n\n\n\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* Visualize your data before you do any normalization. If there is anything odd about your data, discuss this with the person who gave you the data or did the experiment. This could be an error in the machine that generated the data or a data entry error. If there is justification, you can remove the data point.\n\n* Then perform normalization and apply a machine learning technique.\n\n:::\n<!-- end callout -->\n\n\n\n\n## Setup\n\n::: {#671ef9f7 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n```\n:::\n\n\n## Example Data\n\n::: {#bebdac1f .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-7-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## PCA Example\n\n<!--open tab-->\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#54069c64 .cell execution_count=7}\n``` {.python .cell-code}\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.title(\"PCA Projection\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A simple PCA plot (biplot)](normalization_files/figure-html/cell-8-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n\n\n\n## Scree plot\n\nA *scree* plot is a simple graph that shows how much variance (information) each principal component explains in your data after running PCA. The x-axis shows the principal components (PC1, PC2, etc.), and the y-axis shows the proportion of variance explained by each one.\n\nYou can use a scree plot to decide how many principal components to keep: look for the point where the plot levels off (the *elbow*): this tells you that adding more components doesnâ€™t explain much more variance.\n\n::: {#337f8baf .cell execution_count=8}\n``` {.python .cell-code}\n# Scree plot: variance explained by each component\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Variance Explained Ratio\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-9-output-1.png){width=589 height=449}\n:::\n:::\n\n\nA scree plot may have an *elbow* like the plot below.\n\n::: {#680c2c95 .cell fig.cap='An idealized scree plot' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-10-output-1.png){width=663 height=449}\n:::\n:::\n\n\n<!--\n## Loadings\n\n::: {#997c4f7c .cell execution_count=10}\n``` {.python .cell-code}\n# pca.components_.T\n#feature_names = [\"Feature 1\", \"Feature 2\"]  # Replace with your actual feature names if available\n#loadings = pd.DataFrame(pca.components_.T, columns=[\"PC1\", \"PC2\"], index=feature_names)\n#print(\"PCA Loadings:\")\n#print(loadings)\n```\n:::\n\n\n-->\n\n\n\n\n### Hands-on coding\n\n* Perform PCA on a dataset of US Arrests\n\n\n* Simple method first\n\n::: {#b94b99b8 .cell execution_count=11}\n``` {.python .cell-code}\n!pip install pandas numpy scikit-learn seaborn matplotlib\n```\n:::\n\n\n* Load libraries and data\n\n::: {#4a85f8a9 .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the US Arrests data\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\n\nX = pd.read_csv(url, index_col=0)\n\n# alternatively if you have downloaded the data folder \n#   on your computer try the following\n# import os\n# os.getcwd()\n# os.chdir(\"data\")\n# X = pd.read_csv(\"USArrests.csv\", index_col=0)\n\n# what is in the data?\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Murder</th>\n      <th>Assault</th>\n      <th>UrbanPop</th>\n      <th>ViolentCrime</th>\n    </tr>\n    <tr>\n      <th>State</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Alabama</th>\n      <td>13.2</td>\n      <td>236</td>\n      <td>58</td>\n      <td>21.2</td>\n    </tr>\n    <tr>\n      <th>Alaska</th>\n      <td>10.0</td>\n      <td>263</td>\n      <td>48</td>\n      <td>44.5</td>\n    </tr>\n    <tr>\n      <th>Arizona</th>\n      <td>8.1</td>\n      <td>294</td>\n      <td>80</td>\n      <td>31.0</td>\n    </tr>\n    <tr>\n      <th>Arkansas</th>\n      <td>8.8</td>\n      <td>190</td>\n      <td>50</td>\n      <td>19.5</td>\n    </tr>\n    <tr>\n      <th>California</th>\n      <td>9.0</td>\n      <td>276</td>\n      <td>91</td>\n      <td>40.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n* Normalize the data\n\n::: {#0b846462 .cell execution_count=13}\n``` {.python .cell-code}\nscaler_standard = StandardScaler()\nX_scaled = scaler_standard.fit_transform(X)\n```\n:::\n\n\n* Perform PCA\n\n::: {#9c1eb6fd .cell execution_count=14}\n``` {.python .cell-code}\npca_fn = PCA()\nX_pca = pca_fn.fit_transform(X_scaled)\n```\n:::\n\n\n* Plotting\n\n::: {#b49e4307 .cell execution_count=15}\n``` {.python .cell-code}\nplt.figure()\nplt.scatter(X_pca[:,0], X_pca[:,1])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA on crime data\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-16-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* Label the plot by US State\n\n_Note_: These are now categorical, i.e. these take on discrete values (as opposed to continuous). \n\n* In machine learning, we will need to deal with them differently.\n\n* Discussion: on how to _encode_ these values and how to ensure that these values are _equidistant_ from each other.\n\n::: {#9a3fc488 .cell execution_count=16}\n``` {.python .cell-code}\n# States come from the index\nX.index\n\nstates = X.index # fetch states and assign it to a variable\n\n# map each state to a code\ncolour_codes_states = pd.Categorical(states).codes\n\n# pd.Categorical(states): Converts the sequence states (e.g., a list/Index of state names) into a categorical type. It internally builds:\n# categories: the unique labels (e.g., all distinct state names)\n# codes: integer labels pointing to those categories\n\nplt.figure()\nplt.scatter(X_pca[:,0], X_pca[:,1], c = colour_codes_states)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA on crime data (coloured by US state)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-17-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* A method to have text labels in the PCA plot \n\nWe need slightly more complex code to do this.\n\n\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#03882cc5 .cell execution_count=17}\n``` {.python .cell-code}\n#loadings = pca.components_.T * np.sqrt(pca.explained_variance_)  # variable vectors\n\n# Plot\nfig, ax = plt.subplots()\n\n# Scatter of states\nax.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n\n# Label each state\n# X.index has the state names\n# go through each point (which is each row in the table)\nfor i, state in enumerate(X.index):\n    ax.text(X_pca[i, 0], X_pca[i, 1], state, fontsize=8, va=\"center\", ha=\"left\")\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"PCA Biplot: US Arrests\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A simple PCA plot with text labels](normalization_files/figure-html/cell-18-output-1.png){width=660 height=470}\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n\n\n\n\n* Get the loadings\n\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#7c9b4ec2 .cell execution_count=18}\n``` {.python .cell-code}\n# get the loadings\n# pca.components_ contains the principal component vectors\n# transpose them using T\nloadings = pca_fn.components_.T\n\n# create a data frame\ndf_loadings = pd.DataFrame(loadings,\n                            index=X.columns\n)\n\n# the first column is PC1, then PC2, and so on ...\nprint(df_loadings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     0         1         2         3\nMurder        0.533785 -0.428765 -0.331927 -0.648891\nAssault       0.583489 -0.190485 -0.267593  0.742732\nUrbanPop      0.284213  0.865950 -0.386784 -0.140542\nViolentCrime  0.542068  0.173225  0.817690 -0.086823\n```\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n\n\n\n### Interpreting the US Crime PCA biplot\n\nHere is an intutive explanation of the PCA biplot.\n\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n- **Distances matter**: Points that are far apart represent states with more dissimilar overall crime/urbanization profiles. For example, **Vermont** being far from **California** indicates very different feature patterns in the variables used (e.g., assault, murder, urban population).\n- **PC1 (horizontal) â‰ˆ Crime level/severity**: Higher values indicate greater overall crime intensity (e.g., higher assault/murder rates), lower values indicate lower crime intensity.\n- **PC2 (vertical) â‰ˆ Urbanization**: States to the right tend to have higher urban population and associated traits; those to the left are more rural.\n- You can inspect the _loadings_ to understand what each principal component represents. \n- We will have an exercise on this later.\n<!-- \n- **Variable arrows**: Directions show how variables load on the components. Points near an arrowâ€™s direction tend to have higher values for that variable.\n-->\n- **Reading clusters**: States that cluster together have similar profiles. States on opposite sides of the plot (e.g., Vermont vs. California) differ substantially along the dominant patterns captured by PC1 and PC2.\n\n\n* Interpretation of loadings:\n\n- PC1 (Urbanization axis): All crime variables (Murder, Assault, ViolentCrime) load positively, while UrbanPop has a smaller positive loading. This suggests PC1 captures overall crime levels.\n\n<!--\n- PC2 (Crime vs Urbanization axis): UrbanPop loads strongly negative, while crime variables load positively, creating a contrast between urban vs rural crime patterns.\n-->\n\n:::\n<!-- end callout -->\n\n\n\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* Notice, that we have not _told_ PCA anything about the US states\n\n* Yet it is still able to _find_ some interesting patterns in the data\n\n* This is the strength of unsupervised machine learning\n\n:::\n<!-- end callout -->\n\n\n\n<!--* Get the loadings -->\n\n\n\n* Another method using the `pca` package; prettier plots\n\nInstall the `pca` Python package\n\n\n```python\n!pip install pca\n```\n\n\n\n* Load data\n\n::: {#eabb8d26 .cell execution_count=22}\n``` {.python .cell-code}\nfrom pca import pca\nimport pandas as pd\n\n# Load the US Arrests data (available online)\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\ndf = pd.read_csv(url, index_col=0)\n\nprint(\"US Arrests Data (first 5 rows):\")\nprint(df.head())\nprint(\"\\nData shape:\", df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUS Arrests Data (first 5 rows):\n            Murder  Assault  UrbanPop  ViolentCrime\nState                                              \nAlabama       13.2      236        58          21.2\nAlaska        10.0      263        48          44.5\nArizona        8.1      294        80          31.0\nArkansas       8.8      190        50          19.5\nCalifornia     9.0      276        91          40.6\n\nData shape: (48, 4)\n```\n:::\n:::\n\n\n<!-- code below is not used -->\n\n\n\n* Normalize the data and perform PCA\n\n::: {#07d03845 .cell execution_count=24}\n``` {.python .cell-code}\nmodel = pca(normalize=True)\nout = model.fit_transform(df)\nax = model.biplot()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-25-output-1.png){width=2067 height=1271}\n:::\n:::\n\n\n* Variance explained plots\n\n::: {#de305563 .cell execution_count=25}\n``` {.python .cell-code}\nmodel.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n(<Figure size 1440x960 with 1 Axes>,\n <Axes: title={'center': 'Cumulative explained variance\\n 3 Principal Components explain [100.0%] of the variance.'}, xlabel='Principal Component', ylabel='Percentage explained variance'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-26-output-2.png){width=1212 height=875}\n:::\n:::\n\n\n* 3D PCA biplots\n\n::: {#0b435355 .cell execution_count=26}\n``` {.python .cell-code}\nmodel.biplot3d()\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n(<Figure size 3000x2500 with 1 Axes>,\n <Axes3D: title={'center': '3 Principal Components explain [100.0%] of the variance'}, xlabel='PC1 (61.6% expl.var)', ylabel='PC2 (24.7% expl.var)', zlabel='PC3 (9.14% expl.var)'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-27-output-2.png){width=1945 height=1972}\n:::\n:::\n\n\n* Loadings\n\n*Recall*\n\nWhat is being plotted on the axes (PC1 and PC2) are the `scores`.\n\nThe `scores` for each principal component are calculated as follows:\n\n$$\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + .... \n$$\n\nwhere $X$, $Y$ and $Z$ are the normalized *features*.\n\nThe constants $\\alpha$, $\\beta$, $\\gamma$ are determined by the PCA algorithm. They are called the `loadings`.\n\n::: {#04fd45af .cell execution_count=27}\n``` {.python .cell-code}\nprint(model.results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'loadings':        Murder   Assault  UrbanPop  ViolentCrime\nPC1  0.533785  0.583489  0.284213      0.542068\nPC2 -0.428765 -0.190485  0.865950      0.173225\nPC3 -0.331927 -0.267593 -0.386784      0.817690, 'PC':                      PC1       PC2       PC3\nAlabama         0.923886 -1.127792 -0.437720\nAlaska          1.884005 -1.032585  2.032973\nArizona         1.705462  0.730059  0.043498\nArkansas       -0.198714 -1.092074  0.111217\nCalifornia      2.462479  1.513698  0.585558\nColorado        1.453427  0.982671  1.080932\nConnecticut    -1.406810  1.081895 -0.661238\nDelaware       -0.003621  0.319738 -0.730442\nFlorida         2.947649 -0.070435 -0.569823\nGeorgia         1.571384 -1.281416 -0.326932\nHawaii         -0.966398  1.557165  0.034386\nIdaho          -1.689257 -0.178154  0.241665\nIllinois        1.320695  0.653978 -0.681444\nIndiana        -0.561650  0.161720  0.218372\nIowa           -2.302281  0.133259  0.145716\nKansas         -0.850716  0.279295  0.013602\nKentucky       -0.808869 -0.934920 -0.029023\nLouisiana       1.500981 -0.882536 -0.772483\nMaine          -2.444195 -0.340245 -0.083049\nMaryland        1.702710 -0.431039 -0.158134\nMassachusetts  -0.536401  1.454143 -0.626920\nMichigan        2.044350  0.144860  0.383014\nMinnesota      -1.742422  0.647555  0.133541\nMississippi     0.932617 -2.374555 -0.724196\nMissouri        0.637255  0.263934  0.369919\nMontana        -1.239466 -0.507562  0.236769\nNebraska       -1.317489  0.212450  0.160150\nNevada          2.806905  0.760007  1.157898\nNew Hampshire  -2.431886  0.048021  0.018380\nNew Jersey      0.127587  1.417883 -0.775421\nNew Mexico      1.917815 -0.148279  0.181459\nNew York        1.623118  0.790157 -0.646164\nNorth Carolina  1.064086 -2.207350 -0.854340\nNorth Dakota   -3.038797 -0.548177  0.281399\nOhio           -0.281823  0.736114 -0.041732\nOklahoma       -0.366423  0.292555 -0.026415\nOregon          0.003276  0.556212  0.921912\nPennsylvania   -0.941353  0.568486 -0.411608\nRhode Island   -0.909909  1.464948 -1.387731\nSouth Carolina  1.257310 -1.914756 -0.290121\nSouth Dakota   -2.038884 -0.778125  0.375435\nTennessee       0.935690 -0.851392  0.192734\nTexas           1.293269  0.387317 -0.490484\nUtah           -0.602262  1.466342  0.271830\nVermont        -2.851337 -1.332665  0.825094\nVirginia       -0.153441 -0.190521  0.005751\nWashington     -0.270617  0.975724  0.604878\nWest Virginia  -2.160933 -1.375609  0.097337, 'explained_var': array([0.61629429, 0.86387677, 0.95532444, 1.        ]), 'variance_ratio': array([0.61629429, 0.24758248, 0.09144767, 0.04467556]), 'model': PCA(n_components=np.int64(3)), 'scaler': StandardScaler(), 'pcp': np.float64(1.0000000000000002), 'topfeat':     PC       feature   loading  type\n0  PC1       Assault  0.583489  best\n1  PC2      UrbanPop  0.865950  best\n2  PC3  ViolentCrime  0.817690  best\n3  PC1        Murder  0.533785  weak, 'outliers':                  y_proba     p_raw    y_score  y_bool  y_bool_spe  y_score_spe\nAlabama         0.883607  0.572223   4.780770   False       False     1.457903\nAlaska          0.771864  0.061516  12.020355   False       False     2.148419\nArizona         0.883607  0.487777   5.447889   False       False     1.855152\nArkansas        0.994964  0.849865   2.662427   False       False     1.110005\nCalifornia      0.771864  0.073767  11.512662   False       False     2.890516\nColorado        0.883607  0.291940   7.323773   False       False     1.754449\nConnecticut     0.883607  0.376298   6.434671   False       False     1.774715\nDelaware        0.997194  0.934870   1.827387   False       False     0.319759\nFlorida         0.827974  0.105185  10.498050   False       False     2.948491\nGeorgia         0.883607  0.334106   6.858767   False       False     2.027628\nHawaii          0.883607  0.482129   5.494442   False       False     1.832672\nIdaho           0.883607  0.589071   4.652640   False       False     1.698626\nIllinois        0.883607  0.518417   5.200102   False       False     1.473744\nIndiana         0.997522  0.957121   1.535211   False       False     0.584469\nIowa            0.883607  0.341171   6.785183   False       False     2.306135\nKansas          0.997194  0.915334   2.046914   False       False     0.895390\nKentucky        0.942972  0.766164   3.332046   False       False     1.236262\nLouisiana       0.883607  0.372576   6.470687   False       False     1.741210\nMaine           0.883607  0.264681   7.652531   False       False     2.467763\nMaryland        0.883607  0.543590   5.001739   False       False     1.756421\nMassachusetts   0.883607  0.512543   5.247023   False       False     1.549922\nMichigan        0.883607  0.406682   6.149239   False       False     2.049476\nMinnesota       0.883607  0.477475   5.533020   False       False     1.858861\nMississippi     0.827974  0.134373   9.776761   False       False     2.551134\nMissouri        0.997194  0.908446   2.118885   False       False     0.689750\nMontana         0.942972  0.701130   3.819185   False       False     1.339364\nNebraska        0.942972  0.758317   3.391711   False       False     1.334509\nNevada          0.771864  0.052403  12.462954   False       False     2.907976\nNew Hampshire   0.883607  0.317979   7.031119   False       False     2.432360\nNew Jersey      0.883607  0.577856   4.737795   False       False     1.423612\nNew Mexico      0.883607  0.502690   5.326337   False       False     1.923539\nNew York        0.883607  0.382425   6.375898   False       False     1.805231\nNorth Carolina  0.827974  0.137996   9.697217   False       False     2.450444\nNorth Dakota    0.771864  0.080403  11.269266   False       False     3.087845\nOhio            0.997194  0.934716   1.829224   False       False     0.788218\nOklahoma        0.997522  0.982272   1.082928   False       False     0.468886\nOregon          0.994964  0.843369   2.717557   False       False     0.556221\nPennsylvania    0.942972  0.749878   3.455517   False       False     1.099691\nRhode Island    0.883607  0.234460   8.050041   False       False     1.724530\nSouth Carolina  0.883607  0.243404   7.928295   False       False     2.290659\nSouth Dakota    0.883607  0.291991   7.323178   False       False     2.182321\nTennessee       0.942972  0.719456   3.683208   False       False     1.265063\nTexas           0.942972  0.649265   4.202711   False       False     1.350022\nUtah            0.883607  0.576700   4.746601   False       False     1.585206\nVermont         0.771864  0.038043  13.332939   False       False     3.147398\nVirginia        0.997522  0.997522   0.524914   False       False     0.244627\nWashington      0.942972  0.761722   3.365861   False       False     1.012557\nWest Virginia   0.883607  0.177782   8.926009   False       False     2.561626, 'outliers_params': {'paramT2': (np.float64(-2.4671622769447922e-17), np.float64(1.273765915366402)), 'paramSPE': (array([-9.25185854e-17, -1.38777878e-17]), array([[2.51762774e+00, 6.29946348e-17],\n       [6.29946348e-17, 1.01140077e+00]]))}}\n```\n:::\n:::\n\n\n## Exercise for normalization in PCA {#sec-pcanorm}\n\n::::: {#ex-title_pca .callout-exercise}\n\n#### exercise_pca_normalization\n\n{{< level 2 >}}\n\nWork in a group.\n\n* Try the same code above but now *without* normalisation.\n\n* What differences do you observe in PCA *with* and *without* normalization?\n\n\n:::::\n\n\n\n\n## Exercise (theoretical) {#sec-ex-theoretical}\n\n::::: {#ex-titletheor .callout-exercise}\n\n#### exercise_theoretical\n\n{{< level 2 >}}\n\nBreak up into groups and discuss the following problem:\n\n1. Shown are biological samples with scores\n\n2. The features are genes\n\n* Why are `Sample 33` and `Sample 24` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Why is `Sample 2` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Can we treat `Sample 2` as an outlier? Why or why not? Argue your case.\n\nThe PCA biplot is shown below:\n\n::: {#37b0ee25 .cell execution_count=28}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-29-output-1.png){width=737 height=702}\n:::\n:::\n\n\nThe table of loadings is shown below:\n\n::: {#e7d2c595 .cell execution_count=29}\n\n::: {.cell-output .cell-output-stdout}\n```\n            PC1       PC2       PC3       PC4\nGene1 -0.535899  0.418181 -0.341233  0.649228\nGene2 -0.583184  0.187986 -0.268148 -0.743075\nGene3 -0.278191 -0.872806 -0.378016  0.133877\nGene4 -0.543432 -0.167319  0.817778  0.089024\n```\n:::\n:::\n\n\n:::::\n<!-- end callout -->\n\n\n\n\n<!--\n## Exercise (advanced)\n\nPlot prettier *publication ready* plots for PCA.\n\n::: {.callout-tip}\nLook into the documentation available here for the [PCA package](https://erdogant.github.io/pca/pages/html/Examples.html).\n:::\n-->\n\n\n\n\n\n<!--\n## Clustering Example\n\nPCA is different to clustering where you are trying to find patterns in your data. We will encounter clustering later in the course.\n\n\n\n-->\n\n\n## ðŸ§  PCA vs. Other Techniques\n\n* PCA is **unsupervised** (no labels used)\n* Works best for **linear** relationships\n* Alternatives:\n\n  * t-SNE for nonlinear structures (we will encounter this in the next session)\n\n---\n\n## ðŸ§¬ In Practice: Tips for Biologists\n\n* Always **standardize** data before PCA\n* Be cautious interpreting PCs biologically: PCs are **mathematical constructs**\n\n\n\n\n::: {.callout-tip}\n## Summary\n\n- Need to normalize data before doing dimensionality reduction\n- PCA reduces dimensionality for visualization.\n- Clustering algorithms finds clusters in unlabeled data.\n- The goal of unsupervised learning is to find patterns and form hypotheses.\n:::\n\n\n## Resources\n\n[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)\n\n[2] [ISLP book](https://www.statlearning.com/)\n\n[3] [Video lectures by the authors of the book Introduction to Statistical Learning in Python](https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda)\n\n[4] [Visual explanations of machine learning algorithms](https://mlu-explain.github.io)\n\n---\n\n",
    "supporting": [
      "normalization_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}