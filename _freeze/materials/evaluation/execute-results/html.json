{
  "hash": "c186c2123aa2e188c4e35dbd4614b434",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Evaluation for unsupervised machine learning\n---\n\n::: {.callout-tip}\n#### Learning Objectives\n\n- How to evaluate unsupervised machine learning techniques\n- Know the difference between **internal**, **external**, and **biological** evaluation.\n- Be able to compute and interpret common metrics (silhouette).\n- Be able to choose metrics depending on whether you have ground truth, partial labels, or purely exploratory goals.\n- Communicate results in biologically meaningful ways (marker genes).\n\n:::\n\n\n## Conceptual framing\n\n* **Internal metrics**: use only the data + clustering labels. Measure compactness vs separation (e.g., silhouette).\n* **External metrics**: require ground truth/labels (experimental groups, annotated cell types). \n<!--Use ARI, NMI, precision/recall on pairwise same/different labels.-->\n* **Biological validation**: compare clusters to known marker genes, pathways, experimental metadata (batch, donor), or enrichment tests. Often the most important for biologists.\n\n---\n\n## Quick metrics cheat-sheet (what they tell you)\n\n* **Explained variance (PCA)** â€” fraction of variance captured by components (useful for dimensionality reduction decisions).\n* **Silhouette score** (âˆ’1..1) â€” how well each sample fits its cluster vs nearest other cluster; good general-purpose internal metric.\n<!--\n* **Calinskiâ€“Harabasz** â€” ratio of between/within dispersion (higher = better).\n* **Daviesâ€“Bouldin** (lower = better) â€” average similarity between each cluster and its most similar one.\n* **Adjusted Rand Index (ARI)** â€” similarity between two labelings corrected for chance (commonly 0..1).\n* **Normalized Mutual Information (NMI)** â€” information overlap between labelings (0..1).\n* **Trustworthiness** (for embeddings like UMAP/t-SNE) â€” how well local neighborhoods are preserved.\n-->\n* **Stability / reproducibility** â€” how consistent cluster assignments are under parameter changes.\n\n---\n\n\n## (Optional) Exercise on cancer data\n\n* You have been given some data on cancer cell lines\n\n* Team up with someone and perform hierarchical clustering on this data\n\n* You have been given some starter code to help you load the data\n\n* The data has been downloaded and processed for you (after you run the code below). \n\n* The data is in the variable named `X`\n\n<!-- TODO: find out what are labels -->\n\n<!-- TODO: silhouette plots -->\n\n::: {#22e4b308 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport os\nimport requests\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Load data\nX = pd.read_csv(\"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/cancer_data_saved_NC160.csv\", index_col=0)\n\n\nprint(\"Fetching labels from GitHub...\")\nlabs_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60labs.csv'\nresponse = requests.get(labs_url)\nresponse.raise_for_status()\n# Read the raw text and split into lines.\nall_lines = response.text.strip().splitlines()\n\n# Skip the first line (the header) to match the data dimensions.\nlabs = all_lines[1:]\n\n# The labels in the file are quoted (e.g., \"CNS\"), so we remove the quotes.\nlabs = [label.strip('\"') for label in labs]\n\n# Your code below ......\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFetching labels from GitHub...\n```\n:::\n:::\n\n\n* Write your code while working in pairs or a group\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {#f8535e5e .cell execution_count=2}\n``` {.python .cell-code}\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# Compute linkage matrix for the dendrogram\nZ = linkage(X, method='average', metric='cityblock')\n\n# Plot Dendrogram\nplt.figure()\ndendrogram(Z, labels=labs)\nplt.title('Hierarchical Clustering Dendrogram (NCI60, Average Linkage, Manhattan Distance)')\nplt.xlabel('Cell Line')\nplt.ylabel('Distance')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](evaluation_files/figure-html/cell-3-output-1.png){width=705 height=470}\n:::\n:::\n\n\n:::\n<!-- end callout -->\n\n\n\n### Exercise: try another linkage method and another distance metric\n\n\n::: {.callout-tip}\n## Important Concept (recall)\n\n* There is no \"correct\" answer in unsupervised machine learning!\n\n* So how do you know when you are done? \n:::\n<!-- end callout -->\n\n\n## Evaluating the Quality of Clusters\n\nEvaluating the quality of clusters is a crucial step in any unsupervised learning task. Since we do not have a single _correct_ answer, we use several methods that fall into three main categories:\n\n### 1. Internal Evaluation\nMeasures how good the clustering is based only on the data itself (e.g., how dense and well-separated the clusters are).\n\n### 2. External Evaluation\nMeasures how well the clustering results align with known, ground-truth labels. This is possible here because the NCI60 dataset has known cancer cell line types, which we loaded as `labs`.\n\n### 3. Visual Evaluation\nInspecting plots (like the dendrogram or PCA) to see if the groupings seem logical.\n\n---\n\nLet us add the two most common metrics: one internal and one external.\n\n---\n\n### Internal Evaluation: Silhouette Score\n\nThe **Silhouette Score** measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n\n- **Score Range**: -1 to +1  \n- **Interpretation**:\n  - **+1**: The sample is far away from the neighboring clusters (very good).\n  - **0**: The sample is on or very close to the decision boundary between two neighboring clusters.\n  - **-1**: The sample is assigned to the wrong cluster.\n\n---\n\n### External Evaluation: Adjusted Rand Index (ARI)\n\nThe **Adjusted Rand Index (ARI)** measures the similarity between the true labels (`labs`) and the labels assigned by our clustering algorithm (`cluster_labels`). It accounts for chance groupings.\n\n- **Score Range**: -1 to +1  \n- **Interpretation**:\n  - **+1**: Perfect agreement between true and predicted labels.\n  - **0**: Random labeling (no correlation).\n  - **< 0**: Worse than random labeling.\n\n\n* Here is how you would implement this\n\n::: {#bb42ac50 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_samples, silhouette_score, adjusted_rand_score # Import evaluation metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# 1. Internal Evaluation: Silhouette Score\n# Measures how well-separated clusters are based on the data itself.\nsilhouette = silhouette_score(X, cluster_labels, metric='manhattan')\nprint(\"Silhouette score\")\nprint(silhouette)\nprint(\"Score is from -1 to 1. Higher is better\")\n\n# 2. External Evaluation: Adjusted Rand Index\n# Compares our cluster labels to the true cancer type labels.\nari = adjusted_rand_score(labs, cluster_labels)\nprint(\"Adjusted Rand Index\")\nprint(ari)\nprint(\"Compares to true labels. Score is from -1 to 1. Higher is better\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette score\n0.1436950300449066\nScore is from -1 to 1. Higher is better\nAdjusted Rand Index\n0.0554671516253694\nCompares to true labels. Score is from -1 to 1. Higher is better\n```\n:::\n:::\n\n\n### Intuition of silhouette score\n\nThink of each data point as asking two questions:\n\n- **How close am I to points in my own cluster?** (call this **a**)  \n- **How close would I be, on average, to the _nearest other_ cluster?** (call this **b**)\n\nThe silhouette value for the point compares those two answers:\n\n- If **a** is much smaller than **b** (you are much closer to your own cluster than to any other), the silhouette is close to **+1** â†’ *great fit*.  \n- If **a â‰ˆ b**, silhouette is near **0** â†’ *on the boundary between clusters*.  \n- If **a > b**, silhouette is **negative** â†’ *probably misassigned* (you are closer to another cluster than your own).\n\nNumerically:\n\n\\[\ns = \\frac{b - a}{\\max(a, b)}\n\\]\n\nSo\n\n\\[\ns \\in [-1,\\,1]\n\\]\n\n\n\n<!--\n* Silhouette plots\n\nA silhouette plot is a visual diagnostic for clustering quality that (1) computes a silhouette value for each sample and (2) shows the distribution of those values for every cluster. It helps you see which clusters are tight and well-separated and which contain ambiguous or poorly assigned samples.\n\n* Notes on interpreting silhouette plots\n\n- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.\n\n- Values close to +1 â†’ sample is well matched to its own cluster and poorly matched to neighbors.\n\n- Values near 0 â†’ sample lies between clusters.\n\n- Negative values â†’ sample is likely assigned to the wrong cluster.\n\n- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions â€” a high average can hide poorly-formed small clusters.\n-->\n\n\n\n\n\n\n\n<!--\n* Notes on interpreting silhouette plots\n\n- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.\n\n- Values close to +1 â†’ sample is well matched to its own cluster and poorly matched to neighbors.\n\n- Values near 0 â†’ sample lies between clusters.\n\n- Negative values â†’ sample is likely assigned to the wrong cluster.\n\n- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions â€” a high average can hide poorly-formed small clusters.\n-->\n\n* Compare to literature and what others have done\n\n* Plain old visual evaluation\n\n - compare to labels of what these cell lines are (assuming this is available)\n \n<!--TODO: XX what do these labels mean? -->\n\n\n\n\n\n<!--\nThe silhouette value (per sample)\n\nFor sample \nð‘–\ni:\n\nð‘Ž\n(\nð‘–\n)\na(i) = average distance from \nð‘–\ni to all other points in the same cluster (intra-cluster distance).\n\nFor every other cluster \nð¶\nC, compute the average distance from \nð‘–\ni to points in \nð¶\nC; let \nð‘\n(\nð‘–\n)\nb(i) be the smallest of those (the nearest other cluster's average distance).\n\nThe silhouette value is\n\nð‘ \n(\nð‘–\n)\n=\nð‘\n(\nð‘–\n)\nâˆ’\nð‘Ž\n(\nð‘–\n)\nmax\nâ¡\n(\nð‘Ž\n(\nð‘–\n)\n,\nâ€‰\nð‘\n(\nð‘–\n)\n)\n.\ns(i)=\nmax(a(i),b(i))\nb(i)âˆ’a(i)\n\tâ€‹\n\n.\n\nRange: \nâˆ’\n1\nâ‰¤\nð‘ \n(\nð‘–\n)\nâ‰¤\n1\nâˆ’1â‰¤s(i)â‰¤1.\n\nð‘ \n(\nð‘–\n)\nâ‰ˆ\n1\ns(i)â‰ˆ1: sample is well matched to its own cluster and far from neighbours.\n\nð‘ \n(\nð‘–\n)\nâ‰ˆ\n0\ns(i)â‰ˆ0: sample lies between two clusters.\n\nð‘ \n(\nð‘–\n)\n<\n0\ns(i)<0: sample is probably assigned to the wrong cluster (closer on average to another cluster than to its own).\n\nExample: if \nð‘Ž\n=\n0.3\na=0.3 and \nð‘\n=\n0.6\nb=0.6, then \nð‘ \n=\n(\n0.6\nâˆ’\n0.3\n)\n/\n0.6\n=\n0.5\ns=(0.6âˆ’0.3)/0.6=0.5 â€” a reasonably good assignment.\n\nThe silhouette plot\n\nEach cluster is shown as a horizontal block.\n\nWithin each block, samples are ordered by silhouette value and displayed as horizontal bars (width = \nð‘ \n(\nð‘–\n)\ns(i)).\n\nA dashed vertical line shows the average silhouette score across all samples.\n\nThe plot highlights:\n\nCluster compactness (long bars near +1),\n\nPresence of many low/negative values (bad clusters or misassignments),\n\nRelative sizes of clusters.\n-->\n\nHow to interpret (practical heuristics)\n\n- Mean silhouette â‰³ 0.5 â†’ strong structure (good clustering).\n\n- Mean silhouette â‰ˆ 0.25â€“0.5 â†’ weak to moderate structure; inspect clusters individually.\n\n- Mean silhouette â‰² 0.25 â†’ little structure; clustering may be unreliable. (These are rules of thumb â€” context and domain knowledge matter.)\n\n\n<!--\n* Silhouette plots for hierarchical clustering\n\n-->\n\n\n\n* Also compare to clusterings of other cancer cell lines\n\n* Does the cell line also show up in other datasets? (`external validation`)\n\n\n\n<!--\n\n# Practical code snippets (copy into a Jupyter notebook)\n\nReplace `X` with your feature matrix (cells Ã— genes or PCA components), and `y_true` with any ground-truth labels you might have.\n\n```python\n# Required packages\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import (\n    silhouette_score,\n    calinski_harabasz_score,\n    davies_bouldin_score,\n    adjusted_rand_score,\n    normalized_mutual_info_score\n)\nfrom sklearn.manifold import trustworthiness\nfrom sklearn.model_selection import train_test_split\n\n# 1) PCA explained variance (dimensionality reduction check)\npca = PCA(n_components=20).fit(X)\nprint(\"Explained variance ratio (first 10):\", pca.explained_variance_ratio_[:10])\nprint(\"Cumulative:\", np.cumsum(pca.explained_variance_ratio_)[:10])\n\n# 2) Run simple clustering and internal metrics\nk = 6\nkm = KMeans(n_clusters=k, random_state=0).fit(X)\nlabels = km.labels_\nprint(\"Silhouette:\", silhouette_score(X, labels))\nprint(\"Calinski-Harabasz:\", calinski_harabasz_score(X, labels))\nprint(\"Davies-Bouldin:\", davies_bouldin_score(X, labels))\n\n# 3) If you have ground-truth labels (y_true)\n# print(\"ARI:\", adjusted_rand_score(y_true, labels))\n# print(\"NMI:\", normalized_mutual_info_score(y_true, labels))\n\n# 4) Trustworthiness for an embedding (e.g., embedding = UMAP_result)\n# trust = trustworthiness(X, embedding, n_neighbors=15)\n# print(\"Trustworthiness of embedding:\", trust)\n\n# 5) Stability via subsampling\ndef stability_score(clustering_fn, X, n_runs=10, sample_frac=0.8):\n    from sklearn.metrics import adjusted_rand_score\n    labels_list = []\n    rng = np.random.RandomState(0)\n    for i in range(n_runs):\n        idx = rng.choice(np.arange(X.shape[0]), size=int(sample_frac*X.shape[0]), replace=False)\n        labels_sub = clustering_fn(X[idx])\n        labels_list.append((idx, labels_sub))\n    # compute mean ARI between overlapping samples\n    aris = []\n    for i in range(len(labels_list)):\n        for j in range(i+1, len(labels_list)):\n            idx_i, lab_i = labels_list[i]\n            idx_j, lab_j = labels_list[j]\n            common = np.intersect1d(idx_i, idx_j)\n            if len(common) < 10:\n                continue\n            map_i = {v: k for k, v in enumerate(idx_i)}\n            map_j = {v: k for k, v in enumerate(idx_j)}\n            lab_i_common = [lab_i[map_i[c]] for c in common]\n            lab_j_common = [lab_j[map_j[c]] for c in common]\n            aris.append(adjusted_rand_score(lab_i_common, lab_j_common))\n    return np.mean(aris)\n\n# Example clustering function for kmeans on raw X\nclustering_fn = lambda Xsub: KMeans(n_clusters=k, random_state=0).fit(Xsub).labels_\n# print(\"Stability (mean ARI):\", stability_score(clustering_fn, X, n_runs=6))\n```\n\n---\n\n# Visual diagnostics \n\n* **Silhouette plot**: for each cluster show silhouette widths; points near 1 are clean, negative means bad assignment.\n* **Elbow plot**: inertia/within-cluster sum of squares vs k (use with caution).\n* **UMAP/t-SNE** colored by cluster and colored by known biological metadata (donor, batch, cell cycle) â€” if clusters align with batch, that's a red flag.\n* **Heatmap** of marker genes (or top variable genes) with clusters as columns â€” helps biological interpretation.\n* **Violin/boxplots** of expression of canonical marker genes across clusters.\n* **Confusion matrix** comparing cluster labels vs known labels (if available).\n* **Stability heatmap** of ARI between parameter settings (k values, min\\_samples for DBSCAN, perplexity for t-SNE).\n\n---\n\n# Biological validation techniques\n\n* Check expression of known **marker genes** per cluster â€” violin plots, fraction of cells expressing gene, median expression.\n* **Enrichment tests**: test if clusters are enriched for GO terms, pathways, or cell type marker sets.\n* Check **batch/donor composition** per cluster â€” if one donor dominates a cluster, suspect batch effect.\n* Use **pairwise marker identification** (differential expression between clusters) and then ask: does this match known biology?\n\n---\n\n# Hands-on classroom plan (90â€“120 minutes)\n\n1. **10 min** â€” Intuition + categories (internal / external / biological).\n2. **20 min** â€” Demo: run k-means on a tiny single-cell toy (PCA â†’ kmeans), compute silhouette / CH / DB. Show silhouette plot and UMAP colored by clusters.\n3. **25 min** â€” Students exercise (paired): try k = 2..10, make elbow and silhouette; pick â€œbestâ€ k and justify biologically.\n4. **20 min** â€” Introduce external metrics (ARI/NMI). Give a partial ground truth (e.g., FACS labels) and compare clusterings. Interpret mismatch.\n5. **15 min** â€” Stability exercise: subsample and compute ARI between runs. Discuss parameter sensitivity (UMAP perplexity/embedding variance).\n6. **Final 10 min** â€” Discuss pitfalls & how to report results.\n\n---\n\n# Example mini-assignments (graded)\n\n1. **Compare 3 clustering algorithms** (k-means, hierarchical, DBSCAN) on a provided scRNA dataset: produce internal metric table, UMAPs, marker heatmap, and short writeup (max 500 words) interpreting differences and recommending one method.\n2. **Stability report**: for chosen clustering, show results of 10 subsamples, plot ARI distribution, and explain what it implies for reproducibility.\n3. **Biological validation**: pick 5 marker genes and show their distribution across clusters; perform enrichment test and conclude whether clusters are biologically meaningful.\n\n---\n\n# How to grade / rubric (suggested)\n\n* **40%** biological interpretability (marker genes, enrichment)\n* **30%** technical diagnostics (internal metrics, stability)\n* **20%** visualization & clarity (UMAPs, heatmaps, silhouette plot)\n* **10%** reproducibility (notebooks that run, parameter reporting)\n\n---\n\n# Common pitfalls to teach explicitly\n\n* Relying on a single metric (e.g., optimizing silhouette only) â€” always combine metrics + biology.\n* Over-interpreting t-SNE/UMAP global structure â€” these emphasize local neighborhoods; check trustworthiness.\n* Ignoring batch/donor effects â€” clusters that reflect batches are misleading.\n* Cherry-picking k or parameters to match prior beliefs â€” require pre-registered analysis choices or show sensitivity analyses.\n\n---\n\n# Quick â€œcheat-sheetâ€ (one-paragraph version to hand out)\n\n* **No ground truth?** Use internal metrics (silhouette, CH, DB) + stability checks + marker gene verification.\n* **Have partial labels?** Use ARI/NMI to compare and treat labels as guides, not absolute truth.\n* **Embedding checks:** report PCA explained variance and trustworthiness for UMAP/t-SNE.\n* **Visualize:** always show embedding + gene heatmap + violin plots for markers.\n* **Reproducibility:** report parameters and run at least one sensitivity/subsampling analysis.\n\n---\n\n\n-->\n\n\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n- We learnt evaluation is difficult in unsupervised machine learning!\n:::\n\n",
    "supporting": [
      "evaluation_files"
    ],
    "filters": [],
    "includes": {}
  }
}